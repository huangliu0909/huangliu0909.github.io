<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CS5340 Lab2 report1</title>
    <url>/2020/09/25/CS5340%20Lab2%20report1/</url>
    <content><![CDATA[<h2 id="get-jt-clique-and-edges"><a href="#get-jt-clique-and-edges" class="headerlink" title="_get_jt_clique_and_edges()"></a>_get_jt_clique_and_edges()</h2><p>Create a graph G using nx.Graph() and add nodes and edges to it using the input and edges. Using a helper function <strong>undirected_graph_eliminate()</strong> to get the reconstituted graph, and use nx.find_cliques() to get the jt_cliques in this graph. Create a clique graph G_c using the jt_cliques, set the sepsets’ size as edges’ weights. Get G_c’s maximum spanning tree and assign this tree’s edges as jt_edges.<br><a id="more"></a></p>
<h2 id="get-clique-factors"><a href="#get-clique-factors" class="headerlink" title="_get_clique_factors()"></a>_get_clique_factors()</h2><p>Initialize a Factor() for all cliques in jt_cliques. For each clique, for each factor, if a factor’s variables are subset of this clique and this factor has not been assigned to another clique, assign it to this clique.</p>
<h2 id="get-clique-potentials"><a href="#get-clique-potentials" class="headerlink" title="_get_clique_potentials()"></a>_get_clique_potentials()</h2><p>Use the helper functions <strong>N()</strong> to find one node’s neighbors, use <strong>collect()</strong> and <strong>distribute()</strong> to do the inward and outward message transferring the message using the function <strong>send_message()</strong>. For each clique, multiply its factor with all messages from its neighbors and get its clique_potential.</p>
<h2 id="get-node-marginal-probabilities"><a href="#get-node-marginal-probabilities" class="headerlink" title="_get_node_marginal_probabilities()"></a>_get_node_marginal_probabilities()</h2><p>For each clique, normalize its potential’s values. To retrieve marginal probabilities for all query nodes in the graph. For each node in the query nodes, <strong>find the clique containing this node with the minimum size(number of variables in this clique)</strong>, marginalize out all the other nodes in this minimum clique and then get the marginal_probability of this node.</p>
<h2 id="update-mrf-w-evidence"><a href="#update-mrf-w-evidence" class="headerlink" title="_update_mrf_w_evidence()"></a>_update_mrf_w_evidence()</h2><p>Return the query node lists with all nodes except the keys in evidence. Use factor_evidence() to update factors with evidence.</p>
]]></content>
  </entry>
  <entry>
    <title>CNN</title>
    <url>/2020/07/11/CNN/</url>
    <content><![CDATA[<h2 id="卷积运算的定义"><a href="#卷积运算的定义" class="headerlink" title="卷积运算的定义"></a>卷积运算的定义</h2><p>卷积是对两个实变函数的一种特殊的线性运算，满足交换律和结合律。连续形式如下：</p>
<script type="math/tex; mode=display">s(t)=∫x(a)w(t-a)da</script><p>其中x(·)是输入信号强度(input)，w(·)是权重，也称作核函数(kernel function)。<a id="more"></a><br>离散形式如下（$da=1$）：</p>
<script type="math/tex; mode=display">s(t)=Σx(a)w(t-a)</script><p>卷积运算可以用星号表示：</p>
<script type="math/tex; mode=display">s(t)=(x*w)(t)</script><p>卷积操作的实际含义是对输入信号的每个位置进行加权得到对某个参数的估计，其输出被称作特征映射(feature map)。<br>使用一张二维图像$I$进行输入，使用一个二维的核$K$：</p>
<h2 id="稀疏交互，感受野，参数共享，等变表示"><a href="#稀疏交互，感受野，参数共享，等变表示" class="headerlink" title="稀疏交互，感受野，参数共享，等变表示"></a>稀疏交互，感受野，参数共享，等变表示</h2><h3 id="稀疏交互-sparse-interactions"><a href="#稀疏交互-sparse-interactions" class="headerlink" title="稀疏交互 sparse interactions"></a>稀疏交互 sparse interactions</h3><p>也叫稀疏连接(sparse connectivity)、稀疏权重(sparse weights)<br>核的大小远小于输入大小，使得需要存储的参数显著减少，提高了模型的统计效率，计算量减少。卷积核是一个权值矩阵，表示如何处理单个像素和相邻像素之间的关系，会影响输出的效果和亮度。卷积核中各个元素的相对差值小，就具有模糊降噪的效果；差值大就拉大了每个像素与相邻像素的距离，可以提取边缘或者达到锐化的效果。卷积核的元素相加为1，基本上保持了同样的亮度；如果大于1则图像变亮，小于1则变暗；如果累加为0则输出亮度极低，但不是全黑，可以用于边缘提取。<br>感受野 receptive field也称作接受域。输入x中影响输出单元s1的这些单元称作s1的接受域，即：<strong>每一层输出的特征图上像素点在原始图像上的映射区域大小</strong>。处于卷积网络中更深层的单元的接受域比浅层单元的接受域更大。这说明尽管网络层之间的直接连接很稀疏，但更深层的单元可以直接或间接连接到全部或者大部分输入单元。<br>感受野大小：$r_=r_{l-1}+k_{l-1}\cdot j_{l-1}$。<br>其中$j_l=j_{1-1} \cdot s_l$，因此$r_l=r_{l-1}+((k_{l-1}-1)\cdot ∏_{i=1}^{l-1}s_i)$。<br>k是核大小，s是步长，j是特征图上相邻元素间的像素距离，也可以理解为特征图上移动一个元素在原始图像上移动的像素个数。<br>局部感知域是指对于每个计算单元来说，只需要考虑其像素位置附近的输入，并不需要与上一层所有节点相连。</p>
<h3 id="参数共享-parameter-sharing"><a href="#参数共享-parameter-sharing" class="headerlink" title="参数共享 parameter sharing"></a>参数共享 parameter sharing</h3><p>参数共享是指在一个模型的多个函数中使用相同的参数。传统神经网络中权重的每个元素只使用一次，而参数共享时核的每个元素都作用在输入的每个位置上，即网络含有绑定的权重(tied weight)。卷积运算中的参数共享保证了我们只需要学习一个参数集合而不是对每一个位置都需要学习一个参数集合，大大减少了需要学习的参数数量。</p>
<h3 id="等变表示-equivariant-representations"><a href="#等变表示-equivariant-representations" class="headerlink" title="等变表示 equivariant representations"></a>等变表示 equivariant representations</h3><p>参数共享的特殊形式使得神经网络具有平移等变的性质。相同的边缘或多或少地分布在图像各处，所以应当对整个图像进行参数共享。<br>如果f(x)与g(x)满足f(g(x))=g(f(x))，则f(x)对于变换g具有等变性。</p>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p>卷积网络中一个典型层包含三级：卷积级、探测级、池化级<br>卷积级：并行计算多个卷积产生一组线性激活相应（多个卷积核学习到多个特征）<br>探测级：非线性激活函数<br>池化级：使用池化函数来调整输出<br>探测级输出的特征图与原图像像素大小相同，降维的关键在于池化，用于汇总输出。<br>池化函数使用某一位置的相邻输出的总体统计特征（max、mean）来代替该位置的输出，池化能够使特征提取拥有平移不变性(translation invariant)，可以极大提高网络的统计效率。将池化与降采样（步幅移动）结合，可以使得池化单元少于探测单元，提高计算效率，减少过拟合。<br>池化对于处理不同大小的输入具有重要作用。<br>使用池化是一个无限强的先验：每一个单元都具有对少量平移的不变性。<br>卷积和池化可能导致欠拟合：如果一项任务依赖于保存精确的空间信息，那么在所有特征上使用池化会增大训练误差。</p>
<h2 id="步幅卷积，标准卷积，平铺卷积，局部连接，全连接"><a href="#步幅卷积，标准卷积，平铺卷积，局部连接，全连接" class="headerlink" title="步幅卷积，标准卷积，平铺卷积，局部连接，全连接"></a>步幅卷积，标准卷积，平铺卷积，局部连接，全连接</h2><p>步幅卷积：下采样，零填充（valid，same，full）<br>标准卷积等效于1个核的平铺卷积</p>
<h2 id="不通过监督训练得到卷积核"><a href="#不通过监督训练得到卷积核" class="headerlink" title="不通过监督训练得到卷积核"></a>不通过监督训练得到卷积核</h2><pre><code>随机初始化
手动设计
使用无监督的标准来学习核
</code></pre><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>训练CNN时可以对输入进行旋转、平移、缩放等预处理来提高模型泛化能力(generalization ability)。</p>
<h3 id="Classic-cnn"><a href="#Classic-cnn" class="headerlink" title="Classic cnn"></a>Classic cnn</h3><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><h4 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h4><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>PyTorch Tensorflow Keras</p>
<h3 id="A-brief-summary"><a href="#A-brief-summary" class="headerlink" title="A brief summary"></a>A brief summary</h3><p>Convolution is a special kind of linear operation which satisfies the commutative and associative laws. It’s used to accumulate the weighted input.<br>Convolution Neural Network is the network that uses convolution instead of matrix multiplication. CNN mainly uses the idea of sparse connectivity and parameter sharing to improve the computing effectivity rapidly. The receptive field is defined as the region in the input space that a particular CNN’s feature is looking at.<br>CNN learns multiple features in parallel using multiple convolutional kernals.<br>The pooling function represents the output of a location using the overall statistical characteristics of adjacent output at that location. It can reduce the amount of data and prevent over-fitting.</p>
]]></content>
      <categories>
        <category>学习笔记：深度学习</category>
      </categories>
      <tags>
        <tag>NN</tag>
      </tags>
  </entry>
  <entry>
    <title>CS5340 Lab2 report2</title>
    <url>/2020/09/24/CS5340%20Lab2%20report2/</url>
    <content><![CDATA[<h3 id="learn-node-parameter-w"><a href="#learn-node-parameter-w" class="headerlink" title="_learn_node_parameter_w()"></a>_learn_node_parameter_w()</h3><h4 id="When-input-is-None"><a href="#When-input-is-None" class="headerlink" title="When input is None"></a>When input is None</h4><p>It means there are no observation of its parents. In this case weight size is 1, and weights[0] equals to the average of output using numpy.average().<br><a id="more"></a></p>
<h4 id="Otherwise"><a href="#Otherwise" class="headerlink" title="Otherwise"></a>Otherwise</h4><p>Referring to the 𝐶 +1 equations in page 43 from the lec_6 ppt, these equations can be vectorized into:</p>
<script type="math/tex; mode=display">\boldsymbol{b} - \mathbf{A}\cdot \boldsymbol{w} = 0</script><p>where:</p>
<script type="math/tex; mode=display">
\boldsymbol{b}_{(C+1)\times 1} = \begin{pmatrix} \sum_{n=1}^N x_{u,n}\cdot 1 \\ \sum_{n=1}^N x_{u,n}\cdot x_{u_1,n} \\ ... \\ \sum_{n=1}^N x_{u,n}\cdot x_{u_C,n} \end{pmatrix} \qquad
\mathbf{A}_{(C+1)\times (C+1)} = \begin{pmatrix} \sum_{n=1}^N \begin{pmatrix} 1,x_{u_1,n},...,x_{u_C,n}\end{pmatrix}\cdot 1 \\ \sum_{n=1}^N \begin{pmatrix} 1,x_{u_1,n},...,x_{u_C,n}\end{pmatrix}\cdot x_{u_1,n} \\ ... \\ \sum_{n=1}^N \begin{pmatrix} 1,x_{u_1,n},...,x_{u_C,n}\end{pmatrix}\cdot x_{u_C,n} \end{pmatrix}</script><p>$\boldsymbol{w}_{(C+1)\times 1}$ is the unknown weights we need: $\boldsymbol{w} = {\begin{pmatrix}w_0,w_{u_1},…,w_{u_C}\end{pmatrix}}^T$<br>N is the number of observations of this node. C is the number of this node’s parents. $x_{u,n}$ is the $n^{th}$ observation of this node(output[n]).<br>We can use numpy.linalg.solve() to solve this equation to get the weights vector $\boldsymbol{w}$.</p>
<h3 id="learn-node-parameter-var"><a href="#learn-node-parameter-var" class="headerlink" title="_learn_node_parameter_var()"></a>_learn_node_parameter_var()</h3><p>Take the derivative of 𝐿 w.r.t to $\sigma ^ 2$ and equating to zero:</p>
<script type="math/tex; mode=display">
\hat{\sigma ^ 2} = \frac{\sum_{n=1}^N (x_{u,n}-(\sum_{c=1}^C \boldsymbol{w}[c]\cdot x_{c,n} +\boldsymbol{w}[0]))^2}{N}</script><p>where:</p>
<script type="math/tex; mode=display">
x_{u,n}=output[n]\quad x_{c,n}=input[c][n]\quad \boldsymbol{w}:weights</script><h3 id="get-learned-parameters"><a href="#get-learned-parameters" class="headerlink" title="_get_learned_parameters()"></a>_get_learned_parameters()</h3><p>For each node, add the observations of its parents into the input array. Use _learn_node_parameter_w() to learn the weights $\boldsymbol{w}$, using this $\boldsymbol{w}$ and function _learn_node_parameter_var() to get the var $\hat{\sigma ^ 2}$, set $\hat{\sigma ^ 2}$ to be this node’s var, set $\boldsymbol{w}[0]$ to be this node’s bias, and for node’s $i^{th}$ parent set $\boldsymbol{w}[i]$ to be its weight.</p>
]]></content>
  </entry>
  <entry>
    <title>CS5340 Lab3 report</title>
    <url>/2020/10/11/CS5340%20Lab3%20report/</url>
    <content><![CDATA[<p>Name: HUANG Liu<br>Email: e0575772@u.nus.edu<br>Student ID: A0225138J</p>
<h3 id="e-step-x-list-pi-A-phi"><a href="#e-step-x-list-pi-A-phi" class="headerlink" title="e_step(x_list, pi, A, phi)"></a>e_step(x_list, pi, A, phi)</h3><p>Initialize arrays alpha and c with zeros. For each sequence($x_list[o]$) in $x_list$: $alpha[o][0][k] = pi[k] <em> p(x_list[o][0] | phi[k])$, where $k$ is the state of the latent variable and $p$ iis defined as the value of x in the Gaussian probability density function with $phi[k]$ as the parameter: $p = scipy.stats.norm.pdf(x_list[o][0], phi[“mu”][k], phi[“sigma”][k])$<br>$p(x_list[o][n] | phi[k]) </em> \sum_i alpha[o][n - 1][i] <em> A[i][k]$<br>xi_list[o][n - 1][k][i] = (1 / c[o][n]) </em> alpha[o][n - 1][k] <em> p </em> A[k][i] * beta[o][n][i]</p>
<h3 id="m-step-x-list-gamma-list-xi-list"><a href="#m-step-x-list-gamma-list-xi-list" class="headerlink" title="m_step(x_list, gamma_list, xi_list)"></a>m_step(x_list, gamma_list, xi_list)</h3><h3 id="fit-hmm-x-list-n-states"><a href="#fit-hmm-x-list-n-states" class="headerlink" title="fit_hmm(x_list, n_states)"></a>fit_hmm(x_list, n_states)</h3>]]></content>
  </entry>
  <entry>
    <title>Markov Random Fields (Undirected Graphical Models)</title>
    <url>/2020/08/26/CS5340%20Lec03/</url>
    <content><![CDATA[<h1 id="为什么需要无向图模型"><a href="#为什么需要无向图模型" class="headerlink" title="为什么需要无向图模型"></a>为什么需要无向图模型</h1><p>使用无向图模型(UGM: Undirected Graphical Models) $\mathcal G(\mathcal V,\mathcal E)$来表示<strong>Markov Random Field(Markov network)</strong>。<br>$\mathcal V$ : 顶点集合，每个顶点与一个随机变量 $X_i$ 一一对应。<br>$\mathcal E$ : 无向边集合。<br>边没有方向性，这对于一些图像处理或空间分析问题更加自然。<br>For MRF, conditional independence is determined by simple graph separation, not the defination of “blocked”.<br><a id="more"></a></p>
<h1 id="Conditional-independence-properties"><a href="#Conditional-independence-properties" class="headerlink" title="Conditional independence properties"></a>Conditional independence properties</h1><h2 id="Global-Markov-Property"><a href="#Global-Markov-Property" class="headerlink" title="Global Markov Property"></a>Global Markov Property</h2><p>对于点集A、B、C：$X_A\perp X_B|X_C$ 当且仅当：在图 $\mathcal G$ 中 集合C的点将A和B中的点隔离开，即：当移除所有C中点后，不存在A中点到B中点的路径。<br><img src="/images/CS5340/3/global-markov-eg.png" alt="global-markov"></p>
<h2 id="Local-Markov-Property"><a href="#Local-Markov-Property" class="headerlink" title="Local Markov Property"></a>Local Markov Property</h2><p><img src="/images/CS5340/3/local-markov-eg.png" alt="local-markov"></p>
<h2 id="Pairwise-Markov-Property"><a href="#Pairwise-Markov-Property" class="headerlink" title="Pairwise Markov Property"></a>Pairwise Markov Property</h2><p><img src="/images/CS5340/3/pairwise-markov-eg.png" alt="pairwise-markov"></p>
<h2 id="彼此等价"><a href="#彼此等价" class="headerlink" title="彼此等价"></a>彼此等价</h2><p>Global Markov implies local Markov which implies pairwise Markov.<br>Assuming p(x) &gt; 0 for all x, pairwise Markov implies global Markov. </p>
<h3 id="Independence-Map"><a href="#Independence-Map" class="headerlink" title="Independence-Map"></a>Independence-Map</h3><p> The I-Map of a joint distribution $p(𝑥1,…,𝑥𝑁)$, often written as $𝐼(𝑝)$ represents all independencies in $p(𝑥1,…,𝑥𝑁)$.<br> Similarly, the I-Map of a directed/undirected graph $𝒢$, i.e. $\mathcal I(\mathcal G)$ represents all independencies encoded in $𝒢\mathcal G$.</p>
<h3 id="Intersection-Lemma"><a href="#Intersection-Lemma" class="headerlink" title="Intersection Lemma"></a>Intersection Lemma</h3><p><img src="/images/CS5340/3/Intersection.png" alt="Intersection"><br><img src="/images/CS5340/3/p-g.png" alt="pairwise-global"></p>
<h1 id="Comparative-Semantics"><a href="#Comparative-Semantics" class="headerlink" title="Comparative Semantics"></a>Comparative Semantics</h1><p>Convert the DGM to a UGM by dropping the orientation of the edges is not always correct!</p>
<h1 id="Parameterization-of-MRFs"><a href="#Parameterization-of-MRFs" class="headerlink" title="Parameterization of MRFs"></a>Parameterization of MRFs</h1><p>For DGMs, we only need to focus on the local conditional probabilities of a node and its parents ( $p(x_i|x_{\pi_i})$ ), and the joint probability is <strong>a product of local conditional probabiities</strong> as a result of the chain rule:</p>
<script type="math/tex; mode=display">p(x_1,...,x_N)=\prod_{i=1}^Np(x_i|x_{\pi_i})</script><p>Local parameterization is difficult for UGMs as there is no topological ordering associated with UGMs.</p>
<p>So we use functions to represent the local parameterization instead of the conditional distribution.</p>
<p>Recall two nodes $X_i$ and $X_j$ that are not linked directly, they are conditionally independent given all the other nodes. So we can place $X_i$ and $X_j$ in defferent dactors when obtaining a factorization of the joint probability.<br>This implies that we cannot have a local function that depends on both $X_i$ and $X_j$ in the parameterization.</p>
<h2 id="Maximal-Clique"><a href="#Maximal-Clique" class="headerlink" title="Maximal Clique"></a>Maximal Clique</h2><p>If two nodes are linked, they must appeal in the same factor. So, all the nodes $X_C$ that belong to a maximal clique $C$ in the UGM appear together in a local function $\psi(x_C)$ . <strong>Conditional independence is impossible for any two nodes in maximal clique!</strong></p>
<p>Hammersley-Clifford theorem:</p>
<script type="math/tex; mode=display">p(y|\theta)=\frac{1}{Z(\theta)}\prod_{c\in \mathcal C}\psi_c(y_c|\theta_c)</script><p>$\mathcal C$ : the set of all the maximal cliques of $\mathcal G$<br>$\psi_c(.)$: the factor or potential function of clique c<br>$\theta$; the parameter of the factor $\psi_c(.)$ for $c\in\mathcal C$<br>$\psi_c(c|\theta_c)$ is not a conditional distribution but just an arbitrary chosen positive function:</p>
<script type="math/tex; mode=display">Z(\theta)=\sum_y\prod_{c\in\mathcal C}\psi_c(y_c|\theta_c)</script><p>$Z(\theta)$: partition funtion, to ensure the overall distribution sums to 1</p>
<p><strong>Parameterization of MRFs is not unique.</strong></p>
<ul>
<li>define over  maximal cliques</li>
<li>pairwise parameterization</li>
<li>define over all cliques, i.e., cannonical parameterization </li>
</ul>
<h2 id="Canonical-Parameterization"><a href="#Canonical-Parameterization" class="headerlink" title="Canonical Parameterization"></a>Canonical Parameterization</h2><p>Each node is viewed as a clique. In canonical parameterization we include all the possible cliques.<br>Uniform prior can be assumed on any potential function, i.e., set all potential function of edges to 1 (optional)<br><img src="/images/CS5340/3/Canonical.png" alt="Canonical"></p>
<h2 id="Gibbs-Distribution"><a href="#Gibbs-Distribution" class="headerlink" title="Gibbs Distribution"></a>Gibbs Distribution</h2><script type="math/tex; mode=display">p(y|\theta)=\frac{1}{Z(\theta)}exp(-\sum_cE(y_c|\theta_c))</script><p>$E(y_c)&gt;0$: energy associated with the variables in clique c as the potential function, using in particals’ random moving in air or liquid.<br>Convert Gibbs distibution to a UGM: (Energy based models)</p>
<script type="math/tex; mode=display">\psi_c(y_c|\theta_c)=exp(-E(y_c|\theta_c))</script><p>High probability states correspond to low energy configurations.</p>
<h2 id="Log-linear-potential-Functions"><a href="#Log-linear-potential-Functions" class="headerlink" title="Log-linear potential Functions"></a>Log-linear potential Functions</h2><p><strong>Define the log potentials as a linear function of the parameters</strong> $\theta_c\in\mathbb{R}^M$:</p>
<script type="math/tex; mode=display">log\psi_c(y_c)=\phi_c(y_c)^T\theta_c</script><p>$\phi_c$: feature vector derived from values of variables $y_c$</p>
<script type="math/tex; mode=display">logp(y|\theta)=\sum_c\phi_c(y_c)^T\theta_c-logZ(\theta)</script><p>also known as a <strong>maximum entropy</strong> or a <strong>log-linear</strong> model.<br>If $M=|C|=3$, we only need 3 parameter $\theta_c=[\theta_{c1},\theta_{c2},\theta_{c3}]$ instead of $2^3$ parameters.</p>
<h1 id="Combine-UGM-and-DGM"><a href="#Combine-UGM-and-DGM" class="headerlink" title="Combine UGM and DGM"></a>Combine UGM and DGM</h1><p>A DGM can be converted into a UGM by joining all the links among parents for a head-to-head node, i.e. <strong>moralization</strong><br>This process preserves the joint distribution but conditional independence is lost.</p>
<h1 id="Discriminative-vs-Generative-Models"><a href="#Discriminative-vs-Generative-Models" class="headerlink" title="Discriminative vs Generative Models"></a>Discriminative vs Generative Models</h1><p>Discriminative models: model the posterior directly $p(\mathcal C_k|x)$<br>eg. logistic regression model</p>
<p>Generative models: sampling from the distribution is possible to generate synthetic data points in the input space.<br>Likelihood: $p(x|\mathcal C_kx)$</p>
<h1 id="Conditional-Random-Field-CRF"><a href="#Conditional-Random-Field-CRF" class="headerlink" title="Conditional Random Field(CRF)"></a>Conditional Random Field(CRF)</h1><p>A CRF or discriminative random field is a version of an MRF where all clique potentials are conditioned on <strong>global</strong> input x:</p>
<script type="math/tex; mode=display">p(y|x,w)=\frac{1}{Z(x,w)}\prod_c\psi_c(y_c|x,w)</script><p>log-linear representation:</p>
<script type="math/tex; mode=display">\psi_c(y_c|x,w)=exp(w_c^T\phi(x,y_c))</script><p>CRF make use of input data and the predictions can be data-dependent, but require labeled training data and learning is slower.</p>
]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>PGM</tag>
      </tags>
  </entry>
  <entry>
    <title>Probability Reasoning</title>
    <url>/2020/08/12/CS5340%20Lec01/</url>
    <content><![CDATA[<h1 id="概率空间-Probability-Space"><a href="#概率空间-Probability-Space" class="headerlink" title="概率空间 Probability Space"></a>概率空间 Probability Space</h1><p>概率空间 $(\Omega,E,P)$包含了三个部分，用来描述概率现象。<br>样本空间(Outcome/Sample Space) $\Omega$<br>事件空间(Event Space) E<br>概率分布(Probability Distribution) $P:E\to R$<br><a id="more"></a></p>
<h2 id="样本空间-Outcome-Sample-Space"><a href="#样本空间-Outcome-Sample-Space" class="headerlink" title="样本空间 Outcome/Sample Space"></a>样本空间 Outcome/Sample Space</h2><p>对于一个随机现象，所有可能的输出的集合被称作样本空间，记作 $\Omega$。<br>可以通过Tree Diagram，List(样本空间小时直接枚举)，Table来找到一个概率现象的样本空间。<br>eg. 单次掷色子的样本空间为 $\Omega=\{1,2,3,4,5,6\}$</p>
<h2 id="事件空间-Event-Space"><a href="#事件空间-Event-Space" class="headerlink" title="事件空间 Event Space"></a>事件空间 Event Space</h2><p>样本空间的任意一个子集被称作一个事件，事件本身是可以度量概率的。<br>事件e发生等价于实验的输出结果被包含在事件e中。<br>事件空间(Event Space) $E\subset 2^\Omega$ 是事件的集合，满足以下条件：</p>
<ol>
<li>必须包含空事件 $\emptyset$ 和整个样本空间本身(trivial event) $\Omega$ ;</li>
<li>对可数的并操作封闭 $if:\alpha_i\in E,i=1,2…,then:\cup_{i=1}^{\infty}\alpha_i\in E$ ;</li>
<li>对补操作封闭 $if:\alpha\in E,then:\Omega -\alpha\in E$ .</li>
</ol>
<h2 id="概率分布-Probability-Distribution"><a href="#概率分布-Probability-Distribution" class="headerlink" title="概率分布 Probability Distribution"></a>概率分布 Probability Distribution</h2><p>概率分布P是事件到真实值的一个映射 $(P:E\to\mathbb{R})$ ，满足以下条件(axioms of probability)：</p>
<ol>
<li>非负性：$P(\alpha)&gt;=0,\forall\alpha\in E$ ;</li>
<li>所有可能的结果的概率总和为1：$P(\Omega)=1$ ;</li>
<li>对于互不相交的集合 $\alpha,\beta \in E$ 且 $\alpha\cap\beta=\emptyset$，满足 $P(\alpha\cup\beta)=P(\alpha)+P(\beta)$ .</li>
</ol>
<h3 id="随机变量-Random-Variables"><a href="#随机变量-Random-Variables" class="headerlink" title="随机变量 Random Variables"></a>随机变量 Random Variables</h3><p>随机变量是概率空间的一个可测量函数(measurable function) $X:\Omega\to S$ ，即：将实验结果映射到一个可测量空间(measurable space) $S$ ，$S\in \mathbb{R}$ ，$S$ 可写作 $Val(X)$ ，含义是随机变量 $X$ 的可取值范围。使用 $x_i$ 代表 $X$ 的一个特定取值。<br>$P(x)$ 是 $P(X=x)$ 的简写，其中 $x$ : realization/generic value of a random variable $X$ .<br>随机变量的取值空间 $Val(X)$ 可以是连续的(continuous)，也可以是离散的(discrete)。</p>
<h3 id="离散型概率分布"><a href="#离散型概率分布" class="headerlink" title="离散型概率分布"></a>离散型概率分布</h3><p>Probability mass function(PMF)    $P(x)$：<br>精确的给出每个离散随机变量对应的概率值</p>
<script type="math/tex; mode=display">\displaystyle \sum_{i=1}^k P(X=x_i)=1\\
0\leq P(X=x_i)\leq 1</script><h3 id="连续型概率分布"><a href="#连续型概率分布" class="headerlink" title="连续型概率分布"></a>连续型概率分布</h3><p>Probability Density Function(PDF)    $p(x)$：<br>描述这个随机变量的输出值，在某个确定的取值点附近的可能性</p>
<script type="math/tex; mode=display">\int _{Val(x)}p(x)\,dx=1\\
0\leq P(a\leq X\leq b)=\int _{a}^{b}p(x)\,dx\leq 1</script><p>连续随机变量采用任何特定值的绝对可能性为0： $P(X=x_i)=\int _{x_i}^{x_i}p(x)\,dx=0$</p>
<h1 id="Probabilistic-Reasoning"><a href="#Probabilistic-Reasoning" class="headerlink" title="Probabilistic Reasoning"></a>Probabilistic Reasoning</h1><h2 id="联合概率-Joint-Probability"><a href="#联合概率-Joint-Probability" class="headerlink" title="联合概率 Joint Probability"></a>联合概率 Joint Probability</h2><p>事件A、B同时发生的概率。这两个事件的概率分布可以全是连续性，可以全是离散型，也可以一个连续一个离散。<br>eg. 双离散的情况：</p>
<script type="math/tex; mode=display">\sum _{x\in Val(X)}\sum _{y\in Val(Y)}p(x,y)=1</script><h2 id="边缘分布-Marginalization"><a href="#边缘分布-Marginalization" class="headerlink" title="边缘分布 Marginalization"></a>边缘分布 Marginalization</h2><p>边缘概率分布是指仅考虑一个变量的分布而不考虑所有其它变量的分布。<br>假设已经得到了关于随机变量 $X$ 和 $Y$ 的联合分布函数 $p(x,y)$，可以通过<strong>“sum rule”</strong>将 $Y$ 的所有概率累加从而将 $Y$ 边缘化，进而得到 $X$ 的边缘分布。<br>双离散型：</p>
<script type="math/tex; mode=display">
{\displaystyle p_{X}(x_{i})=\sum _{j}p(x_{i},y_{j})}\\
{\displaystyle p_{Y}(y_{j})=\sum _{i}p(x_{i},y_{j})}</script><p>双连续型：</p>
<script type="math/tex; mode=display">
{\displaystyle p_{X}(x)=\int p(x,y)\,dy}\\
{\displaystyle p_{Y}(y)=\int p(x,y)\,dx}</script><p>对高维同样适用：(随机变量 $W$ 是离散的，$Z$ 是连续的，已知 $W,X,Y,Z$ 的联合分布)：</p>
<script type="math/tex; mode=display">{\displaystyle p(x,y)=\sum _m\int p(w,x,y,z)\,dz}</script><h2 id="条件概率-Conditional-Probability"><a href="#条件概率-Conditional-Probability" class="headerlink" title="条件概率 Conditional Probability"></a>条件概率 Conditional Probability</h2><p>$p(X|Y=y^*)$ ：给定 $Y=y^*$ 时 $X$ 的概率分布，也称作概率的<strong>“chain rule”或”product rule”</strong>，其中 $X$ 是隐随机变量， $Y$ 是观察值(observation/evidence).</p>
<script type="math/tex; mode=display">
P(x|Y=y^{*})=\frac{p(x,Y=y^*)}{\int p(x,Y=y^*)\,dx}=\frac{p(x,Y=y^*)}{p(Y=y^*)}</script><p>可以简写为：</p>
<script type="math/tex; mode=display">
p(x|y)=\frac{p(x,y)}{p(y)}\\\\
p(x,y)=p(x|y)p(y)=p(y|x)p(x)</script><h2 id="贝叶斯法则-Bayes’-Rule"><a href="#贝叶斯法则-Bayes’-Rule" class="headerlink" title="贝叶斯法则 Bayes’ Rule"></a>贝叶斯法则 Bayes’ Rule</h2><script type="math/tex; mode=display">
p(y|x)=\frac{p(x|y)p(y)}{p(x)}=\frac{p(x|y)p(y)}{\int p(x,y)\,dy}=\frac{p(x|y)p(y)}{\int p(x|y)p(y)\,dy}</script><p>$p(x|y)$ 是似然函数(likelihood function)，代表给定 $Y$ 时观察到特定 $X$ 值的倾向性(propensity)<br>$p(y)$ 是先验(Prior)概率，代表在观测到 $X$ 之前掌握的 $Y$ 的信息<br>$p(y|x)$ 是后验(posterior)概率，代表在观测到 $X$ 之后掌握的 $Y$ 的信息<br>$p(x)$ 是观测到的信息(evidence)，是常数，用来将分子正则化以保证左侧是一个有效的概率分布</p>
<h2 id="独立性-independence"><a href="#独立性-independence" class="headerlink" title="独立性 independence"></a>独立性 independence</h2><p>随机变量 $X$ 和 $Y$ 独立意味着每个条件分布都相同：</p>
<script type="math/tex; mode=display">
p(x|y)=p(x)\\\\
p(y|x)=p(y)</script><p>此时 $X$ 和 $Y$ 的联合概率等于边缘概率的乘积：</p>
<script type="math/tex; mode=display">p(x,y)=p(x|y)p(y)=p(x)p(y)</script><h2 id="期望-Expectation"><a href="#期望-Expectation" class="headerlink" title="期望 Expectation"></a>期望 Expectation</h2><p>函数 $f(x)$ 根据 $X$ 的分布取得的平均值被称作期望：</p>
<script type="math/tex; mode=display">E[f(x)]=\sum_x f(x)p(x)\\E[f(x)]=\int f(x)p(x)\,dx</script><p>Rules of Expectation:</p>
<ol>
<li>常数的期望还是常数：<script type="math/tex; mode=display">E[k]=k</script></li>
<li>常数倍的函数的期望等于函数的期望的常数倍：<script type="math/tex; mode=display">E[kf(x)]=kE[f(x)]</script></li>
<li>函数和的期望等于函数期望的和：<script type="math/tex; mode=display">E[f(x)+g(x)]=E[f(x)]+E[g(x)]</script></li>
<li>相互独立的随机变量的函数乘积的期望等于函数期望的乘积：<script type="math/tex; mode=display">E[f(x)g(y)]=E[f(x)]E[g(y)]</script>if $X$, $Y$ are independent.</li>
</ol>
<h1 id="常见的分布"><a href="#常见的分布" class="headerlink" title="常见的分布"></a>常见的分布</h1><h2 id="伯努利分布-Bernoulli-Distribution"><a href="#伯努利分布-Bernoulli-Distribution" class="headerlink" title="伯努利分布 Bernoulli Distribution"></a>伯努利分布 Bernoulli Distribution</h2><p>随机变量 $X$ 是一个二元随机变量，$x\in\{0,1\}$</p>
<script type="math/tex; mode=display">{\displaystyle p(X=0|\lambda)=1-\lambda\\p(X=1|\lambda)=\lambda}</script><p>Or</p>
<script type="math/tex; mode=display">{\displaystyle p(x)=\lambda^x(1-\lambda)^{1-x}\\p(x)=Bern_x[\lambda]}</script><h2 id="Categorical-Distribution"><a href="#Categorical-Distribution" class="headerlink" title="Categorical Distribution"></a>Categorical Distribution</h2><p>随机变量 $X$ 是一个非连续型随机变量，有1-k个彼此互斥的取值。<br>随机变量 $X$ 的取值 $x$ 被表示为一个 $K$ 维向量 $e_k$ 其中 $x_k=1$ 且 $\sum_{k=1}^Kx_k=1$<br>$K$ 维参数 $\lambda=[\lambda_1,…,\lambda_K]^T$，其中 $\lambda&gt;= 0$，$\sum_k\lambda_k=1$</p>
<script type="math/tex; mode=display">p(X=e_k|\lambda)=\lambda_k</script><p>Or</p>
<script type="math/tex; mode=display">{\displaystyle p(x)=\prod_{k=1}^K\lambda_k^{x_k}=\lambda_k\\p(x)=Cat_x[\lambda]}</script><h2 id="单元高斯分布-Univariate-Normal-Distribution"><a href="#单元高斯分布-Univariate-Normal-Distribution" class="headerlink" title="单元高斯分布 Univariate Normal Distribution"></a>单元高斯分布 Univariate Normal Distribution</h2><p>随机变量 $X\in \mathbb{R}$ , $\mu\in\mathbb{R}$ (mean) , $\sigma^2&gt;0$ (variance)</p>
<script type="math/tex; mode=display">{\displaystyle p(x)={\frac {1}{\sqrt {2\pi\sigma^2}}}exp{-{\frac {(x-\mu)^2}{2\sigma^2}}}\\p(x)=Norm_x[\mu,\sigma^2]}</script><h2 id="多元高斯分布-Multivariate-Normal-Distribution"><a href="#多元高斯分布-Multivariate-Normal-Distribution" class="headerlink" title="多元高斯分布 Multivariate Normal Distribution"></a>多元高斯分布 Multivariate Normal Distribution</h2><p>随机变量 $X$ 是一个 $k$ 维向量：$\mathbf{x}\in\mathbb{R}^k$ , $\mathbf{x}=x_{1},\ldots ,x_{k}$<br>均值mean: $\boldsymbol{\mu}\in\mathbb{R}^k$<br>协方差矩阵covariance matrix: $\boldsymbol{\Sigma}\in\mathbb{R}_{+}^{k\times k}$</p>
<script type="math/tex; mode=display">
{\displaystyle p(\mathbf{x})={\frac {\exp \{-{\frac {1}{2}}({\mathbf {x} }-{\boldsymbol {\mu }})^{\mathrm {T} }{\boldsymbol {\Sigma }}^{-1}({\mathbf {x} }-{\boldsymbol {\mu }})\}}{\sqrt {(2\pi )^{k}|{\boldsymbol {\Sigma }}|}}}\\
p(\mathbf{x})=Norm_{\mathbf{x}}[\boldsymbol {\mu },\boldsymbol {\Sigma }]}</script><h1 id="共轭分布-Conjugate-Distribution"><a href="#共轭分布-Conjugate-Distribution" class="headerlink" title="共轭分布 Conjugate Distribution"></a>共轭分布 Conjugate Distribution</h1><blockquote>
<p>if the posterior distributions $p(\theta|x)$ are in the same probability distribution family as the prior probability distribution $p(\theta)$, the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function $p(x | \theta)$.   —- wikipedia</p>
</blockquote>
<p>即：在贝叶斯公式(后验=似然*先验/常数)中，如果后验和先验是同族函数，即 $p_{post}=k\times p_{prior}$ ，则这个先验分布是似然函数的共轭分布，每个似然函数 $p(x | \theta)$ 都有唯一共轭分布与之对应。</p>
<p><img src="/images/conjugate_distribution.png" alt="history"></p>
<h2 id="用法一：学习一个概率分布的参数-theta"><a href="#用法一：学习一个概率分布的参数-theta" class="headerlink" title="用法一：学习一个概率分布的参数 $\theta$"></a>用法一：学习一个概率分布的参数 $\theta$</h2><ol>
<li>找到与似然函数(likelihood)对应的共轭先验分布</li>
<li>后验 $p(\theta|x)$ 必须与共轭先验分布有相同形式</li>
<li>说明 $p(x)$ (evidence)必须是一个常数</li>
</ol>
<h2 id="用法二：边缘化参数"><a href="#用法二：边缘化参数" class="headerlink" title="用法二：边缘化参数"></a>用法二：边缘化参数</h2><script type="math/tex; mode=display">p(x^*|\boldsymbol{x})=\int p(x^*|\theta)p(\theta|\boldsymbol{x})\,d\theta</script><ol>
<li>选择另一项 $p(x^*|\theta)$ 的共轭 $p(\theta|\boldsymbol{x})$</li>
<li>乘积简化成了一个分布与常数的乘积</li>
</ol>
<ul>
<li>i.e. : that is</li>
<li>a.k.a : Also Known As</li>
</ul>
]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Bayesian Networks (Directed Graphical Models)</title>
    <url>/2020/08/19/CS5340%20Lec02/</url>
    <content><![CDATA[<h1 id="条件独立-conditional-independence"><a href="#条件独立-conditional-independence" class="headerlink" title="条件独立 conditional independence"></a>条件独立 conditional independence</h1><p>完全独立不足以对现实的随机变量建模，而完全相关会导致计算量过大。<br>随机变量 $X_A$ 和 $X_C$ 关于 $X_B$ 条件独立：$p(x_A,x_C|x_B)=p(x_A|x_B) p(x_C|x_B)$<br>等价于：$p(x_A|x_B,x_C)=p(x_A|x_B) \qquad \forall X_B:x_B&gt;0$<br>也可以写作：$X_A \perp X_C | X_B$<br><a id="more"></a></p>
<p>例如：一个人的基因与父母之外的非后代的基因在给定其父母基因的条件下相互独立：</p>
<script type="math/tex; mode=display">X_{child}\perp (X_{non-descendants}\backslash X_{parent}) | X_{parent}</script><p><strong>Markov假设：任意节点在局部仅仅依赖于它的直接父节点</strong></p>
<h1 id="贝叶斯网络-Bayes-Network"><a href="#贝叶斯网络-Bayes-Network" class="headerlink" title="贝叶斯网络 Bayes Network"></a>贝叶斯网络 Bayes Network</h1><h2 id="定义-Definition"><a href="#定义-Definition" class="headerlink" title="定义 Definition"></a>定义 Definition</h2><p>使用有向无环图(DAG: directed acyclic graph)$\mathcal G(\mathcal V,\mathcal E)$来表示条件独立，其中 $\mathcal V$ 是顶点集合，$\mathcal E$ 是有向边集合。每个顶点代表一个随机变量，有向边代表随机变量之间的依赖关系。</p>
<p>Ancestors：前序结点<br>Descendants：后续结点</p>
<p>$X_{\pi_i}$是随机变量 $X_i$ 的parent<br>如果$X_i$入度为0，则parent为空集</p>
<ul>
<li>必须是无环图，即：图中必须存在拓扑排序(“There exists an ordering of the nodes such that there are no links that go from any node to any lower numbered node. “)</li>
<li>For practical applications of probabilistic models, it will typically be the highernumbered variables corresponding to terminal nodes of the graph that represent the observations, with lower-numbered nodes corresponding to latent variables. </li>
<li>The hidden variables in a probabilistic model need not, however, have any explicit physical interpretation but may be introduced simply to allow a more complex joint distribution to be constructed from simpler components. </li>
</ul>
<h2 id="Markov-Assumption"><a href="#Markov-Assumption" class="headerlink" title="Markov Assumption"></a>Markov Assumption</h2><script type="math/tex; mode=display">X_i\perp (X_{nonDesc(X_i)})\backslash X_{\pi_i})|X_{\pi_i}</script><p>这种parent-child模型代表了条件独立：$p(x_i|X_{\pi_i})$</p>
<h2 id="联合概率-Joint-Probability"><a href="#联合概率-Joint-Probability" class="headerlink" title="联合概率 Joint Probability"></a>联合概率 Joint Probability</h2><p>“ The joint distribution deﬁned by a graph is given by the product, over all of the nodes of the graph, of a conditional distribution for each node conditioned on the variables corresponding to the parents of that node in the graph”<br>随机变量的联合概率可以从图中表示为所有局部条件独立的乘积：（假设$x_1,…x_N$是拓扑排序）</p>
<script type="math/tex; mode=display">p(x_1,...x_N)=\prod_{i=1}^Np(x_i|x_{\pi_i})</script><p>This key equation expresses the factorization properties of the joint distribution for a directed graphical model.<br><img src="/images/Markov_Assumption.png" alt="Markov_Assumption"><br>要证明 $X_1$ 和 $X_3$ 在给定 $X_2$ 的情况下与 $X_4$ 独立，即证：$p(x_4|x_1,x_2,x_3)=p(x_4|x_2)$，可以分别求出 $p(x_1,x_2,x_3,x_4)$ 和 $p(x_1,x_2,x_3)$，两者相除得到 $p(x_4|x_1,x_2,x_3)$</p>
<h2 id="Parameter-Reduction"><a href="#Parameter-Reduction" class="headerlink" title="Parameter Reduction"></a>Parameter Reduction</h2><p>$m_i$ 是点 $X_i$ 的直接parent结点的个数，$K$ 是每个点可取值的个数。<br>$X_i$ 的条件概率可以用 $K^{(m_i+1)}$ 个参数来表示。<br>参数个数从 $K^N$ 减少到了 $K^{(m_i+1)}$，其中 $m_i &lt;&lt; N$。<br><img src="/images/parameter_reduction.png" alt="history"></p>
<h2 id="其它条件独立"><a href="#其它条件独立" class="headerlink" title="其它条件独立"></a>其它条件独立</h2><p>除了parent-child产生的条件独立，图中也有其它的条件独立。如：$X_1\perp X_6| \{X_2,X_3\}$<br>直观解释：图中 $X_2,X_3$ 这两个点阻隔了(block)所有从 $X_1$ 到 $X_6$ 的路径<br>这暗示了图的分隔可以导向相对独立的产生。</p>
<h3 id="Three-Canonical-3-Node-Graphs"><a href="#Three-Canonical-3-Node-Graphs" class="headerlink" title="Three Canonical 3-Node Graphs"></a>Three Canonical 3-Node Graphs</h3><p>顶点 $C$ 被称作 “head-to-tail” w.r.t $A$ 到 $B$ 的路径，这个顶点隔开了 $A$ 和 $B$ 并且成为了他们相互独立的条件。即：<strong>给定当前状态的情况下过去状态和未来状态相互独立。</strong><br><img src="/images/3-Node-Graph-1.png" alt="history"></p>
<p>顶点 $C$ 被称作 “tail-to-tail” w.r.t $A$ 到 $B$ 的路径，这个顶点隔开了 $A$ 和 $B$ 并且成为了他们相互独立的条件。即：<strong>$C$ 解释了 $A$ 和 $B$ 之间所有的关联性。</strong><br><img src="/images/3-Node-Graph-2.png" alt="3-Node-Graph"></p>
<p>顶点 $C$ 被称作 “head-to-head” w.r.t $A$ 到 $B$ 的路径，也称作”V-structure”。当 $C$ 不存在时，$A$ 和 $B$ 之间路径被阻隔，$A$ 和 $B$ 相互独立。而将 $C$ 当作条件时，连通了这条路径，导致存在了关联性(explaining-away effect)。<br><img src="/images/3-Node-Graph-3.png" alt="3-Node-Graph"><br>$C$ 的每个子节点也都提供了这种连通性。<br><img src="/images/3-Node-Graph-4.png" alt="3-Node-Graph"><br>In summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked unless it is observed in which case it blocks the path. By contrast, a head-to-head node blocks a path if it is unobserved, but once the node, and/or at least one of its descendants, is observed the path becomes unblocked. </p>
<h3 id="Graph-Separation-directional-seperated"><a href="#Graph-Separation-directional-seperated" class="headerlink" title="Graph Separation (directional-seperated)"></a>Graph Separation (directional-seperated)</h3><p>如果所有从点集$A$到点集$B$的路径都在点集$C$被观察到时被阻隔了(blocked)，那么 $A\perp B|C$。<br><strong>$A$ is d-separated from $B$ by $C$.</strong><br> Any such path is said to be blocked if it includes a node such that either </p>
<ul>
<li>the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the node is in the set C, or </li>
<li>the arrows meet head-to-head at the node, and neither the node, nor any of its descendants, is in the set C.</li>
</ul>
<h3 id="Bayes-Ball-Algorithm"><a href="#Bayes-Ball-Algorithm" class="headerlink" title="Bayes Ball Algorithm"></a>Bayes Ball Algorithm</h3><p>是一个可达性算法，可以用BFS完成。<br>将点集$A$中的每个点上放置一个球，沿着路径进行滚动，如果被$C$阻隔则停住。<br>如果$A$中每个球都最终无法到达$B$中的点，则说明 $A\perp B|C$ ，否则$A$和$B$在给定$C$时不满足条件独立。</p>
<h3 id="Markov-Blanket"><a href="#Markov-Blanket" class="headerlink" title="Markov Blanket"></a>Markov Blanket</h3><p>一个顶点 $X_i$ 仅依赖于自己的直接父节点(parent) $X_{\pi_i}$、直接子节点(child)和子节点的其它父节点(co-parent)。<br>$p(x_m|x_{\pi_m}:x_i\in x_{\pi_m})$ ：代表了顶点 $X_i$ 的直接子节点与它的直接父节点( $X_i$ 和它的co-parent)之间的依赖关系。<br><img src="/images/Markov_Blanket.png" alt="history"></p>
]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>PGM</tag>
      </tags>
  </entry>
  <entry>
    <title>Factor Graph and the Junction Tree Algorithm</title>
    <url>/2020/09/09/CS5340%20Lec05/</url>
    <content><![CDATA[<h1 id="Factor-Graph"><a href="#Factor-Graph" class="headerlink" title="Factor Graph"></a>Factor Graph</h1><h2 id="VS-DGM-amp-UGM"><a href="#VS-DGM-amp-UGM" class="headerlink" title="VS DGM &amp; UGM"></a>VS DGM &amp; UGM</h2><p>DGM and UGM: allow a global function of several variables to be expressed as <strong>a product of factors</strong> over subsets of those variables, designed for conditional independence or potentials.<br>Factor graphs: make this decomposition explicit by <strong>introducing additional nodes</strong> for the factors in addition to the nodes representing the variables, designed for more explicit details of the factorization.<br><a id="more"></a></p>
<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p><img src="/images/CS5340/5/FG.png" alt="FG"><br>$p(x_1,…,x_n)=\prod_sf_s(x_s)\qquad x_s\in\{x_1,…,x_n\}$<br>Each $f_s$ is a function of a set of $x_s$.</p>
<h2 id="Convert-a-UGM-into-a-factor-graph"><a href="#Convert-a-UGM-into-a-factor-graph" class="headerlink" title="Convert a UGM into a factor graph"></a>Convert a UGM into a factor graph</h2><p>Representing the potential functions over cliques as factors $f_s(x_s)$.<br>Different factor graphs may correspond to the same DGM/UGM.<br>Loopy DGM/UGM becomes a tree when converted to factor graph.</p>
<p><strong>Goal: compute all singleton marginal probabilities.</strong></p>
<h2 id="Messages"><a href="#Messages" class="headerlink" title="Messages"></a>Messages</h2><p>From variable to factor:</p>
<script type="math/tex; mode=display">v_{is}=\prod_{t\in N(i)-s}\mu_{ti}</script><p>and $v_{is}=1$ when $x_i$ is a leaf.<br>From factor to variable:</p>
<script type="math/tex; mode=display">\mu_{si}=\sum_{x\in N(s)-i}(f_s(x_{N(s)})\prod_{j\in N(s)-i}v_{js})</script><p>and $\mu_{si}=f_s$ when $f_s$ is a leaf.<br><strong>This message is equal to $m_{ji}$ in UGM.</strong></p>
<p>Therefore, we can get the marginalization probability:</p>
<script type="math/tex; mode=display">p(x_i)\propto\prod_{s\in N(i)}\mu_{si}(x_i)=v_{is}\mu_{si}\qquad \forall s\in N(i)</script><p>since $v_{is} = \prod_{t\in N(i)-s}\mu_{ti}$</p>
<h2 id="Sum-product-algorithm-of-factor-graph"><a href="#Sum-product-algorithm-of-factor-graph" class="headerlink" title="Sum-product algorithm of factor graph"></a>Sum-product algorithm of factor graph</h2><p><img src="/images/CS5340/5/sum-product.png" alt="sum-product"></p>
<h1 id="Max-product-Algorithm"><a href="#Max-product-Algorithm" class="headerlink" title="Max-product Algorithm"></a>Max-product Algorithm</h1><h1 id="Junction-Tree"><a href="#Junction-Tree" class="headerlink" title="Junction Tree"></a>Junction Tree</h1>]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>PGM</tag>
      </tags>
  </entry>
  <entry>
    <title>Parameter Learning with Complete Data</title>
    <url>/2020/09/16/CS5340%20Lec06/</url>
    <content><![CDATA[<p>Motivation: How to get the unknown parameter $\theta$ of a DGM/UGM $p(x_1,x_2…,x_M|\theta)$ from fully observed data?<br>Given: a set of 𝑁 independent and identically distributed (i.i.d) complete observation of each random variable 𝑋:$x_{1,1}, … x_{1,N},…, x_{M,1},…x_{M,N}$.</p>
<ul>
<li>Maximum Likelihood Estimate (MLE)<script type="math/tex; mode=display">\begin{aligned}\hat{\theta}&=\arg\max_{\theta} p(x_1,...,x_M|\theta)\\&=\arg\max_{\theta} \prod_{i=0}^N p(x_{1,i},...,x_{M,i}|\theta)\qquad (i.i.d)\end{aligned}</script></li>
<li>Maximum a Posteriori (MAP)<script type="math/tex; mode=display">\begin{aligned}\hat{\theta}&=\arg\max_{\theta} p(\theta|x_1,...,x_M)\\&=\arg\max_{\theta}\frac{p(x_1,...,x_M|\theta)p(\theta)}{p(x_1,...,x_M)}\qquad (Bayes'rule)\\&=\arg\max_{\theta}\frac{\prod_{i=0}^N p(x_{1,i},...,x_{M,i}|\theta)p(\theta)}{p(x_1,...,x_M)}\qquad (i.i.d)\\&=\arg\max_{\theta}\prod_{i=0}^N p(x_{1,i},...,x_{M,i}|\theta)p(\theta)
\end{aligned}</script><a id="more"></a>
</li>
</ul>
<h1 id="DGM-MLE-amp-MAP"><a href="#DGM-MLE-amp-MAP" class="headerlink" title="DGM: MLE &amp; MAP"></a>DGM: MLE &amp; MAP</h1><h2 id="Special-Cases-Single-Random-Variable-DGM"><a href="#Special-Cases-Single-Random-Variable-DGM" class="headerlink" title="Special Cases: Single Random Variable DGM"></a>Special Cases: Single Random Variable DGM</h2><p><pre class="mermaid">graph TD;
    parameters-->X
    theta-->X_observation_1;
    theta-->X_observation_2;
    theta-->...;
    theta-->X_observation_N;</pre><br>theta is the parameter we want to learn from the N disconnected observations of variable.</p>
<h3 id="Continuous-Univariate-Normal-Distribution"><a href="#Continuous-Univariate-Normal-Distribution" class="headerlink" title="Continuous: Univariate Normal Distribution"></a>Continuous: Univariate Normal Distribution</h3><p>Fit an univariate normal distribution model to a set of scalar data $X:x_1,x_2,…,x_N$.<br>Goal is to find the parameter $theta = (\mu,\sigma^2)$</p>
<script type="math/tex; mode=display">p(x|\theta)={\frac {1}{\sqrt {2\pi\sigma^2}}}exp{-{\frac {(x-\mu)^2}{2\sigma^2}}}=Norm_x[\mu,\sigma^2]</script><ul>
<li>Maximum Likelihood Estimation (MLE)</li>
<li>Maximum a Posteriori (MAP)</li>
</ul>
<h3 id="Discrete-Univatiate-Categorical-Distribution"><a href="#Discrete-Univatiate-Categorical-Distribution" class="headerlink" title="Discrete: Univatiate Categorical Distribution"></a>Discrete: Univatiate Categorical Distribution</h3><ul>
<li>Maximum Likelihood Estimation (MLE)</li>
<li>Maximum a Posteriori (MAP)</li>
</ul>
<h2 id="General-Cases"><a href="#General-Cases" class="headerlink" title="General Cases:"></a>General Cases:</h2><h3 id="Discrete"><a href="#Discrete" class="headerlink" title="Discrete"></a>Discrete</h3><h4 id="Maximum-Log-Likelihood"><a href="#Maximum-Log-Likelihood" class="headerlink" title="Maximum Log-Likelihood"></a>Maximum Log-Likelihood</h4><h4 id="Maximum-A-Posteriori-MAP"><a href="#Maximum-A-Posteriori-MAP" class="headerlink" title="Maximum A Posteriori(MAP)"></a>Maximum A Posteriori(MAP)</h4><h3 id="Continuous"><a href="#Continuous" class="headerlink" title="Continuous"></a>Continuous</h3><h4 id="Maximum-Log-Likelihood-1"><a href="#Maximum-Log-Likelihood-1" class="headerlink" title="Maximum Log-Likelihood"></a>Maximum Log-Likelihood</h4><h4 id="Maximum-A-Posteriori-MAP-1"><a href="#Maximum-A-Posteriori-MAP-1" class="headerlink" title="Maximum A Posteriori(MAP)"></a>Maximum A Posteriori(MAP)</h4><h1 id="MRF-stochasitc-maximum-likelihood-amp-iterative-proportional-fitting"><a href="#MRF-stochasitc-maximum-likelihood-amp-iterative-proportional-fitting" class="headerlink" title="MRF: stochasitc maximum likelihood &amp; iterative proportional fitting"></a>MRF: stochasitc maximum likelihood &amp; iterative proportional fitting</h1><h2 id="Stochasitc-Maximum-Likelihood"><a href="#Stochasitc-Maximum-Likelihood" class="headerlink" title="Stochasitc Maximum Likelihood"></a>Stochasitc Maximum Likelihood</h2><h2 id="Iterative-Proportional-Fitting-IFP"><a href="#Iterative-Proportional-Fitting-IFP" class="headerlink" title="Iterative Proportional Fitting(IFP)"></a>Iterative Proportional Fitting(IFP)</h2><h1 id="CRF-stochastic-gradient-descent"><a href="#CRF-stochastic-gradient-descent" class="headerlink" title="CRF: stochastic gradient descent"></a>CRF: stochastic gradient descent</h1><h2 id="Stochasitc-Gradient-Descent"><a href="#Stochasitc-Gradient-Descent" class="headerlink" title="Stochasitc Gradient Descent"></a>Stochasitc Gradient Descent</h2><h2 id="Maximum-A-Posteriori-MAP-2"><a href="#Maximum-A-Posteriori-MAP-2" class="headerlink" title="Maximum A Posteriori(MAP)"></a>Maximum A Posteriori(MAP)</h2>]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>PGM</tag>
      </tags>
  </entry>
  <entry>
    <title>Mixture Models and EM Algorithms</title>
    <url>/2020/09/30/CS5340%20Lec07/</url>
    <content><![CDATA[<h1 id="K-Mean-Deterministic"><a href="#K-Mean-Deterministic" class="headerlink" title="K-Mean (Deterministic)"></a>K-Mean (Deterministic)</h1><h1 id="Gaussian-Mixture-Models"><a href="#Gaussian-Mixture-Models" class="headerlink" title="Gaussian Mixture Models"></a>Gaussian Mixture Models</h1><h1 id="EM-Algorithms"><a href="#EM-Algorithms" class="headerlink" title="EM Algorithms"></a>EM Algorithms</h1>]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>Cluster</tag>
      </tags>
  </entry>
  <entry>
    <title>比较排序 - 1：选择 插入 冒泡 鸡尾酒 希尔</title>
    <url>/2020/08/02/Comparison%20Sorts-1/</url>
    <content><![CDATA[<h1 id="Comparison-sort"><a href="#Comparison-sort" class="headerlink" title="Comparison sort"></a>Comparison sort</h1><blockquote>
<p>“A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a “less than or equal to” operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list. The only requirement is that the operator forms a total preorder over the data, with:<br>    if a ≤ b and b ≤ c then a ≤ c (transitivity)<br>    for all a and b, a ≤ b or b ≤ a (connexity).”<br>    From Wikipedia</p>
</blockquote>
<p><strong>完整排序算法代码：<a href="https://github.com/huangliu0909/Sort-Algorithms">https://github.com/huangliu0909/Sort-Algorithms</a></strong><br><a id="more"></a><br>比较排序的核心方法是进行两个数字的比较并进行位置的交换(swap)。</p>
<h1 id="冒泡排序-Bubble-sort"><a href="#冒泡排序-Bubble-sort" class="headerlink" title="冒泡排序 Bubble sort"></a>冒泡排序 Bubble sort</h1><p>对每个元素逐个与后面的元素进行比较，如果顺序不正确则交换。类似于水中冒泡，较大的数沉下去，较小的数慢慢冒起来。i = 0时会把最大的数移动到数组的最后一位，i = k时会把第k+1大的数放在length-k-1处。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">bubble_sort</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; array.length - <span class="number">1</span>; i++) </span><br><span class="line">           <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; array.length - i - <span class="number">1</span>; j++) </span><br><span class="line">               <span class="keyword">if</span> (array[j] &gt; array[j+<span class="number">1</span>]) swap(array, j, j + <span class="number">1</span>);</span><br><span class="line">               </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="选择排序-Selection-sort"><a href="#选择排序-Selection-sort" class="headerlink" title="选择排序 Selection sort"></a>选择排序 Selection sort</h1><p>重复寻找剩余数组的最小值，与当前位置的值进行交换。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">selection_sort</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; array.length; i ++) &#123;</span><br><span class="line">		<span class="keyword">int</span> min = array[i];</span><br><span class="line">		<span class="keyword">int</span> index = i;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &lt; array.length; j ++) &#123;</span><br><span class="line">			<span class="keyword">if</span>(array[j] &lt; min) &#123;</span><br><span class="line">				min = array[j];</span><br><span class="line">				index = j;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(i != index) swap(array, i, index);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="插入排序-Insertion-sort"><a href="#插入排序-Insertion-sort" class="headerlink" title="插入排序 Insertion sort"></a>插入排序 Insertion sort</h1><p>遍历数组，将第i个数插入到前i-1个数的合适位置上。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertion_sort</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; array.length; i ++) &#123;</span><br><span class="line">		<span class="keyword">int</span> index = array[i]; </span><br><span class="line">           <span class="keyword">int</span> j = i - <span class="number">1</span>; </span><br><span class="line">           <span class="keyword">while</span> (j &gt;= <span class="number">0</span> &amp;&amp; array[j] &gt; index) &#123; </span><br><span class="line">           	array[j + <span class="number">1</span>] = array[j]; </span><br><span class="line">               j = j - <span class="number">1</span>; </span><br><span class="line">           &#125; </span><br><span class="line">           array[j + <span class="number">1</span>] = index; </span><br><span class="line">	&#125;    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="鸡尾酒排序-Cocktail-shaker-sort"><a href="#鸡尾酒排序-Cocktail-shaker-sort" class="headerlink" title="鸡尾酒排序 Cocktail shaker sort"></a>鸡尾酒排序 Cocktail shaker sort</h1><p>冒泡排序的变种。冒泡排序将最大的逐个放到最后，鸡尾酒排序是双向的冒泡。<br>第i轮时遍历范围索引是i到length-i-1，正向遍历一次，判断有无swap，无则返回，否则再进行一次逆向遍历，判断有无swap，循环直至没有swap。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">cocktail_shaker_sort</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">boolean</span> swapped = <span class="keyword">true</span>; </span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(swapped == <span class="keyword">true</span>) &#123;</span><br><span class="line">		swapped = <span class="keyword">false</span>;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &lt; array.length - i - <span class="number">1</span>; j++) </span><br><span class="line">               <span class="keyword">if</span> (array[j] &gt; array[j+<span class="number">1</span>]) &#123;</span><br><span class="line">               	 swap(array, j, j + <span class="number">1</span>);</span><br><span class="line">               	 swapped = <span class="keyword">true</span>;</span><br><span class="line">               &#125;</span><br><span class="line">		<span class="keyword">if</span>(swapped == <span class="keyword">false</span>) <span class="keyword">return</span>;</span><br><span class="line">		swapped = <span class="keyword">false</span>;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j = array.length - i - <span class="number">2</span>; j &gt;= i; j --) &#123;</span><br><span class="line">			<span class="keyword">if</span> (array[j] &gt; array[j+<span class="number">1</span>]) &#123;</span><br><span class="line">              	 swap(array, j, j + <span class="number">1</span>);</span><br><span class="line">              	 swapped = <span class="keyword">true</span>;</span><br><span class="line">              &#125;</span><br><span class="line">		&#125;</span><br><span class="line">		i ++;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="希尔排序-Shellsort"><a href="#希尔排序-Shellsort" class="headerlink" title="希尔排序 Shellsort"></a>希尔排序 Shellsort</h1><p>插入排序的变种，首先对比间隔(gap)较远的元素而不是相邻(gap=1)元素，逐轮reduce到gap=1，这样可以有效减少最坏情况下元素的交换操作的数量。常见的gap选择为：gap=length/2，reduce过程gap=gap/2。最外层循环0~gap-1，对这其中的每个数不断+gap得到的间隔较远的元素进行插入排序。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">shell_sort</span><span class="params">(<span class="keyword">int</span>[] array)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> gap = array.length/<span class="number">2</span>;</span><br><span class="line">		<span class="keyword">while</span>(gap != <span class="number">0</span>) &#123;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; gap; i ++) &#123;</span><br><span class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> j = i + gap; j &lt; array.length; j += gap) &#123;</span><br><span class="line">					<span class="keyword">int</span> index = array[j]; </span><br><span class="line">		            <span class="keyword">int</span> k = j - gap; </span><br><span class="line">		            <span class="keyword">while</span> (k &gt;= i &amp;&amp; array[k] &gt; index) &#123; </span><br><span class="line">		            	array[k + gap] = array[k]; </span><br><span class="line">		                k = k - gap; </span><br><span class="line">		            &#125; </span><br><span class="line">		            array[k + gap] = index; </span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			gap /= <span class="number">2</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>完整排序算法代码：<a href="https://github.com/huangliu0909/Sort-Algorithms">https://github.com/huangliu0909/Sort-Algorithms</a></p>
]]></content>
      <categories>
        <category>学习笔记：算法</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title>Summary</title>
    <url>/2020/09/30/CS5340%20Summary/</url>
    <content><![CDATA[<h1 id="Graph-Models"><a href="#Graph-Models" class="headerlink" title="Graph Models"></a>Graph Models</h1><h2 id="DGM"><a href="#DGM" class="headerlink" title="DGM"></a>DGM</h2><h2 id="UGM-MRF-Markov-Random-Field"><a href="#UGM-MRF-Markov-Random-Field" class="headerlink" title="UGM (MRF: Markov Random Field)"></a>UGM (MRF: Markov Random Field)</h2><p>Markov Property:</p>
<ul>
<li>Global: no path between</li>
<li>Local: condition upon Markov blanket</li>
<li>Pair: given the rest, no direct edges<a id="more"></a>
</li>
</ul>
<p>Intersection Lemma:</p>
<script type="math/tex; mode=display">X\perp Y|\{W,Z\}\quad + \quad X\perp W|\{Y,Z\}\quad\to\quad X\perp\{Y,W\}|Z</script><p>$X\perp Y|\{W,Z\}$: $p(X,Y,Z,W)=f_{XWZ}(X,W,Z)f_{WYZ}(W,Y,Z)$<br>$X\perp W|\{Y,Z\}$: $p(X,Y,Z,W)=g_{XYZ}(X,Y,Z)g_{WYZ}(W,Y,Z)$<br>$X\perp\{Y,W\}|Z$: $p(X,Y,Z,W)=\mu_{XZ}(X,Z)\mu_{WYZ}(W,Y,Z)$<br><strong>Functions are valid factorizations, or a graph is a valid representation of distribution, if all independencies encoded in this graph is a subset of the distribution.</strong></p>
<p>MRF does <strong>not preserve the local probabilitic interpretations</strong>(conditional probabilities like parent-child relation in DGM), but still has all important representation of the joint distribution.</p>
<p>$X_i$, $X_j$ that are not directly linked are independent given all the rest nodes, which means they should appear in different factors of the factorization of joint distribution.<br>In contrast, all nodes in the same maximal clique should appear together in one local function $\psi(x_c)$.</p>
<p>Joint probability can be factorized as the product of potential functions for all cliques $C$:</p>
<script type="math/tex; mode=display">p(y|\theta)=\frac{1}{Z(\theta)}\prod_{c\in C}\psi_c(y_c|\theta_c)</script><p>Log-linear potetial is defined as:</p>
<script type="math/tex; mode=display">log\psi_c(y_c|\theta)=\phi(y_c)^T\theta_c</script><p><strong>The cliques can be choosen as maximal cliques, pairwise cliques(each edge forms a clique) and canonical cliques(all possible cliques).</strong></p>
<ul>
<li>Generative Models: Likelihood — $p(x|C_k)$</li>
<li>Discriminative Models: Likelihood — $p(C_k|x)$</li>
</ul>
<h2 id="CRF-Conditional-Random-Field-a-special-kind-of-UGM-discriminative-model"><a href="#CRF-Conditional-Random-Field-a-special-kind-of-UGM-discriminative-model" class="headerlink" title="CRF: Conditional Random Field (a special kind of UGM, discriminative model)"></a>CRF: Conditional Random Field (a special kind of UGM, discriminative model)</h2><p>UGMs condition upon a global input $x$, $x_1,…,x_n$ are fully connected to each other:</p>
<script type="math/tex; mode=display">p(y|x,w)=\frac{1}{Z(x,w)}\prod_{c\in C}\psi_c(y_c|x,w_c)</script><script type="math/tex; mode=display">log\psi_c(y_c|x,w)=\phi(y_c,x)^Tw_c</script><p>So we have:</p>
<script type="math/tex; mode=display">\begin{aligned}
p(y|x,w)&=\frac{1}{Z(x,w)}\psi(y|x,w)
\\&=\frac{exp(\phi(y,x_1,...,x_n)^Tw)}{\sum_y exp(\phi(y,x_1,...,x_n)^Tw)}
\\&=\frac{exp(\sum_n \phi(y,x_n)w_n)}{\sum_y exp(\sum_n \phi(y,x_n)w_n)}
\end{aligned}</script><p>where:</p>
<script type="math/tex; mode=display">\phi(y=1,x_n)w_n=I(y^+)x_nw_n^+</script><script type="math/tex; mode=display">\phi(y=0,x_n)w_n=I(y^-)x_nw_n^-</script><p>$I()$ is an indicate function that is used to choose from $w_n^+$ and $w_n^-$ according to the value of input($y^+$ or $y^-$).</p>
<script type="math/tex; mode=display">\begin{aligned}
p(y^+|x)&=\frac{exp(w_+^Tx)}{exp(w_+^Tx)+exp(w_-^Tx)}\qquad w'=w_+-w_-
\\&=\frac{1}{1+exp(-w^{'T}x)}\qquad Logistic Regression!
\end{aligned}</script><p>The equations above indicate that CRF can be transformed into a DNN when $w_+^Tx$ is a non-linear function $f(x;w)$: </p>
<script type="math/tex; mode=display">p(y+|x)=\sigma(f(x;w))</script><p>The sigmoid function can be replaced by the softmax function for multi-class classification.<br><img src="/images/CS5340/3/models_conpare.png" alt="models_conpare"></p>
<ul>
<li>Benefits of CRF:<br>Make use of observations(data labels)<br>Make the model ata-dependent(make the latent labels depend on global properties )</li>
<li>Shortbacks of CRF:<br>Require labeled training data<br>Learning is slower </li>
</ul>
<h1 id="Inference-for-marginalization-conditional-probability-or-maximum-probability"><a href="#Inference-for-marginalization-conditional-probability-or-maximum-probability" class="headerlink" title="Inference for marginalization/conditional probability or maximum probability"></a>Inference for marginalization/conditional probability or maximum probability</h1><h2 id="Variable-Elimination-query-sensitive"><a href="#Variable-Elimination-query-sensitive" class="headerlink" title="Variable Elimination (query sensitive)"></a>Variable Elimination (query sensitive)</h2><h2 id="Junction-Tree-Algorithm"><a href="#Junction-Tree-Algorithm" class="headerlink" title="Junction Tree Algorithm"></a>Junction Tree Algorithm</h2><h2 id="Factor-Tree-solve-the-loopy-problem"><a href="#Factor-Tree-solve-the-loopy-problem" class="headerlink" title="Factor Tree (solve the loopy problem)"></a>Factor Tree (solve the loopy problem)</h2><h2 id="Cluster-Graph-convert-to-Junction-Tree"><a href="#Cluster-Graph-convert-to-Junction-Tree" class="headerlink" title="Cluster Graph (convert to Junction Tree)"></a>Cluster Graph (convert to Junction Tree)</h2><h1 id="Parameter-Learning"><a href="#Parameter-Learning" class="headerlink" title="Parameter Learning"></a>Parameter Learning</h1>]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>比较排序 - 2：归并 timsort 桶排序 圈排序</title>
    <url>/2020/08/03/Comparison%20Sorts-2/</url>
    <content><![CDATA[<p><strong>完整排序算法代码：<a href="https://github.com/huangliu0909/Sort-Algorithms">https://github.com/huangliu0909/Sort-Algorithms</a></strong></p>
<h1 id="归并排序-Merge-sort"><a href="#归并排序-Merge-sort" class="headerlink" title="归并排序 Merge sort"></a>归并排序 Merge sort</h1><p>分治法，每次将一个数组均分成两部分，对这两部分进行merge。<a id="more"></a><br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">merge_sort</span><span class="params">(<span class="keyword">int</span>[] array, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(start &lt; end) &#123;</span><br><span class="line">			<span class="keyword">int</span> mid = (end + start)/<span class="number">2</span>;</span><br><span class="line">			merge_sort(array, start, mid);</span><br><span class="line">			merge_sort(array, mid + <span class="number">1</span>, end);</span><br><span class="line">			merge(array, start, mid, end);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span>[] array, <span class="keyword">int</span> start, <span class="keyword">int</span> mid, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span>[] left = Arrays.copyOfRange(array, start, mid + <span class="number">1</span>);</span><br><span class="line">		<span class="keyword">int</span>[] right = Arrays.copyOfRange(array, mid + <span class="number">1</span>, end + <span class="number">1</span>);</span><br><span class="line">		<span class="comment">//Arrays.copyOfRange左闭右开</span></span><br><span class="line">		<span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>, count = start;</span><br><span class="line">		<span class="keyword">while</span>(i != left.length &amp;&amp; j != right.length) &#123;</span><br><span class="line">			<span class="keyword">if</span>(left[i] &lt; right[j]) &#123;</span><br><span class="line">				array[count] = left[i];</span><br><span class="line">				count ++;</span><br><span class="line">				i ++;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">else</span> &#123;</span><br><span class="line">				array[count] = right[j];</span><br><span class="line">				count ++;</span><br><span class="line">				j ++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//更新count和i或者j</span></span><br><span class="line">		<span class="keyword">while</span>(i &lt; left.length) &#123;</span><br><span class="line">			array[count] = left[i];</span><br><span class="line">			i ++;</span><br><span class="line">			count ++;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">while</span>(j &lt; right.length) &#123;</span><br><span class="line">			array[count] = right[j];</span><br><span class="line">			j ++;</span><br><span class="line">			count ++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">```	</span><br><span class="line"></span><br><span class="line"># Timsort</span><br><span class="line">**Used in Java’s Arrays.sort() as well as Python’<span class="function">s <span class="title">sorted</span><span class="params">()</span> and <span class="title">sort</span><span class="params">()</span>.**</span></span><br><span class="line"><span class="function">混合自归并排序和插入排序。当数组中元素个数超过64，插入排序效率较低。Timsort中已经排序好的子数组被称作run。规定一个minRun的大小，即：如果一个run大小小于minRun，将它的后一个元素插入到合适的位置形成minRun。对于每个排序好的minRun，进行merge sort，初始size </span>= run，size+= <span class="number">2</span>* size。</span><br><span class="line">```Java</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">tim_sort</span><span class="params">(<span class="keyword">int</span>[] array, <span class="keyword">int</span> RUN)</span>  </span></span><br><span class="line"><span class="function">    </span>&#123; </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; array.length; i += RUN)  </span><br><span class="line">        &#123; </span><br><span class="line">        	insertion_sort(array, i, Math.min((i + RUN), (array.length - <span class="number">1</span>))); </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> size = RUN; size &lt; array.length; size = <span class="number">2</span> * size)  </span><br><span class="line">        &#123; </span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> left = <span class="number">0</span>; left &lt; array.length; left += <span class="number">2</span> * size)  </span><br><span class="line">            &#123; </span><br><span class="line">                <span class="keyword">int</span> mid = left + size - <span class="number">1</span>; </span><br><span class="line">                <span class="keyword">int</span> right = Math.min((left + <span class="number">2</span> * size - <span class="number">1</span>), (array.length - <span class="number">1</span>)); </span><br><span class="line">                merge(array, left, mid, right); </span><br><span class="line">            &#125; </span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="桶排序-Bucket-sort"><a href="#桶排序-Bucket-sort" class="headerlink" title="桶排序 Bucket sort"></a>桶排序 Bucket sort</h1><p>原理是将数组的元素分布(scatter)到多个桶中，然后每个桶被单独排序，最后收集(gather)桶中元素得到结果。<br>时间复杂度：最优：$\Omega (n+k)$，最坏：$O(n^2)$，平均：$\Theta(n+k)$<br>最坏空间复杂度：$O(n*k)$<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[]  heapsort(<span class="keyword">int</span>[] array) &#123;</span><br><span class="line">	<span class="keyword">int</span>[] h = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; array.length; i ++) &#123;</span><br><span class="line">		h = arrAppend(h, array[i]);</span><br><span class="line">		<span class="keyword">int</span> k = i;</span><br><span class="line">		<span class="keyword">while</span>(h[k] &gt; h[(<span class="keyword">int</span>) Math.floor(k/<span class="number">2</span>)]) &#123;</span><br><span class="line">			swap(h, k, (<span class="keyword">int</span>) Math.floor(k/<span class="number">2</span>));</span><br><span class="line">			k = (<span class="keyword">int</span>) Math.floor(k/<span class="number">2</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	swap(h, <span class="number">0</span>, h.length - <span class="number">1</span>);</span><br><span class="line">	<span class="keyword">return</span> heapify(h, h.length - <span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] heapify(<span class="keyword">int</span>[] array, <span class="keyword">int</span> i) &#123;</span><br><span class="line">	<span class="keyword">if</span>(<span class="number">0</span> &lt; i) &#123;</span><br><span class="line">		<span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">while</span>(k + <span class="number">2</span> &lt;= i) &#123;</span><br><span class="line">			<span class="keyword">if</span>(array[k + <span class="number">1</span>] &lt; array[k + <span class="number">2</span>]) &#123;</span><br><span class="line">				swap(array, k, k + <span class="number">2</span>);</span><br><span class="line">				k = k + <span class="number">2</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">else</span> &#123;</span><br><span class="line">				swap(array, k, k + <span class="number">1</span>);</span><br><span class="line">				k = k + <span class="number">1</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		swap(array, <span class="number">0</span>, i);</span><br><span class="line">		<span class="keyword">return</span> heapify(array, i - <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">return</span> array;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="圈排序-Cycle-sort"><a href="#圈排序-Cycle-sort" class="headerlink" title="圈排序 Cycle sort"></a>圈排序 Cycle sort</h1><blockquote>
<p><a href="https://www.geeksforgeeks.org/cycle-sort/">https://www.geeksforgeeks.org/cycle-sort/</a><br><a href="https://www.raychase.net/1814">https://www.raychase.net/1814</a></p>
</blockquote>
<p>交换次数最少的排序，与计数排序类似，但圈排序只给那些需要计数的数字计数。<br>圈排序又叫循环排序，是不稳定的原地排序算法。选择排序本来已经让写元素的次数变得很少了，但是圈排序可以让它更少：试想一下这种情况，位置 1 的元素应该放当前位置 3 的元素，选择排序把当前位置 3 的元素和位置 1 的元素互换——那么原本位置 3 的元素，直接就到达了最后它应该在的位置；但是原本位置 1 的元素呢，却有可能会被再换位置，也就是说，选择排序依然可能存在着某元素被换了不止一次地方。圈排序真正使得任一元素要么不动，要动也最多动一次。圈排序最好和最坏的时间复杂度都是 O(n2)，但是因为最小的写入次数，对于在写入非常慢的介质中排序来说，会有它的价值（例如在某些 Flash 闪存中）。</p>
<p>完整排序算法代码：<a href="https://github.com/huangliu0909/Sort-Algorithms">https://github.com/huangliu0909/Sort-Algorithms</a></p>
]]></content>
      <categories>
        <category>学习笔记：算法</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title>比较排序 - 3：快排 堆排 平滑 内省 奇偶</title>
    <url>/2020/08/04/Comparison%20Sorts-3/</url>
    <content><![CDATA[<p><strong>完整排序算法代码：<a href="https://github.com/huangliu0909/Sort-Algorithms">https://github.com/huangliu0909/Sort-Algorithms</a></strong></p>
<h1 id="快速排序-Quicksort"><a href="#快速排序-Quicksort" class="headerlink" title="快速排序 Quicksort"></a>快速排序 Quicksort</h1><p>快速排序是一种分治算法，不断找到pivot元素，将比它小的放在它左边，比它大的放在右边。双指针从头、从末尾分别开始与pivot比较，如果满足则移动，如果不满足则交换。<a id="more"></a><br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Quicksort</span><span class="params">(<span class="keyword">int</span>[] array, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(end &gt; start) &#123;</span><br><span class="line">		<span class="keyword">int</span> pivot = array[end];</span><br><span class="line">		<span class="keyword">int</span> i = start, j = end;</span><br><span class="line">		<span class="keyword">while</span>(i &lt; j) &#123;</span><br><span class="line">			swap(array, i, j);</span><br><span class="line">			<span class="keyword">if</span>(array[i] &lt; pivot) i ++;</span><br><span class="line">			<span class="keyword">if</span>(array[j] &gt; pivot) j --;</span><br><span class="line">		&#125;</span><br><span class="line">		Quicksort(array, start, i - <span class="number">1</span>);  <span class="comment">// Before pi</span></span><br><span class="line">		Quicksort(array, i + <span class="number">1</span>, end); <span class="comment">// After pi</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="堆排序-Heapsort"><a href="#堆排序-Heapsort" class="headerlink" title="堆排序 Heapsort"></a>堆排序 Heapsort</h1><p>根据输入数据构造一个最大堆（堆：有序二叉树）将root(index = 0)与数组的最后一个点交换(index = len - 1)，即将最大值放在了数组的最后，整理前len - 1个使之成为最大堆，对前len - 1个数重复以上步骤。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] heapsort(<span class="keyword">int</span>[] array) &#123;</span><br><span class="line">	<span class="keyword">int</span>[] h = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; array.length; i ++) &#123;</span><br><span class="line">		h = arrAppend(h, array[i]);</span><br><span class="line">		<span class="keyword">int</span> k = i;</span><br><span class="line">		<span class="keyword">while</span>(h[k] &gt; h[(<span class="keyword">int</span>) Math.floor(k/<span class="number">2</span>)]) &#123;</span><br><span class="line">			swap(h, k, (<span class="keyword">int</span>) Math.floor(k/<span class="number">2</span>));</span><br><span class="line">			k = (<span class="keyword">int</span>) Math.floor(k/<span class="number">2</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	swap(h, <span class="number">0</span>, h.length - <span class="number">1</span>);</span><br><span class="line">	<span class="keyword">return</span> heapify(h, h.length - <span class="number">2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[] heapify(<span class="keyword">int</span>[] array, <span class="keyword">int</span> i) &#123;</span><br><span class="line">	<span class="keyword">if</span>(<span class="number">0</span> &lt; i) &#123;</span><br><span class="line">		<span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">while</span>(k + <span class="number">2</span> &lt;= i) &#123;</span><br><span class="line">			<span class="keyword">if</span>(array[k + <span class="number">1</span>] &lt; array[k + <span class="number">2</span>]) &#123;</span><br><span class="line">				swap(array, k, k + <span class="number">2</span>);</span><br><span class="line">				k = k + <span class="number">2</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">else</span> &#123;</span><br><span class="line">				swap(array, k, k + <span class="number">1</span>);</span><br><span class="line">				k = k + <span class="number">1</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		swap(array, <span class="number">0</span>, i);</span><br><span class="line">		<span class="keyword">return</span> heapify(array, i - <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">return</span> array;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="平滑排序-Smoothsort"><a href="#平滑排序-Smoothsort" class="headerlink" title="平滑排序 Smoothsort"></a>平滑排序 Smoothsort</h1><blockquote>
<p><a href="https://en.wikipedia.org/wiki/Smoothsort">https://en.wikipedia.org/wiki/Smoothsort</a></p>
</blockquote>
<p>是堆排序的一个变种</p>
<h1 id="内省排序-Introsort"><a href="#内省排序-Introsort" class="headerlink" title="内省排序 Introsort"></a>内省排序 Introsort</h1><blockquote>
<p><a href="https://en.wikipedia.org/wiki/Introsort">https://en.wikipedia.org/wiki/Introsort</a></p>
</blockquote>
<p>procedure sort(A : array):<br>    let maxdepth = ⌊log(length(A))⌋ × 2<br>    introsort(A, maxdepth)</p>
<p>procedure introsort(A, maxdepth):<br>    n ← length(A)<br>    if n ≤ 1:<br>        return  // base case<br>    else if maxdepth = 0:<br>        heapsort(A)<br>    else:<br>        p ← partition(A)<br>        // assume this function does pivot selection, p is the final position of the pivot<br>        introsort(A[0:p-1], maxdepth - 1)<br>        introsort(A[p+1:n], maxdepth - 1)</p>
<h1 id="奇偶排序-Odd–even-sort"><a href="#奇偶排序-Odd–even-sort" class="headerlink" title="奇偶排序 Odd–even sort"></a>奇偶排序 Odd–even sort</h1><p>通过比较数组中相邻的（奇-偶）位置数字对，如果该奇偶对是错误的顺序（第一个大于第二个），则交换。可以使用并行计算，每个处理器处理一个值</p>
]]></content>
      <categories>
        <category>学习笔记：算法</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title>计数排序 &amp; 基数排序</title>
    <url>/2020/08/01/Counting%20Sorts/</url>
    <content><![CDATA[<h1 id="计数排序-Counting-sort"><a href="#计数排序-Counting-sort" class="headerlink" title="计数排序 Counting sort"></a>计数排序 Counting sort</h1><blockquote>
<p>“In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. “   From Wikipedia</p>
</blockquote>
<p><strong>完整排序算法代码：<a href="https://github.com/huangliu0909/Sort-Algorithms">https://github.com/huangliu0909/Sort-Algorithms</a></strong><br><a id="more"></a></p>
<p>计数排序，有别于比较排序，不需要进行数据之间的比较、交换。适用于不同数值总数较小的序列。<br>首先，找到数组的最大值max和最小值min，构造一个count数组，大小为max-min+1，初始化为0.<br>对于原始array中的每个数a-min作为count数组的index进行计数。<br>调整count数组使得每个数值为这个index在result中出现的最后一个位置。<br><strong>对原始array从右向左进行遍历</strong>，对每个元素a找到count数组中对应的位置填入result数组，并且将count数组中的该数值减一，因为上一个位置已经被填入了，下次被填入需要向前挪一位。<br>计数排序时间复杂度为$O(n)$。计数排序通常被使用为基数排序(Radix Sort)的一个步骤。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Counting_Sort</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		 <span class="keyword">int</span>[] array = <span class="keyword">new</span> <span class="keyword">int</span>[] &#123;<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line">		 <span class="keyword">int</span> max = Integer.MIN_VALUE;</span><br><span class="line">		 <span class="keyword">int</span> min = Integer.MAX_VALUE;</span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> a : array)  <span class="keyword">if</span>(a &gt; max) max = a; <span class="comment">//find max value</span></span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> a : array)  <span class="keyword">if</span>(a &lt; min) min = a; <span class="comment">//find min value</span></span><br><span class="line">		 <span class="keyword">int</span> size = max - min + <span class="number">1</span>; <span class="comment">//the num of distinct values</span></span><br><span class="line">		 <span class="keyword">int</span>[] count = <span class="keyword">new</span> <span class="keyword">int</span>[size]; </span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i ++) count[i] = <span class="number">0</span>; <span class="comment">//initialize</span></span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> a : array) count[a - min] ++; <span class="comment">//counting</span></span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; size; i ++) count[i] += count[i - <span class="number">1</span>]; <span class="comment">//find last index </span></span><br><span class="line">		 <span class="keyword">int</span>[] result = <span class="keyword">new</span> <span class="keyword">int</span>[array.length];</span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> i = array.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i --) &#123;</span><br><span class="line">			 <span class="keyword">int</span> a = array[i];</span><br><span class="line">			 result[count[a] - <span class="number">1</span>] = a;</span><br><span class="line">			 count[a]--;</span><br><span class="line">		 &#125;</span><br><span class="line">		 System.out.println(Arrays.toString(result));</span><br><span class="line">		 <span class="comment">//[0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 5, 6, 7, 8, 9]</span></span><br><span class="line">	  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="基数排序-Radix-Sort"><a href="#基数排序-Radix-Sort" class="headerlink" title="基数排序 Radix Sort"></a>基数排序 Radix Sort</h1><p>通过数组的最大值确定位数pos的范围(max/pos != 0)，从个位(pos=1)开始根据每一位(<strong>(a/pos)%10</strong>)进行计数排序，每一轮计数排序结束，pos*=10。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Radix_Sort</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">		 <span class="keyword">int</span>[] array = <span class="keyword">new</span> <span class="keyword">int</span>[] &#123;<span class="number">123</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">678</span>, <span class="number">345</span>, <span class="number">890</span>, <span class="number">234</span>, <span class="number">789</span>, <span class="number">23</span>, <span class="number">49</span>, <span class="number">843</span>, <span class="number">56</span>, <span class="number">234</span>&#125;;</span><br><span class="line">		 <span class="keyword">int</span> max = Integer.MIN_VALUE;</span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> a : array)  <span class="keyword">if</span>(a &gt; max) max = a; <span class="comment">//find max value</span></span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> pos = <span class="number">1</span>; max/pos != <span class="number">0</span>; pos*=<span class="number">10</span>) &#123;</span><br><span class="line">			 count_sort(array, pos);		 </span><br><span class="line">		&#125;</span><br><span class="line">		 System.out.println(Arrays.toString(array));</span><br><span class="line">		 <span class="comment">//[2, 23, 34, 49, 56, 123, 234, 234, 345, 678, 789, 843, 890]</span></span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">count_sort</span><span class="params">(<span class="keyword">int</span>[] array, <span class="keyword">int</span> pos)</span> </span>&#123;</span><br><span class="line">		 <span class="keyword">int</span>[] count = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">10</span>]; </span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i ++) count[i] = <span class="number">0</span>; <span class="comment">//initialize</span></span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> a : array) count[(a/pos)%<span class="number">10</span>] ++; <span class="comment">//counting</span></span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i ++) count[i] += count[i - <span class="number">1</span>]; <span class="comment">//find last index </span></span><br><span class="line">		 <span class="keyword">int</span>[] result = <span class="keyword">new</span> <span class="keyword">int</span>[array.length];</span><br><span class="line">		 <span class="keyword">for</span>(<span class="keyword">int</span> i = array.length - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i --) &#123;</span><br><span class="line">			 <span class="keyword">int</span> a = array[i];</span><br><span class="line">			 result[count[(a/pos)%<span class="number">10</span>] - <span class="number">1</span>] = a;</span><br><span class="line">			 count[(a/pos)%<span class="number">10</span>]--;</span><br><span class="line">		 &#125;</span><br><span class="line">		 <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; array.length; i++) array[i] = result[i]; </span><br><span class="line">		 </span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>完整排序算法代码：<a href="https://github.com/huangliu0909/Sort-Algorithms">https://github.com/huangliu0909/Sort-Algorithms</a></p>
]]></content>
      <categories>
        <category>学习笔记：算法</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Sort</tag>
      </tags>
  </entry>
  <entry>
    <title>分治 动态规划 贪心</title>
    <url>/2020/07/18/Dynamic_Programming/</url>
    <content><![CDATA[<h2 id="分治-Divide-and-Conquer"><a href="#分治-Divide-and-Conquer" class="headerlink" title="分治 Divide and Conquer"></a>分治 Divide and Conquer</h2><p>将问题划分为互不相交的子问题。递归地求解子问题的解，再将这些解组合起来，得到原问题的最优解。（eg. 归并排序算法 $T(n)$）</p>
<ol>
<li><strong>分解</strong>原问题为若干子问题，这些子问题是原问题的规模较小的实例。<br>eg. 分解带排序的n个元素的序列成各具n/2个元素的两个子序列。 - - -$D(n)=\theta (1)$</li>
<li><strong>解决</strong>这些子问题。<br>如果子问题规模足够小（<strong>基本情况</strong>），就直接求解，否则（<strong>递归情况</strong>）使用递归进行求解。<br>eg. 使用归并排序递归地排序两个子序列。 - - -$2T(n/2)$</li>
<li><strong>合并</strong>这些子问题的解获得原问题的解。<br>eg. 合并两个已排序的子序列以产生已排序的答案。 - - -$C(n)=\theta (n)$<a id="more"></a>
</li>
</ol>
<p>将原问题划分为a个问题，每个问题的大小是n/b：$T(n)=aT(n/b)+D(n)+C(n)$。<br>递归式：一个等式或不等式，通过更小的输入上的函数值来描述一个函数。（用自身定义自身）<br>求解递归式的方法：</p>
<ol>
<li><strong>代入法 Substitution</strong> 猜测解的形式，减少不确定范围，用数学归纳法求出解中常熟并证明这个界是正确的。<br>$T(n)=2T(n/2)+n\to T(n)=O(nlgn)$<br>$T(n)=T(n-1)+n\to T(n)=O(n^2)$<br>$T(n)=T(n/2)+1\to T(n)=O(lgn)$</li>
<li><strong>递归树法 Iteration</strong> 将递归式转换为一棵树，其结点表示不同层次的递归调用产生的代价。然后采用边界和技术来求解递归式。</li>
<li><strong>主方法 Master</strong>  对于递归式 $T(n)=aT(n/b)+f(n)$：<br>a. 若对于某个常数 $\epsilon &gt;0$ 有 $f(n)=O(n^{log_ba-\epsilon })$，则 $T(n)=\theta (n^{log_ba})$。<br>b. 若 $f(n)=\theta (n^{log_ba})$，则 $T(n)=\theta (n^{log_ba}lgn)$。（适用于分治的递归式）<br>c. 若对某个常数$\epsilon &gt;0$ 有$f(n)=\Omega (n^{log_ba+\epsilon })$，且对某个常数$c&lt;1$和所有足够大的n有$af(n/b)\le cf(n)$，则 $T(n)=\theta (f(n))$。</li>
</ol>
<h2 id="动态规划-Dynamic-Programming"><a href="#动态规划-Dynamic-Programming" class="headerlink" title="动态规划 Dynamic Programming"></a>动态规划 Dynamic Programming</h2><p><strong>优化子结构</strong>：一个问题包含其子问题的最优解<br><strong>重叠子问题</strong>：利用递归算法反复求解相同的子问题<br><strong>动态规划算法无后效性，总是能够得到全局最优解。</strong><br>动态规划算法对每个子问题只求解一次并将其解记录在表格中。任何子问题都会等到它依赖的子问题已经求解结束才会进行求解。动态规划算法运行时间相当于每个子问题的求解时间之和。</p>
<ol>
<li>寻找最优解的特征结构<br>证明问题含有<strong>优化子结构</strong><br>eg. 最短路径问题具有优化子结构，而最长简单路径问题（不允许成环）不具有优化子结构。根本原因在于求解最长路径子问题时用到的某些资源（顶点），导致这些资源在求解其它子问题时不可用（若两个最长路径子问题旋律共同的顶点则会成环）。<br>a. 证明问题的最优解的第一个组成部分是做出一个选择。例如选择钢条的第一次切割位置、选择矩阵链的划分位置。做出这次选择会产生一个或者多个子问题。<br>b. 对于一个给定问题，在其可能的第一步选择中，假定已经知道哪种选择才会得到最优解。<br>c. 给定可获得最优解的选择之后，确定这个选择会产生哪些子问题，以及如何最好地刻画子问题空间。<br>d. 证明：<u>作为构成原问题最优解的组成部分，每个子问题的解就是它本身的最优解。（反证）</u></li>
<li>递归地定义最优解的值<br>找到问题规模为n的最优解与规模小于n的子问题的最优解之间的数量关系。</li>
<li>计算最优解的值，通常采用<strong>自底向上</strong>的方法<br>自底向上方法使得任何子问题的求解只依赖于规模更小的问题的求解，也可以用过自顶向下加入备忘机制来实现。<br>计算最优解的过程中可以重构解来记录最优解的获取方式。</li>
<li>利用计算出的信息构造一个最优解</li>
</ol>
<p>eg. 最长公共子序列、钢条切割、最优二叉搜索树</p>
<h2 id="贪心-Greedy"><a href="#贪心-Greedy" class="headerlink" title="贪心 Greedy"></a>贪心 Greedy</h2><p><strong>优化子结构</strong>：问题的优化解包含了子问题的最优解。<br><strong>贪心选择性(greedy-choice property)</strong>：全局优化解可以通过局部优化选择得到。即：进行选择时可以直接做出在当前问题中看来最优的选择而不必考虑子问题的解。即：存在一个最优解是以贪心选择开始的。<br>证明贪心选择性：先考察一个全局最优解，如果全局最优解可以转换成以贪心选择开始，则已得证，如果不能，则将这个全局最优解的开始替换成贪心选择，从而可以得到一个更优的解。因此全局最优解应该选择这个贪心选择做为第一次选择，从而得到一个规模更小的子问题，<u>通过数学归纳法可证每次都对子问题进行贪心选择可以得到原问题的最优解。</u><br><strong>贪心算法并不能保证得到全局最优解。</strong><br>通过贪心选择性的证明，我们说明了存在一个全局最优解是以贪心选择开始的，<u>但我们没有证明所有贪心选择开始的解都是全局最优解，</u>因此在某些问题上使用贪心算法可能无法得到全局最优解。<br>贪心算法与动态规划最大的不同在于，贪心并不是首先寻找子问题的最优解然后在其中进行选择（这种选择通常依赖于子问题的解），而是直接做出一次贪心选择（局部最优解）然后求解剩下的唯一子问题，不必求解所有可能相关的子问题。即：贪心算法总是做出局部最优的选择，希望这样的选择可以最终达到全局最优，这种选择可能依赖于之前做出的选择，但不会依赖于任何将来的选择或者子问题的解。<br>贪心算法通常采用<strong>自顶向下</strong>的设计：做出一个选择，然后求解剩下那个子问题。而不是自底向上求解出很多子问题再做出选择。<em>每个贪心算法下，几乎总有一个更繁琐的动态规划算法。</em></p>
<ol>
<li>将最优化问题转化为这样的形式：对其做出一次选择之后，只剩下一个子问题需要求解。</li>
<li>证明做出贪心选择后，原问题总是存在最优解，<strong>即贪心总是安全的</strong>。<br>进行贪心选择时，可以通过数据预处理（排序）或者使用合适的数据结构（优先队列）来使得贪心选择更快速高效。</li>
<li>证明做出贪心选择后，剩余的子问题的最优解与该贪心选择组合即可得到原问题的最优解，这样就得到了<strong>最优子结构</strong>。</li>
</ol>
<p>eg. 最小生成树、Dijkstra算法、赫夫曼编码、任务调度问题、部分背包问题</p>
<blockquote>
<p>参考：《算法导论》机械工业出版社 第2、4、15、16章</p>
</blockquote>
]]></content>
      <categories>
        <category>学习笔记：算法</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>并查集 Disjoint Set</title>
    <url>/2020/08/08/Disjointed_Set/</url>
    <content><![CDATA[<h2 id="并查集-Disjoint-Set"><a href="#并查集-Disjoint-Set" class="headerlink" title="并查集(Disjoint Set)"></a>并查集(Disjoint Set)</h2><p>互不相交的数据集合。用一个代表元素来识别每个集合，这个元素属于这个集合。基本操作如下：<br>Make-Set：建立一个新的单元素集合<br>Find：判断元素在哪个集合中。可用于判断两个元素是都属于同一集合。<br>Union：把一个集合中的元素并入另一个集合中。<br>该数据结构可用于确定无向图的连通分量：对每个顶点make-set，遍历每一条边对每条边的两顶点的所在set进行union，最终得到的set的集合就是无向图的连通分量。<br><a id="more"></a></p>
<h3 id="并查集的链表表示"><a href="#并查集的链表表示" class="headerlink" title="并查集的链表表示"></a>并查集的链表表示</h3><p>每个集合的对象包含head属性和tail属性，head属性指向第一个对象，tail属性指向最后一个对象。链表中每个对象包含一个集合成员、一个next指向下一个对象，一个head回到集合对象。此时make-set、find操作时间复杂度为$O(1)$，union的时间复杂第为$O(n)$，其中n为被合并的集合大小，union操作将改变这n个元素的head指针。<br><strong>一种加权合并启发式策略 weighted-union heuristic</strong>：每次都是将较小的链合并到较长的链中。<br><strong>一个具有m个make-set、union、find操作的序列需要消耗$O(m+nlgn)$的时间（n：make-set的个数）。</strong>：总共至多执行n-1次union操作，对某个元素x，x的指针每次被更新，说明x一定在一个较小的集合中，x被更新$[lgk]$次说明集合一定至少有k个成员了，因此每个元素所在集合最多被合并$[lgn]$次。总共花在union操作的时间为$O(nlgn)$。</p>
<h3 id="并查集的数组表示"><a href="#并查集的数组表示" class="headerlink" title="并查集的数组表示"></a>并查集的数组表示</h3><p>Make-Set：对n个元素创建一个长度为n的数组P，每个数组元素都是-1。$O(1)$<br>Find：查找这个元素x作为index存储的数值P(x)，如果是-1则返回自己，如果不是则返回Find(P(x))。$O(n)$<br>Union：将被合并的集合的代表元素的对应数值改为合并的集合的代表元素。$O(1)$<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">disjoint_set</span></span>&#123;</span><br><span class="line"> <span class="keyword">int</span>[] parent;</span><br><span class="line"> disjoint_set(<span class="keyword">int</span> n)&#123;</span><br><span class="line"> 	<span class="keyword">this</span>.parent = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line"> 	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i ++) &#123;</span><br><span class="line"> 		<span class="keyword">this</span>.parent[i] = -<span class="number">1</span>;</span><br><span class="line"> 	&#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">int</span> <span class="title">find_set</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line"> 	<span class="keyword">if</span>(<span class="keyword">this</span>.parent[x] &lt; <span class="number">0</span>) <span class="keyword">return</span> x;</span><br><span class="line"> 	<span class="keyword">return</span> find_set(<span class="keyword">this</span>.parent[x]);</span><br><span class="line"> &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span> x1, <span class="keyword">int</span> x2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> s1 = find_set(x1);</span><br><span class="line">  <span class="keyword">int</span> s2 = find_set(x2);</span><br><span class="line">  <span class="keyword">if</span>(s1 == s2) <span class="keyword">return</span>;</span><br><span class="line">        <span class="comment">//union by rank, using size as rank</span></span><br><span class="line">  <span class="keyword">if</span>(<span class="keyword">this</span>.parent[s1] &lt;= <span class="keyword">this</span>.parent[s2]) &#123;</span><br><span class="line">  	<span class="comment">//the set of x1 have more elements</span></span><br><span class="line">  	<span class="keyword">this</span>.parent[s1] += <span class="keyword">this</span>.parent[s2];</span><br><span class="line">  	<span class="keyword">this</span>.parent[s2] = s1;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">  	<span class="keyword">this</span>.parent[s2] += <span class="keyword">this</span>.parent[s1];</span><br><span class="line">  	<span class="keyword">this</span>.parent[s1] = s2;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//main function</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">       disjoint_set ds = <span class="keyword">new</span> disjoint_set(<span class="number">5</span>);</span><br><span class="line">       System.out.println(ds.find_set(<span class="number">3</span>)); <span class="comment">//output:3</span></span><br><span class="line">       ds.union(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">       ds.union(<span class="number">1</span>, <span class="number">3</span>);</span><br><span class="line">       System.out.println(ds.find_set(<span class="number">1</span>)); <span class="comment">//output:1</span></span><br><span class="line">       System.out.println(ds.find_set(<span class="number">2</span>)); <span class="comment">//output:1</span></span><br><span class="line">       System.out.println(ds.find_set(<span class="number">3</span>)); <span class="comment">//output:1</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="并查集森林-disjointed-set-forest-与启发式策略"><a href="#并查集森林-disjointed-set-forest-与启发式策略" class="headerlink" title="并查集森林 disjointed-set forest 与启发式策略"></a>并查集森林 disjointed-set forest 与启发式策略</h2><p>每个成员仅指向其父节点，每棵树的根就是集合的代表，根节点的父节点就是根节点本身。<br>Find-Set操作即沿着指向父节点的指针找到树根。这一通向根节点的简单路径上所访问的结点构成了<strong>查找路径(find path)</strong>。<br>Union操作即将一棵树的根节点指向另一棵树的根节点。<br>一个包含n-1个union操作的序列可以构造出一颗恰好含有n个结点的并查集森林。</p>
<h3 id="按秩合并-Union-by-Rank"><a href="#按秩合并-Union-by-Rank" class="headerlink" title="按秩合并 Union by Rank"></a>按秩合并 Union by Rank</h3><p>使具有较少节点的树的根指向具有较多结点的树的根。对于每个结点，维护一个秩(rank)用来表示该节点高度的一个上界。在union操作中可以让具有较小秩的根指向具有较大秩的根。</p>
<h3 id="路径压缩-Path-Expression"><a href="#路径压缩-Path-Expression" class="headerlink" title="路径压缩 Path Expression"></a>路径压缩 Path Expression</h3><p>指针策略可以使查找路径中每个结点直接指向根。路径压缩并不改变任何结点的秩。</p>
<h2 id="无向图中探测环"><a href="#无向图中探测环" class="headerlink" title="无向图中探测环"></a>无向图中探测环</h2><p>对每个顶点make-set，遍历每条边，找到当前边的两个顶点所在的集合，如果不同，则对这两个集合进行union操作；如果相同，则图中出现环。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">graph</span></span>&#123;</span><br><span class="line"> disjoint_set ds;</span><br><span class="line"> graph(<span class="keyword">int</span> v)&#123;</span><br><span class="line"> 	<span class="keyword">this</span>.ds = <span class="keyword">new</span> disjoint_set(v);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="function"><span class="keyword">void</span> <span class="title">addEdge</span><span class="params">(<span class="keyword">int</span> x1, <span class="keyword">int</span> x2)</span> </span>&#123;</span><br><span class="line"> 	System.out.println(<span class="string">"add edge: "</span> + String.valueOf(x1) + <span class="string">" "</span> + String.valueOf(x2));</span><br><span class="line"> 	<span class="keyword">int</span> s1 = <span class="keyword">this</span>.ds.find_set(x1);</span><br><span class="line">  <span class="keyword">int</span> s2 = <span class="keyword">this</span>.ds.find_set(x2);</span><br><span class="line">  <span class="keyword">if</span>(s1 == s2) System.out.println(<span class="string">"there is a circle"</span>);</span><br><span class="line">  <span class="keyword">this</span>.ds.union(x1, x2);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//main function</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     graph g = <span class="keyword">new</span> graph(<span class="number">3</span>);</span><br><span class="line">     g.addEdge(<span class="number">0</span>, <span class="number">1</span>); <span class="comment">//add edge: 0 1</span></span><br><span class="line">     g.addEdge(<span class="number">1</span>, <span class="number">2</span>); <span class="comment">//add edge: 1 2</span></span><br><span class="line">     g.addEdge(<span class="number">0</span>, <span class="number">2</span>); <span class="comment">//add edge: 0 2 there is a circle</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>学习笔记：数据结构</category>
      </categories>
      <tags>
        <tag>Disjoint Set</tag>
      </tags>
  </entry>
  <entry>
    <title>图算法</title>
    <url>/2020/08/08/Graph_Algorithms/</url>
    <content><![CDATA[<p><strong>完整代码见：<a href="https://github.com/huangliu0909/Graph-Algorithms">https://github.com/huangliu0909/Graph-Algorithms</a></strong></p>
<h2 id="基本图算法"><a href="#基本图算法" class="headerlink" title="基本图算法"></a>基本图算法</h2><h3 id="深度优先搜索-DFS"><a href="#深度优先搜索-DFS" class="headerlink" title="深度优先搜索 DFS"></a>深度优先搜索 DFS</h3><p>使用栈(First-In-Last-Out)，起始点入栈，当栈不为空时，pop，如果这个点未被访问，则标记为已访问，加入结果数组，将与这个点相连的顶点逐个入栈，循环直到栈为空。<a id="more"></a><br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	stack4DFS = <span class="keyword">new</span> Stack&lt;Integer&gt;();</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) </span><br><span class="line">		vertex[i] = -<span class="number">1</span>;</span><br><span class="line">	<span class="comment">//start from 0</span></span><br><span class="line">	stack4DFS.push(<span class="number">0</span>);</span><br><span class="line">	<span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(!stack4DFS.isEmpty()) &#123;</span><br><span class="line">		<span class="keyword">int</span> now = stack4DFS.pop();</span><br><span class="line">		<span class="keyword">if</span>(vertex[now] == -<span class="number">1</span>) &#123;</span><br><span class="line">			vertex[now] = <span class="number">1</span>;</span><br><span class="line">			res[count] = now;</span><br><span class="line">			count ++;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) &#123;</span><br><span class="line">				<span class="keyword">if</span>(edges[now][i] &gt; <span class="number">0</span>) &#123;</span><br><span class="line">					stack4DFS.push(i);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.print(Arrays.toString(res) + <span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="广度优先搜索-BFS"><a href="#广度优先搜索-BFS" class="headerlink" title="广度优先搜索 BFS"></a>广度优先搜索 BFS</h3><p>使用队列(Queue: First-In-First-Out)，将源结点插入队列，当队列非空：队列弹出作为当前遍历的值，将与这个点相连的其它结点都加入队列，重复以上步骤直到队列为空。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">BFS</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	queue4BFS = <span class="keyword">new</span> LinkedList&lt;Integer&gt;();</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) </span><br><span class="line">		vertex[i] = -<span class="number">1</span>;</span><br><span class="line">	<span class="comment">//start from 0</span></span><br><span class="line">	queue4BFS.add(<span class="number">0</span>);</span><br><span class="line">	vertex[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(!queue4BFS.isEmpty()) &#123;</span><br><span class="line">		<span class="keyword">int</span> now = queue4BFS.poll();</span><br><span class="line">		res[count] = now;</span><br><span class="line">		<span class="comment">//System.out.print(Arrays.toString(edges[now]));</span></span><br><span class="line">		count ++;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) &#123;</span><br><span class="line">			<span class="keyword">if</span>(edges[now][i] &gt; <span class="number">0</span> &amp;&amp; vertex[i] == -<span class="number">1</span>) &#123;</span><br><span class="line">				vertex[i] = <span class="number">1</span>;</span><br><span class="line">				queue4BFS.add(i);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.print(Arrays.toString(res) + <span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h3><p>有向无环图(Directed Acyclic Graph)的所有顶点的有序线性排列。不断输出并删除入度为0的结点，如果最终无法输出所有结点则该图中存在环。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">topological_sort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) </span><br><span class="line">		vertex[i] = -<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(count != edges.length) &#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) &#123;</span><br><span class="line">			<span class="keyword">if</span>(vertex[i] == -<span class="number">1</span>) &#123;</span><br><span class="line">				<span class="keyword">int</span> flag = <span class="number">0</span>;</span><br><span class="line">				<span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; edges.length; j ++) &#123;</span><br><span class="line">					<span class="keyword">if</span>(edges[j][i] != Integer.MAX_VALUE) &#123;</span><br><span class="line">						flag = <span class="number">1</span>;</span><br><span class="line">						<span class="keyword">break</span>;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">if</span>(flag == <span class="number">0</span>) &#123; <span class="comment">//找到入度为0的结点i</span></span><br><span class="line">					res[count] = i;</span><br><span class="line">					vertex[i] = <span class="number">1</span>; <span class="comment">//标记为已访问</span></span><br><span class="line">					<span class="keyword">break</span>;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) </span><br><span class="line">			edges[res[count]][i] = Integer.MAX_VALUE;</span><br><span class="line">		count ++;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.println(Arrays.toString(res));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="最小生成树-Minimum-Spanning-Tree"><a href="#最小生成树-Minimum-Spanning-Tree" class="headerlink" title="最小生成树 Minimum Spanning Tree"></a>最小生成树 Minimum Spanning Tree</h2><p>生成树：连接图的所有结点的无环连通子图，顶点个数=图顶点个数，边个数=图顶点个数-1。减少一条边会使得ST不连通，增加一条边会使得ST出现环。非连通图不存在生成树ST。<br>最小生成树：边的总权重和最小的生成树。可以通过不断删除最大的e-n+1条边来得到MST。</p>
<h3 id="Kruskal算法"><a href="#Kruskal算法" class="headerlink" title="Kruskal算法"></a>Kruskal算法</h3><p><strong>往集合里加入边</strong><br>对每条边进行排序。使用Find-Set(u)返回u所在树的代表结点，使Union对两棵树进行合并。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Kruskal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	vertex = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; vertex.length; i ++) vertex[i] = i; <span class="comment">//每个结点的父节点都初始化为自己</span></span><br><span class="line">	<span class="keyword">int</span>[][] e = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>][<span class="number">3</span>]; <span class="comment">//将边整理为（u，v，w）的形式</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) &#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; edges.length; j ++) &#123;</span><br><span class="line">			<span class="keyword">if</span>(edges[i][j] &lt; Integer.MAX_VALUE)</span><br><span class="line">				e = arrAppend(e, <span class="keyword">new</span> <span class="keyword">int</span>[] &#123;i, j, edges[i][j]&#125;);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	 Arrays.sort(e, <span class="keyword">new</span> edgeComp()); </span><br><span class="line">	 <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; e.length; i ++) &#123;</span><br><span class="line">		 <span class="comment">//System.out.println(Arrays.toString(e[i]));</span></span><br><span class="line">		 <span class="keyword">if</span>(find_set(vertex, e[i][<span class="number">0</span>]) != find_set(vertex, e[i][<span class="number">1</span>])) &#123;</span><br><span class="line">			 union(vertex, e[i][<span class="number">0</span>], e[i][<span class="number">1</span>]);</span><br><span class="line">			 System.out.print(e[i][<span class="number">0</span>]);</span><br><span class="line">			 System.out.print(<span class="string">" -- "</span>);</span><br><span class="line">			 System.out.print(e[i][<span class="number">1</span>]);</span><br><span class="line">			 System.out.print(<span class="string">" : "</span>);</span><br><span class="line">			 System.out.print(e[i][<span class="number">2</span>]);</span><br><span class="line">			 System.out.println();</span><br><span class="line">		 &#125;</span><br><span class="line">	 &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_set</span><span class="params">(<span class="keyword">int</span>[] p, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(p[x] == x) <span class="keyword">return</span> x;</span><br><span class="line">	<span class="keyword">return</span> find_set(p, p[x]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">union</span><span class="params">(<span class="keyword">int</span>[] p, <span class="keyword">int</span> x1, <span class="keyword">int</span> x2)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> s1 = find_set(p, x1);</span><br><span class="line">	<span class="keyword">int</span> s2 = find_set(p, x2);</span><br><span class="line">	p[s2] = s1; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>[][] arrAppend(<span class="keyword">int</span>[][] arr, <span class="keyword">int</span>[] value) &#123;</span><br><span class="line">       arr = Arrays.copyOf(arr, arr.length + <span class="number">1</span>);</span><br><span class="line">       arr[arr.length - <span class="number">1</span>] = value;</span><br><span class="line">       <span class="keyword">return</span> arr;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">class edgeComp implements Comparator&lt;int[]&gt; </span><br><span class="line">&#123; </span><br><span class="line">    <span class="comment">// Used for sorting in ascending order of </span></span><br><span class="line">    <span class="comment">// roll number </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(<span class="keyword">int</span>[] a, <span class="keyword">int</span>[] b)</span> </span></span><br><span class="line"><span class="function">    </span>&#123; </span><br><span class="line">        <span class="keyword">return</span> a[<span class="number">2</span>] - b[<span class="number">2</span>]; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Prim算法"><a href="#Prim算法" class="headerlink" title="Prim算法"></a>Prim算法</h3><p><strong>往集合里加入点</strong>   $O(E log V)$<br>首先去掉所有的从自己到自己的环，去掉两节点之间的多条边保留最短的一条。<br>在连接集合A和A之外的结点的所有边中，选择一条轻量级边加入A中，从而保证每一步所加入的边都必须是使树的总权重增加量最小的边。<br>从任意结点开始都可以得到唯一的MST。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findMin</span><span class="params">(<span class="keyword">int</span> v, <span class="keyword">int</span>[] visit, <span class="keyword">int</span>[] e)</span> </span>&#123;</span><br><span class="line">	<span class="comment">//找到e中与v相连的未访问过的距离最近的结点</span></span><br><span class="line">	<span class="keyword">int</span> min = Integer.MAX_VALUE;</span><br><span class="line">	<span class="keyword">int</span> index = -<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; e.length; i ++) &#123;</span><br><span class="line">		<span class="keyword">if</span>(e[i] &lt; min &amp;&amp; visit[i] == -<span class="number">1</span>) &#123;</span><br><span class="line">			min = e[i];</span><br><span class="line">			index = i;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> index;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Prim</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	minPath = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	vertex = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; vertex.length; i ++) vertex[i] = -<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	minPath[count] = <span class="number">0</span>;</span><br><span class="line">	vertex[minPath[count]] = <span class="number">1</span>;</span><br><span class="line">	count ++;</span><br><span class="line">	<span class="keyword">while</span>(count &lt; edges.length) &#123;</span><br><span class="line">		<span class="keyword">int</span> min = Integer.MAX_VALUE;</span><br><span class="line">		<span class="keyword">int</span> index_s = -<span class="number">1</span>;</span><br><span class="line">		<span class="keyword">int</span> index = -<span class="number">1</span>;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count; i ++) &#123;</span><br><span class="line">			<span class="keyword">int</span> e = minPath[i];</span><br><span class="line">			<span class="keyword">int</span> now = findMin(e, vertex, edges[e]);</span><br><span class="line">			<span class="keyword">if</span>(now &gt;= <span class="number">0</span> &amp;&amp; edges[e][now] &lt; min) &#123;</span><br><span class="line">				index_s = e;</span><br><span class="line">				min = edges[e][now];</span><br><span class="line">				index = now;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		minPath[count] = index;</span><br><span class="line">		vertex[minPath[count]] = <span class="number">1</span>;</span><br><span class="line">		count ++;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.print(Arrays.toString(minPath) + <span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="单源最短路径"><a href="#单源最短路径" class="headerlink" title="单源最短路径"></a>单源最短路径</h2><h3 id="Dijkstra算法-O-VlgV"><a href="#Dijkstra算法-O-VlgV" class="headerlink" title="Dijkstra算法  $O(VlgV)$"></a>Dijkstra算法  $O(VlgV)$</h3><p>初始化距离数组dis，大小等于顶点数目，源结点对应0，其它都对应正无穷。在未访问的结点中找到离x的路径最短的结点作为当前结点，用当前结点的邻接矩阵对dis进行更新，将该结点标记为已访问，重复上述步骤。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Dijkstra</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span>[] dis = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">int</span>[] path = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) dis[i] = Integer.MAX_VALUE;</span><br><span class="line">	dis[x] = <span class="number">0</span>;<span class="comment">//自己到自己的最短路径为0</span></span><br><span class="line">	path[x] = -<span class="number">1</span>;</span><br><span class="line">	Set&lt;Integer&gt; set = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">while</span>(count != edges.length) &#123;</span><br><span class="line">		<span class="keyword">int</span> now = -<span class="number">1</span>; <span class="comment">//在未访问的结点中找到离x的路径最短的结点作为当前结点</span></span><br><span class="line">		<span class="keyword">int</span> min = Integer.MAX_VALUE;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) &#123;</span><br><span class="line">			<span class="keyword">if</span>(!set.contains(i) &amp;&amp; dis[i] &lt; min) &#123;</span><br><span class="line">				now = i;</span><br><span class="line">				min = dis[i];</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//用当前结点now的临界边与dis中的路径长度作比较并进行更新</span></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) &#123;</span><br><span class="line">			<span class="keyword">if</span>(edges[now][i] != Integer.MAX_VALUE &amp;&amp; edges[now][i] + dis[now] &lt; dis[i]) &#123;</span><br><span class="line">				dis[i] = edges[now][i] + dis[now];</span><br><span class="line">				path[i] = now;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//将now加入已访问的集合中</span></span><br><span class="line">		set.add(now);</span><br><span class="line">		count ++;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.print(<span class="string">"distance: "</span> + Arrays.toString(dis) + <span class="string">"\n"</span>);</span><br><span class="line">	System.out.print(<span class="string">"last vertex: "</span> + Arrays.toString(path) + <span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Bellman–Ford-O-VE"><a href="#Bellman–Ford-O-VE" class="headerlink" title="Bellman–Ford  $O(VE)$"></a>Bellman–Ford  $O(VE)$</h3><p>对n个点做(n - 1)次遍历，每一轮遍历需要检查所有的边。原理比Dijkstra简单，但时间复杂度更高。<br>Dijkstra不适用于边权重为负的有向图，Bellman-Ford可用于此类图，且更适用于分布式系统。<br>初始化距离数组dis，大小等于顶点数目，源结点对应0，其它都对应正无穷。对每个边uv进行遍历，如果$dis[v]&gt;dis[u]+w(u,v)$，则更新$dis[v]&gt;dis[u]+w(u,v)$。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Bellman_Ford</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span>[] dis = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	<span class="keyword">int</span>[] path = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length];</span><br><span class="line">	path[x] = -<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) dis[i] = Integer.MAX_VALUE;</span><br><span class="line">	dis[x] = <span class="number">0</span>;<span class="comment">//自己到自己的最短路径为0</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; edges.length - <span class="number">1</span>; k ++) &#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++) &#123;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; edges.length; j ++) &#123;</span><br><span class="line">				<span class="keyword">if</span>( dis[i] != Integer.MAX_VALUE &amp;&amp; edges[i][j] != Integer.MAX_VALUE) &#123;	</span><br><span class="line">					<span class="keyword">if</span>(dis[j] &gt; dis[i] + edges[i][j]) &#123;</span><br><span class="line">						dis[j] = dis[i] + edges[i][j];</span><br><span class="line">						path[j] = i;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	System.out.print(<span class="string">"distance: "</span> + Arrays.toString(dis) + <span class="string">"\n"</span>);</span><br><span class="line">	System.out.print(<span class="string">"last vertex: "</span> + Arrays.toString(path) + <span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Floyd-Warshall-O-V-3"><a href="#Floyd-Warshall-O-V-3" class="headerlink" title="Floyd Warshall  $O(V^3)$"></a>Floyd Warshall  $O(V^3)$</h3><p>解决所有点对之间的最短路径问题：简单粗暴的三层循环。根据dis[i][k]+dis[k][j]对dist[i][j]进行更新。<br><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Floyd_Warshall</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span>[][] dis = <span class="keyword">new</span> <span class="keyword">int</span>[edges.length][edges.length];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++)</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; edges.length; j ++)</span><br><span class="line">			dis[i][j] = edges[i][j];</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; edges.length; k ++)</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; edges.length; i ++)</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; edges.length; j ++)</span><br><span class="line">				<span class="keyword">if</span>(dis[i][k] != Integer.MAX_VALUE &amp;&amp; dis[k][j] != Integer.MAX_VALUE)</span><br><span class="line">					dis[i][j] = Math.min(dis[i][j], dis[i][k] + dis[k][j]);</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span>[] x : dis)</span><br><span class="line">		System.out.println(Arrays.toString(x));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="图搜索策略"><a href="#图搜索策略" class="headerlink" title="图搜索策略"></a>图搜索策略</h2><h3 id="爬山算法-Hill-Climbing"><a href="#爬山算法-Hill-Climbing" class="headerlink" title="爬山算法 Hill Climbing"></a>爬山算法 Hill Climbing</h3><p>用贪心确定搜索方向，是优化的深度优先搜索<br>用启发式测度来排序节点<br>扩展的顺序：对兄弟进行比较<br>子节点按照测度f(n)从大到小入栈</p>
<h3 id="Best-First搜索"><a href="#Best-First搜索" class="headerlink" title="Best-First搜索"></a>Best-First搜索</h3><p>优化的广度&amp;深度优先<br>比较所有子节点和兄弟结点的深度<br>在一些情况下，如果我们可以预先计算出每个节点到终点的距离，则我们可以利用这个信息更快的到达终点。如果起点和终点之间存在障碍物，则最佳优先算法找到的很可能不是最短路径。</p>
<h3 id="路径规划A-算法"><a href="#路径规划A-算法" class="headerlink" title="路径规划A*算法"></a>路径规划A*算法</h3><p>用Best-First进行搜索，某些情况下一旦找到解，则解为优化解，无需搜索整个解空间。<br>如果图满足dis(A, G) &lt; dis(A, D) + h(D, g)，即a到g的实际值大于a到g的估计值，则A*算法一定能找到优化解。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/54510444">https://zhuanlan.zhihu.com/p/54510444</a></p>
<ul>
<li>初始化open_set和close_set；</li>
<li>将起点加入open_set中，并设置优先级为0（优先级最高）；</li>
<li>如果open_set不为空，则从open_set中选取优先级最高的节点n：<ul>
<li>如果节点n为终点，则：<ul>
<li>从终点开始逐步追踪parent节点，一直达到起点；</li>
<li>返回找到的结果路径，算法结束；</li>
</ul>
</li>
<li>如果节点n不是终点，则：<ul>
<li>将节点n从open_set中删除，并加入close_set中；</li>
<li>遍历节点n所有的邻近节点：<ul>
<li>如果邻近节点m在close_set中，则：<ul>
<li>跳过，选取下一个邻近节点</li>
</ul>
</li>
<li>如果邻近节点m也不在open_set中，则：<ul>
<li>设置节点m的parent为节点n</li>
<li>计算节点m的优先级</li>
<li>将节点m加入open_set中</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Branch-and-Bound策略"><a href="#Branch-and-Bound策略" class="headerlink" title="Branch-and-Bound策略"></a>Branch-and-Bound策略</h3><ol>
<li>用评价函数构造一个堆H，首先构造由根组成的单元素堆</li>
<li>if rH的根r是目标结点 then 停止</li>
<li>从H中删除r，把r的子节点插入H</li>
<li>if H为空 then 失败；else goto 2</li>
</ol>
<p><strong>完整代码见：<a href="https://github.com/huangliu0909/Graph-Algorithms">https://github.com/huangliu0909/Graph-Algorithms</a></strong></p>
]]></content>
      <categories>
        <category>学习笔记：算法</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Graph</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Variable Elimination and Belief Propagation</title>
    <url>/2020/09/02/CS5340%20Lec04/</url>
    <content><![CDATA[<h1 id="Variable-Elimination-Algorithm"><a href="#Variable-Elimination-Algorithm" class="headerlink" title="Variable Elimination Algorithm"></a>Variable Elimination Algorithm</h1><p>General probabilistic problem: calculate $p(X_F|X_E)$ for arbitrary disjoint subsets $E$ and $F$: ( $X_F$ are query nodes, $X_E$ are evidence nodes)</p>
<script type="math/tex; mode=display">p(X_F|X_E)=\frac{p(X_F,X_E)}{p(X_E)}=\frac{\sum_{X_R}p(X_E,X_F,X_R)}{\sum_{X_F}p(X_F,X_E)}</script><p>$X_R$ are nodes that must be marginalized out of joint probability.</p>
<p>Joint probability table size of $n$ variabels that takes $k$ states:<br>a naive summation: $O(k^n)$.<br>a factored form: $O(k^r)$ where $r\ll n$<br><a id="more"></a><br><img src="/images/CS5340/4/k^r.png" alt="k^r"></p>
<p>$p(X_6|X_2,X_5)$: 3d table<br>$p(\bar{X_6}=1|X_2,X_5)$: a 2d slice of the 3d table</p>
<p>Evidence potential:</p>
<script type="math/tex; mode=display">\delta(x_i,\bar{x_i})\begin{cases} 1,\qquad if\quad x_i = \bar{x_i} \\ 0,\qquad else\end{cases}</script><p>Turn evaluation into sum:</p>
<script type="math/tex; mode=display">g(\bar{x_i})=\sum_{x_i}\delta(x,\bar{x_i})=g(x_i=\bar{x_i})</script><p>Which means setting lines of table $p(x_i)$ where $x_i \neq \bar{x_i}$ to 0.<br>Evidence potential is a trick to simplify the description. In practice we take slices instead of doing summation.<br><img src="/images/CS5340/4/eg.png" alt="eg"></p>
<p>Eliminate(G,E,F):</p>
<ul>
<li>Initialize: choose elimination ordering, query nodes appear at the last.</li>
<li>Evidence: place evidence</li>
<li>Update: marginalization.<br>Loop: 1. $\phi_i = $ product of …, which is the unnormalized conditional probability. 2. $m_i(x_{S_i})=\sum_{x_i}\phi_i$, which is the narmalization factor.</li>
<li>Normalize(F)<script type="math/tex; mode=display">p(x_F|\bar{x_E})=\frac{\phi_F(x_F)}{\sum_F\phi_F(x_F)}=\frac{\phi_F(x_F)}{m_F(x_{S_F})}</script>For conditional probability, normalization factor Z cancels.<br>For a marginal probability, Z cannot be canceled.</li>
</ul>
<p><img src="/images/CS5340/4/elimination.png" alt="elimination"></p>
<h1 id="Constituted-Graph"><a href="#Constituted-Graph" class="headerlink" title="Constituted Graph"></a>Constituted Graph</h1><p>Eliminate: removing the node from graph and connecting the remaining neighbors, thus creating “elimination cliques”.<br><strong>Reconstitute</strong>: add additional edges during the elimination process to the original graph. (Complexity depends on the choice of elimination order)<br><strong>Treewidth</strong>: one less than the smallest achievable cardinality of the largest elimination clique over all possible elimination ordering.</p>
<p><em>Computaional Complexity of DGM can be convert to UGM by moralization.</em></p>
<p>Limitation: re-run elimination algorithm for every new query nodes, since different query nodes require different eliminate order.</p>
<h1 id="Sum-Product-Algorithm-Belief-Propagation-Algorithm"><a href="#Sum-Product-Algorithm-Belief-Propagation-Algorithm" class="headerlink" title="Sum-Product Algorithm (Belief Propagation Algorithm)"></a>Sum-Product Algorithm (Belief Propagation Algorithm)</h1><p>$ \checkmark$ All single-node marginals<br>$ \checkmark$ Tree like graphs (undirected tree without loop or can be moralized to such tree)<br>$ \checkmark$ Using one single run<br>$ \checkmark$ Cliques are single or pairs of nodes</p>
<h2 id="Convert-a-directed-tree-to-an-undirected-one"><a href="#Convert-a-directed-tree-to-an-undirected-one" class="headerlink" title="Convert a directed tree to an undirected one"></a>Convert a directed tree to an undirected one</h2><p>Define: $\psi(x_r) = p(x_r)$, $\psi(x_i.x_j)=p(x_j|x_i)$, $\psi(x_i)=1\quad\forall i\neq r$, Partition function $Z=1$.</p>
<h2 id="Define-local-potetials"><a href="#Define-local-potetials" class="headerlink" title="Define local potetials"></a>Define local potetials</h2><script type="math/tex; mode=display">
\psi_i^E(x_i)=\begin{cases}\psi_i(x_i)\delta(x_i,\bar{x_i})\quad i\in E
\\\psi_i(x_i)\qquad\qquad otherwise
\end{cases}</script><script type="math/tex; mode=display">
p(x|\bar{x_E})=\frac{1}{Z^E}(\prod_{i\in V}\psi^E(x_i)\prod_{(i,j)\in E}\psi(x_i,x_j))</script><h2 id="Construct-tree-structure"><a href="#Construct-tree-structure" class="headerlink" title="Construct tree structure"></a>Construct tree structure</h2><p>Order: Depth-Fist Traversal.<br>Take query node $X_f$ as root.<br>Directing all edges pointing away from $X_f$.<br>Elimination proceeds inward from leaves with tree width equals to one.</p>
<h2 id="Inward"><a href="#Inward" class="headerlink" title="Inward"></a>Inward</h2><p>Send message $(j,i)$: </p>
<script type="math/tex; mode=display">m_{ji}=\sum_{x_j}(\psi^E(x_j)\psi(x_i,x_j)\prod_{k\in N(j)- i}m_{kj})</script><p>This message can be re-used.<br>Compute Marginal:</p>
<script type="math/tex; mode=display">p(x_f|\bar{x_E})\propto\psi^E(x_f)\prod_{e\in N(f)}m_{ef}</script><p>which is the unnormalized conditional probability.</p>
<p>Message Passing Protocol: a node can send a message to a neihboring node if and only if it has received messages from all its other neighbors.</p>
<p><strong>Two phase approval: inward &amp; outward.</strong><br><img src="/images/CS5340/4/sum-product.png" alt="sum-product"></p>
]]></content>
      <categories>
        <category>Uncertainty Modelling in AI</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>PGM</tag>
      </tags>
  </entry>
  <entry>
    <title>ConvNet Architecture 2</title>
    <url>/2020/09/29/ML_ConvNet_2/</url>
    <content><![CDATA[<h1 id="InceptionNetV2"><a href="#InceptionNetV2" class="headerlink" title="InceptionNetV2"></a>InceptionNetV2</h1><p>Inception V1 + Batch Normalization(BN), converge faster<br>After convolution / linear/ fully connected layer: $z=ReLU(BN(f(x)))$<br>Inreality: $z=BN(ReLU(f(x)))$<br><a id="more"></a></p>
<p><img src="/images/CS5242/7/BN.png" alt="BN"></p>
<h1 id="InceptionNetV3"><a href="#InceptionNetV3" class="headerlink" title="InceptionNetV3"></a>InceptionNetV3</h1><h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h1 id="XceptionNet"><a href="#XceptionNet" class="headerlink" title="XceptionNet"></a>XceptionNet</h1><h1 id="Architecture-search-and-compression"><a href="#Architecture-search-and-compression" class="headerlink" title="Architecture search and compression"></a>Architecture search and compression</h1><h1 id="Transfer-learning-for-image-classification"><a href="#Transfer-learning-for-image-classification" class="headerlink" title="Transfer learning for image classification"></a>Transfer learning for image classification</h1>]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Classification</title>
    <url>/2020/08/18/ML_Classification/</url>
    <content><![CDATA[<h1 id="Regression-VS-Classification"><a href="#Regression-VS-Classification" class="headerlink" title="Regression VS Classification"></a>Regression VS Classification</h1><p>回归问题：定量(quantity)<br>分类问题：定性(label)<br>分类问题的标签一般由回归或者直接测量来得到。eg. 预测明天的天气，降水量超过一定阈值$C$为有雨，否则为不下雨。这里的降水量是个连续型随机变量，可以直接测量也可以通过回归来预测。</p>
<script type="math/tex; mode=display">\widetilde{y}=\begin{cases} 1,\qquad\qquad if\boldsymbol{w^Tx}>c \\ 0,\qquad\qquad else\end{cases}</script><p>也可以将阈值$C$与待学习的参数$w$合并：</p>
<script type="math/tex; mode=display">\widetilde{y}=\begin{cases} 1,\qquad\qquad if\boldsymbol{w^Tx}>0 \\ 0,\qquad\qquad else\end{cases}</script><a id="more"></a>
<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>上述函数在 $\boldsymbol{x}=\boldsymbol{0}$ 的时候不可微(differentiable)而且在每一处的导数都是0，在求Loss进行梯度下降的过程中无法学习到任何信息。因此我们引入Logistic Function(Sigmoid Function):</p>
<script type="math/tex; mode=display">p=\sigma (\boldsymbol{w^Tx})=\frac{1}{1+e^{-\boldsymbol{w^Tx}}}</script><p>这个函数将 $\boldsymbol{w^Tx}$ 映射到概率取值 $[0,1]$，含义是将 $\boldsymbol{x}$ 标记为 $1$ 的概率。<br>如果我们使用 ${\mathcal L}2$ 损失函数：<script type="math/tex">{\mathcal L}(\boldsymbol{x},y)=\frac{1}{2}||\sigma(\boldsymbol{w^Tx})-y||^2</script><br>令 $z=\boldsymbol{w^Tx}$ 则：<script type="math/tex">{\mathcal L}=\frac{1}{2}(\sigma(z)-y)^2</script><br>此时有：</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial {\mathcal L}}{\partial \boldsymbol{w}}&=\frac{\partial {\mathcal L}}{\partial\sigma(z)}\frac{\partial\sigma(z)}{\partial z}\frac{\partial z}{\partial \boldsymbol{w}}\\
&=(\sigma(z)-y)\cdot\frac{\partial\sigma(z)}{\partial z}\cdot\boldsymbol{x}
\end{aligned}</script><p>其中：</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial\sigma(z)}{\partial z}&=\frac{\partial ((1+e^{-z})^{-1})^{-1}}{\partial z}\\
&=-1\cdot(1+e^{-z})^{-2}\cdot\frac{\partial (1+e^{-z})}{\partial z}\\
&=-(1+e^{-z})^{-2}(-e^{-z})\\
&=(1+e^{-z})^{-2}(e^{-z})\\
&=(\sigma(z))^2(\frac{1}{\sigma(z)}-1)\\
&=\sigma(z)(1-\sigma(z))
\end{aligned}</script><p>由于梯度下降的公式为：</p>
<script type="math/tex; mode=display">{\boldsymbol{w}}={\boldsymbol{w}}-\alpha\frac{\partial {\mathcal L}}{\partial {\boldsymbol{w}}}</script><p>可以看到如果 $\sigma(z)\approx 0or1$：</p>
<script type="math/tex; mode=display">\frac{\partial\sigma(z)}{\partial z}\approx 0\to\frac{\partial {\mathcal L}}{\partial \boldsymbol{w}}\approx 0</script><p>这种情况下会出现<strong>梯度消失(Gradient Vanishing)</strong>，参数 ${\boldsymbol{w}}$ 无法继续进行更新。</p>
<h1 id="Binary-Cross-entropy"><a href="#Binary-Cross-entropy" class="headerlink" title="Binary Cross-entropy"></a>Binary Cross-entropy</h1><p>为了避免 ${\mathcal L}2$ 损失函数在训练过程中的梯度消失问题，我们使用以下损失函数：</p>
<script type="math/tex; mode=display">\begin{aligned}{\mathcal L}_{ce}(x,y)&=-ylogp-(1-y)log(1-p)\\
&=\begin{cases}\begin{aligned} -logp,\qquad\qquad &if\qquad y=1 \\ -log(1-p),\qquad&if\qquad y=0\end{aligned}\end{cases}\end{aligned}</script><p>其中： $p=\sigma(z)\quad z=\boldsymbol{w^Tx}$<br>该式解释如下：</p>
<script type="math/tex; mode=display">\begin{aligned}p(correct|\boldsymbol{x})&=\begin{cases}P(\widetilde{y}=1|\boldsymbol{x}),\qquad if\quad y=1\\P(\widetilde{y}=0|\boldsymbol{x}),\qquad if\quad y=0\\\end{cases}\\&=P(\widetilde{y}=1|\boldsymbol{x})^yP(\widetilde{y}=0|\boldsymbol{x})^{(1-y)}\end{aligned}</script><p>要想最大化上述值，就要最小化它的log值的相反数(negative log-likelihood)：</p>
<script type="math/tex; mode=display">\begin{aligned}min\quad-logP(correct|\boldsymbol{x})&=min\quad-P(\widetilde{y}=1|\boldsymbol{x})^yP(\widetilde{y}=0|\boldsymbol{x})^{(1-y)}\\&=min\quad-ylogp-(1-y)log(1-p)\\&=min\quad L_{ce}(\boldsymbol{x},y)\end{aligned}</script><p>Cross-entropy的梯度为：</p>
<script type="math/tex; mode=display">\begin{aligned}\frac{\partial L_{ce}}{\partial \boldsymbol{w}}&=\frac{\partial L_{ce}}{\partial z}\frac{\partial z}{\partial \boldsymbol{w}} =\frac{\partial L_{ce}}{\partial p}\frac{\partial p}{\partial z}\frac{\partial z}{\partial \boldsymbol{w}}\\
&=(-\frac{y}{p}+\frac{1-y}{1-p})\ast p(1-p)\ast\boldsymbol{x}\\
&=(p-y)\boldsymbol{x}\end{aligned}</script><h1 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h1><ul>
<li>Multi-label 一个输入可以有多个标签</li>
<li>Single-label 一个输入只能有一个标签</li>
</ul>
<h2 id="Multi-label"><a href="#Multi-label" class="headerlink" title="Multi-label"></a>Multi-label</h2><p>对于一个输入可以有多个标签的情况，可以同时训练多个分类器，每个分类器对应一个类别，判断该输入是否属于该类别，这些类别之间彼此独立。<br>对第 $i$ 个类别有：<br>$z_i=\boldsymbol{W}_i\boldsymbol{x}+b_i\quad p_i=\sigma(z_i)\quad L_{ce}=-y_ilogp_i-(1-y_i)log(1-p_i)$<br>对于所有类别向量化表示为:<br>$\boldsymbol{z}=\boldsymbol{W}\boldsymbol{x}+\boldsymbol{b}\quad \boldsymbol{p}=\sigma(\boldsymbol{z})\quad L_{ce}=-\boldsymbol{y}^Tlog\boldsymbol{p}-(1-\boldsymbol{y})^Tlog(1-\boldsymbol{p})$</p>
<h2 id="Single-label"><a href="#Single-label" class="headerlink" title="Single-label"></a>Single-label</h2><p>必须在所有标签中选择一个标签出来：维持所有标签的概率总和为1，取概率最大的标签为预测的结果。可以使用<strong>Softmax regression (multinomial logistic regression)</strong>：</p>
<script type="math/tex; mode=display">p_i=\frac{e^{z_i}}{\sum_j e^{z_j}}=softmax(z_i)</script><p>满足：$\boldsymbol z=\boldsymbol{Wx}\quad \sum_ip_i=1\quad t= \mathop{\arg\max}\limits_ip_i\quad \widetilde{y_i}=i=t?1:0$<br>单个实例的预测值为one-hot向量 $\widetilde{\boldsymbol{y}} =(\widetilde{y_1},…,\widetilde{y_K})$，共$K$个类别。<br>Loss函数为：$L_{ce}(\boldsymbol{x},\boldsymbol{y})=\sum_i-y_ilogp_i=-\boldsymbol{y}^Tlog\boldsymbol{p}=-logp_{k_{right}}$<br>其中 $\mathbf{X}$ 的每一行是特征向量 $\boldsymbol{x}$， $\mathbf{Y}$ 的每一行是目标one-hot向量 $\boldsymbol{y}$ ：</p>
<script type="math/tex; mode=display">
\mathbf{X} = \begin{pmatrix} {\boldsymbol{x^{(1)}}}^T \\ {\boldsymbol{x^{(2)}}}^T \\ ... \\ {\boldsymbol{x^{(m)}}}^T\end{pmatrix} \qquad
\mathbf{Y} = \begin{pmatrix} {\boldsymbol{y}^{(1)}}^T \\ {\boldsymbol{y}^{(2)}}^T \\ ... \\ {\boldsymbol{y}^{(m)}}^T\end{pmatrix}</script><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{W}}=\frac{1}{m}(\mathbf{P}-\mathbf{Y})\mathbf{X}^T\in R^{k\times n}, \mathbf{X}\in R^{(m\times n)} ,\mathbf{Y}\in R^{(m\times k)}</script><p><img src="/images/CS5242/2/Multi-gd.png" alt="history"></p>
<ul>
<li>Sigmoid 代表小写的希腊字母sigma (uppercase Σ, lowercase σ)</li>
</ul>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>散列表 Hash Table</title>
    <url>/2020/07/19/Hashing/</url>
    <content><![CDATA[<p>散列表是实现字典操作的一种有效数据结构，最坏情况的查找时间是$\theta (n)$，平均查找时间为$O(1)$。<br>散列表是普通数组概念的推广，数组可以直接寻址。<br>当实际存储的关键字数目比全部可能的关键字总数要小时，可以使用散列表来替代直接数组寻址。<br><a id="more"></a></p>
<h2 id="直接寻址表-Direct-address-table"><a href="#直接寻址表-Direct-address-table" class="headerlink" title="直接寻址表 Direct-address table"></a>直接寻址表 Direct-address table</h2><p>关键字的全域U较小，没有两个元素具有相同的关键字。<br>使用数组，即直接寻址表，其中每个位置成为槽(slot)，实际关键字集合K对应的槽指向对应的元素（关键字为k的元素存放在槽k中），其它槽为NIL，也可以使用其他关键字说明该槽为空。<br>缺点：如果全域U很大，计算机可能无法完整存储这张表；如果实际存储的关键字集合K相对于U来说很小，则T的大部分空间被浪费。</p>
<h2 id="散列表"><a href="#散列表" class="headerlink" title="散列表"></a>散列表</h2><p>散列表需要的存储空间比直接寻址表少很多，关键字为k的元素存放在槽$h(k)$中。即：利用散列函数（hash function）h来计算出关键字为k的元素的槽的位置。散列函数将全域U映射到大小为m的散列表上，其中$m&lt;&lt;|U|$。<br>两个关键字可能被映射到同一个槽中，成为冲突(collision)。<br><strong>通过链接法解决冲突</strong>：把映射到同一个槽中的关键字连成链表。<br>最坏情况：所有n个关键字都散列到同一个槽中，性能相当于普通链表。<br>平均情况：依赖于散列函数把n个关键字映射到m个槽位的平均程度。定义表T的装载因子$\alpha =n/m$。<br>在简单均匀散列(simple uniform hash)的情况下，一次查找的平均时间为$\theta(1+\alpha)$，说明总元素个数应该与表中元素个数成正比，则$\alpha=n/m=O(m)/m=O(1)$，此时查找操作平均需要常数时间。<br>当链表使用双向链表时，插入和删除的最坏情况也是$O(1)$，此时字典的全部操作都可以在常数时间完成。</p>
<h2 id="散列函数"><a href="#散列函数" class="headerlink" title="散列函数"></a>散列函数</h2><p>好的散列函数应该尽量满足简单均匀，尽量将相近的字符散列到不同的槽。</p>
<h3 id="除法散列法"><a href="#除法散列法" class="headerlink" title="除法散列法"></a>除法散列法</h3><p>散列函数：$k(h) = k\% m$<br>注意：m不应该为2的幂。如果$m=2^p$，则$h(k)$就是k的二进制表示的p个最低位数字。设计散列函数时最好考虑所有位，因为无法保证各种最低p位的排列形式为等可能。<em>m最好是一个不太接近2的幂的素数。</em></p>
<h3 id="乘法散列法"><a href="#乘法散列法" class="headerlink" title="乘法散列法"></a>乘法散列法</h3><p>散列函数：$\lfloor m(kA\%1)\rfloor$<br>其中常数A满足$0 &lt; A &lt; 1$。即：对kA的小数部分乘以m再向下取整。<br>优点：对于m的取值不是很关键。一般选择$A\approx (\sqrt5-1)/2=0.618…$</p>
<h3 id="全域散列法-universal-hashing"><a href="#全域散列法-universal-hashing" class="headerlink" title="全域散列法 universal hashing"></a>全域散列法 universal hashing</h3><p>在一个函数组中随机地选择散列函数，使之独立于要存储的关键字，从而避免了将全部关键字散列到同一个槽中的最坏情况，选定后不再更改。</p>
<h2 id="开放寻址法-open-addressing"><a href="#开放寻址法-open-addressing" class="headerlink" title="开放寻址法 open addressing"></a>开放寻址法 open addressing</h2><p>所有的元素都存放在散列表里，每个表项要么是u元素要么是NIL。散列表可能会填满，以至于无法再插入新的元素。该方法导致的一个结果是装载因子$\alpha$绝对不会超过1。槽中同样可以存放链表。<br>使用开放寻址法插入元素时，需要连续地检查散列表，或称为探查(probe)，直到找到一个空槽来放置待插入的元素。<br>线性探查，二次探查，双重探查</p>
<h2 id="完全散列"><a href="#完全散列" class="headerlink" title="完全散列"></a>完全散列</h2><p>主要针对静态(static)的关键字集合，即：一旦各关键字存入表中，关键字集合就不再变化了。<br>采用两级的散列方法来设计完全散列方案，每级上都使用全域散列。<br>为了保证不冲突，每个二级哈希表的数量是第一级映射到这个槽中元素个数的平方（在第 1 级，如果有$N_i$个元素映射到第i个槽，那么第$N_i$个槽对应的2级哈希表采用全域哈希。表的长度取$M_i = N_i^2$），这样可以保证整个哈希表非常的稀疏。<br>属性：将n个键映射到$n^2$个槽，如果从全域哈希H中随机选择h，那么期望的冲突次数小于1/2，不发生冲突的概率大于1/2。</p>
<blockquote>
<p> <a href="https://blog.csdn.net/lzq20115395/article/details/80517225">https://blog.csdn.net/lzq20115395/article/details/80517225</a></p>
</blockquote>
<p><img src="/images/perfect-hash.png" alt="hash"></p>
<h2 id="java中HashTable和HashMap的源代码"><a href="#java中HashTable和HashMap的源代码" class="headerlink" title="java中HashTable和HashMap的源代码"></a>java中HashTable和HashMap的源代码</h2>]]></content>
      <categories>
        <category>学习笔记：数据结构</category>
      </categories>
      <tags>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title>ConvNet Architecture 1</title>
    <url>/2020/09/15/ML_ConvNet_1/</url>
    <content><![CDATA[<h1 id="Historical-Notes"><a href="#Historical-Notes" class="headerlink" title="Historical Notes"></a>Historical Notes</h1><h2 id="Fukushima’s-Neocognitron"><a href="#Fukushima’s-Neocognitron" class="headerlink" title="Fukushima’s Neocognitron"></a>Fukushima’s Neocognitron</h2><ul>
<li>Hierarchical feature extraction</li>
<li>Local connectivity field</li>
<li>Hand crafted weight (before BP was developed)<a id="more"></a>
</li>
</ul>
<p><img src="/images/CS5242/6/Fukushima.png" alt="Fukushima"></p>
<h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><ul>
<li>Convolution</li>
<li>Subsampling = pooling</li>
</ul>
<p><img src="/images/CS5242/6/LeNet.png" alt="LeNet"></p>
<h1 id="Modern-CNNs"><a href="#Modern-CNNs" class="headerlink" title="Modern CNNs"></a>Modern CNNs</h1><h2 id="General-Architecture-and-Design-Guide"><a href="#General-Architecture-and-Design-Guide" class="headerlink" title="General Architecture and Design Guide"></a>General Architecture and Design Guide</h2><p>ConvBlock(module): convolution, activation, batch normalization, pooling<br>Classification: Linear + Activation + Softmax<br>Regression: Linear<br>Accuracy: large representation capacity $\to$ overfitting</p>
<h2 id="AlexNet-2012"><a href="#AlexNet-2012" class="headerlink" title="AlexNet 2012"></a>AlexNet 2012</h2><p>GPUs instead of CPUs<br>Emsemble modelling<br>ReLU: reduce the chance of gradient vanishing<br>Dropout: avoid overfitting<br>Image Augmentation<br><img src="/images/CS5242/6/AlexNet.png" alt="AlexNet"></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><ul>
<li><p>Training<br>Randomly set some neurons to 0 with probability p<br>Different layers may have different dropout rate</p>
</li>
<li><p>Inference<br><strong>Do Nothing !</strong> Stay deterministic during prediction</p>
</li>
</ul>
<p><img src="/images/CS5242/6/Dropout.png" alt="Dropout"></p>
<h3 id="Image-Augmentation"><a href="#Image-Augmentation" class="headerlink" title="Image Augmentation"></a>Image Augmentation</h3><p>Increase the training data to cover more types of (test) data.<br><img src="/images/CS5242/6/image_augment.png" alt="image_augment"></p>
<ul>
<li><p>Training<br>Random augmentation operations</p>
</li>
<li><p>Test<br><strong>No random operations</strong>, otherwise model may generate different predictions for the same input run twice.<br>Make predictions by <strong>aggregating the results from all augmented images</strong>.</p>
</li>
</ul>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p><strong>Unified kernel size</strong><br>Computational cost: 2 3x3 kernels &lt; 1 5x5 Kernel<br>Deeper structure(16, 19 convolution layers)<br>More parameters(138M).<br>More non-linear tranformation; Larger capacity.<br>Consecutive convolution.<br><img src="/images/CS5242/6/VGG.png" alt="VGG"></p>
<h2 id="Inception-V1"><a href="#Inception-V1" class="headerlink" title="Inception V1"></a>Inception V1</h2><p><strong>Inception Block</strong> : $1\times 1$ convolution (reduce channel number); ensemble multiple paths with different kernel sizes.<br>Achieve fusion of feature maps in a single level using concatenation of output from kernels with various size.</p>
<ul>
<li>Average pooling: reduce model size &amp; time complexity.</li>
</ul>
<p><img src="/images/CS5242/6/InceptionNet.png" alt="InceptionNet"><br><img src="/images/CS5242/6/concat.png" alt="concat"></p>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>From Shallow to Deep Neural Network</title>
    <url>/2020/08/25/ML_Deep_Neural_Network/</url>
    <content><![CDATA[<h1 id="Multilayer-Perceptron-MLP"><a href="#Multilayer-Perceptron-MLP" class="headerlink" title="Multilayer Perceptron (MLP)"></a>Multilayer Perceptron (MLP)</h1><h2 id="The-Perception"><a href="#The-Perception" class="headerlink" title="The Perception"></a>The Perception</h2><p>input: $x_1,x_2…,x_N$<br>weight: $w_1,w_2,…,w_N$<br>output: $y={\mathcal g }(\boldsymbol{w^Tx})=\begin{cases} 1,\qquad if\quad\boldsymbol{w^Tx}&gt;0 \\ 0,\qquad else\end{cases}$<br>Linear functions are limited: Linear models cannot fit in data from XOR function:<br><a id="more"></a></p>
<p><table>
        <tr>
            <th>$x_1$</th>
            <th>$x_2$</th>
            <th>$y$</th>
        </tr>
        <tr>
            <th>0</th>
            <th>0</th>
            <th>0</th>
        </tr>
        <tr>
            <th>0</th>
            <th>1</th>
            <th>1</th>
        </tr>
        <tr>
            <th>1</th>
            <th>0</th>
            <th>1</th>
        </tr>
        <tr>
            <th>1</th>
            <th>1</th>
            <th>0</th>
        </tr>
    </table></p>
<ul>
<li>sign function: $y={\mathcal g }(x)=\begin{cases} 1,\qquad if\quad x&gt;0 \\ 0,\qquad else\end{cases}$</li>
</ul>
<h2 id="Non-Linear-feature-transformations"><a href="#Non-Linear-feature-transformations" class="headerlink" title="Non-Linear feature transformations"></a>Non-Linear feature transformations</h2><p>$\boldsymbol{h}=\mathbf{W}\boldsymbol{x}+\boldsymbol{c}$, $\mathbf{W}\in R^{2\times 2}$, $\boldsymbol{c}\in R^2$<br>$\\\mathbf{W}=\begin{pmatrix}1\quad 1\\1\quad 1\end{pmatrix}\qquad \boldsymbol{c}=\begin{pmatrix}0\\ -1\end{pmatrix}$</p>
<p><table>
        <tr>
            <th>$x_1$</th>
            <th>$x_2$</th>
            <th>$\boldsymbol{x}$</th>
            <th>$\mathbf{W}\boldsymbol{x}+\boldsymbol{c}$</th>
            <th>$\boldsymbol{h}$</th>
            <th>$h_1$</th>
            <th>$h_2$</th>
            <th>$y$</th>
        </tr>
        <tr>
            <th>0</th>
            <th>0</th>
            <th>$\begin{pmatrix}0\\0\end{pmatrix}$</th>
            <th>$\begin{pmatrix}0\\ -1\end{pmatrix}$</th>
            <th>$\begin{pmatrix}0\\0\end{pmatrix}$</th>
            <th>0</th>
            <th>0</th>
            <th>0</th>
        </tr>
        <tr>
            <th>0</th>
            <th>1</th>
            <th>$\begin{pmatrix}0\\1\end{pmatrix}$</th>
            <th>$\begin{pmatrix}1\\0\end{pmatrix}$</th>
            <th>$\begin{pmatrix}1\\0\end{pmatrix}$</th>
            <th>1</th>
            <th>0</th>
            <th>1</th>
        </tr>
        <tr>
            <th>1</th>
            <th>0</th>
            <th>$\begin{pmatrix}1\\0\end{pmatrix}$</th>
            <th>$\begin{pmatrix}1\\0\end{pmatrix}$</th>
            <th>$\begin{pmatrix}1\\0\end{pmatrix}$</th>
            <th>1</th>
            <th>0</th>
            <th>1</th>
        </tr>
        <tr>
            <th>1</th>
            <th>1</th>
            <th>$\begin{pmatrix}1\\1\end{pmatrix}$</th>
            <th>$\begin{pmatrix}2\\1\end{pmatrix}$</th>
            <th>$\begin{pmatrix}2\\1\end{pmatrix}$</th>
            <th>2</th>
            <th>1</th>
            <th>0</th>
        </tr>
    </table><br><img src="/images/CS5242/3/non-linear.png" alt="non-linear"></p>
<p>In this case, there exist a linear function that can fit in $h_1,h_2,y$:<br>$\widetilde{y}=\boldsymbol{h^Tw}+b,\boldsymbol{w}\in R^2,b\in R\\\boldsymbol{w}=\begin{pmatrix}1\\ -2\end{pmatrix},b=0$</p>
<ul>
<li><p>Rectified Linear Unit, ReLU: ${\displaystyle f(x)=\max(0,x)}$</p>
</li>
<li><p>$max(\begin{pmatrix}0\\0\end{pmatrix},\begin{pmatrix}1\\ -1\end{pmatrix})=\begin{pmatrix}1\\0\end{pmatrix}$</p>
</li>
</ul>
<h2 id="Multilayer-Perceptron"><a href="#Multilayer-Perceptron" class="headerlink" title="Multilayer Perceptron"></a>Multilayer Perceptron</h2><p>In perceptron, there are <strong>at least one non-linear hidden layer</strong><br>$i^{th}$ layer consists of a <strong>linear/affine</strong> transformation function:</p>
<script type="math/tex; mode=display">\boldsymbol{z}^{[i]}=a^{[i]}(\boldsymbol{h}^{[i-1]})=W^{[i]}\boldsymbol{h}^{[i-1]}+\boldsymbol{b}^{[i]}</script><script type="math/tex; mode=display">W^{[i]}\in R^{n_i\times n_{i-1}}\quad \boldsymbol{b}^{[i]}\in R^{n_i}</script><p>A linear function is always <strong>followed by a non-linear function</strong> $g()$:</p>
<script type="math/tex; mode=display">\boldsymbol{h}^{[i]}=g^{[i]}(\boldsymbol{z}^{[i]})</script><p><img src="/images/CS5242/3/MLP.png" alt="MLP"><br>If one linear function is directly connected to another linear function, the two functions can be combined to one single linear function, i.e. <strong>succesive linear transformations together form another linear transformation</strong>.</p>
<p><img src="/images/CS5242/3/fc.png" alt="fc"><br>number of neurons(perceptrons): 4 + 2 = 6<br>number of weights(edges): (3 * 4) + (4 * 2) = 20<br>number of parameters total: 20 + (4 + 2) = 26 (weight_num + bias_num)</p>
<h2 id="Activate-Function-g"><a href="#Activate-Function-g" class="headerlink" title="Activate Function $g()$"></a>Activate Function $g()$</h2><p><img src="/images/CS5242/3/activate_function.png" alt="activate_function"></p>
<h2 id="Training-MLP"><a href="#Training-MLP" class="headerlink" title="Training MLP"></a>Training MLP</h2><p>for classification: Cross-entropy loss<br>for regression: L2(Squared Euclidean distancess)<br>GD algorithm:</p>
<ul>
<li>compute average loss $J$</li>
<li>computer gradient for each parameter</li>
<li>update every parameter according to their gradients</li>
</ul>
<p><img src="/images/CS5242/3/GD_algorithm.png" alt="GD_algorithm"></p>
<h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p><img src="/images/CS5242/3/BP.png" alt="BP"></p>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Convolution and Pooling</title>
    <url>/2020/09/08/ML_Convolution_and_Pooling/</url>
    <content><![CDATA[<h1 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h1><p>Convolution is a linear transformation and the output is the feature of the input.</p>
<ul>
<li>1D: text processing</li>
<li>2D: image processing</li>
<li>3D: 3D data, CT, microscopy, etc</li>
</ul>
<p>mD: m dimensions where the kernel is shifted<br><a id="more"></a></p>
<h1 id="1D-convulution"><a href="#1D-convulution" class="headerlink" title="1D convulution"></a>1D convulution</h1><script type="math/tex; mode=display">\boldsymbol{y}_t=\sum_{i=0}^{k-1}\boldsymbol{w}_i\times \boldsymbol{x}_{t+i}</script><p>This operation is actually cross-correlation, but we call it convolution.<br>$\boldsymbol{w}$: kernel/filter, length: k<br>$\boldsymbol{x}$: input, length: n<br>$\boldsymbol{y_t}$: the output feature, length: o<br>The area of input data where a kernel applied is called the <strong>receptive field</strong>, each receptive field is corresponding to a single output feature $\boldsymbol{y_t}$, which means, every time the kernel is shifted, there’s a new output $\boldsymbol{y_t}$.</p>
<p>Every cell of vector $\boldsymbol{w}$ is a parameter to be learned. In the 1D convolution below, Every output is generated from 3 input data x and these 3 cells is called a receptive field. We move the kernel along the input data until we cannot move further.<br><img src="/images/CS5242/5/1D.png" alt="1D"></p>
<h2 id="Benefits-of-Convolution"><a href="#Benefits-of-Convolution" class="headerlink" title="Benefits of Convolution"></a>Benefits of Convolution</h2><ul>
<li><p>Sparse connections<br>Each output is only connected to the receptive field instead of the whole input data(fully-connected) $\to$ fewer parameter and less overfitting</p>
</li>
<li><p>Weight sharing<br>Regularization<br>Less overfitting</p>
</li>
<li><p>Location invariant<br>Make the same prediction no matter where the object is in the image. (the same values of receptive field lead to the same output regradless of the place of the receptive field)</p>
</li>
</ul>
<h2 id="Perceptron-MLP-and-Convolution"><a href="#Perceptron-MLP-and-Convolution" class="headerlink" title="Perceptron, MLP and Convolution"></a>Perceptron, MLP and Convolution</h2><p><img src="/images/CS5242/5/MLP.png" alt="MLP"></p>
<h2 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h2><p>When stride = 1:</p>
<script type="math/tex; mode=display">o=(n+p)-k+1</script><p>$o$ : output length $\quad n$ : input length $\quad k$ : kernel length $\quad p$ : padding length<br>$p = 0\to$ <strong>valid padding</strong>  $\qquad p=k-1\to$ <strong>same padding</strong> ( left: $p/2$, right: $p/2$ ).<br>$p$ can be set manually or automatically.<br>Pad with 0 for convenience.</p>
<h2 id="Stride"><a href="#Stride" class="headerlink" title="Stride"></a>Stride</h2><p>Stride: how many steps to move towards the next receptive field.<br>Larger stride sizes skip more elements, effective receptive field size increases quickly, one output contains larger range of information of deeper lever.<br>Meanwhile increasing the stride is computationally faster, since we compute less convolution.<br><img src="/images/CS5242/5/stride.png" alt="stride"></p>
<script type="math/tex; mode=display">o=\lfloor\frac{n+p-k}{s}\rfloor+1</script><script type="math/tex; mode=display">\to \frac{n+p-k}{s}\geq o-1</script><script type="math/tex; mode=display">\to p\geq s(o-1)+k-n</script><script type="math/tex; mode=display">\to p = max(0, s(o-1)+k-n)</script><p>When stride &gt; 1, output length cannot be equal to the input length.<br>“same” is defined as:</p>
<script type="math/tex; mode=display">o=\lceil \frac{n}{s}\rceil</script><p><img src="/images/CS5242/5/stride1.png" alt="stride"></p>
<h2 id="Conv1D-forward-amp-backward"><a href="#Conv1D-forward-amp-backward" class="headerlink" title="Conv1D: forward &amp; backward"></a>Conv1D: forward &amp; backward</h2><p>Convert each receptive field to a column.<br><img src="/images/CS5242/5/conv1f.png" alt="forward"><br><img src="/images/CS5242/5/conv1b.png" alt="backward"></p>
<h1 id="2D-Convolution"><a href="#2D-Convolution" class="headerlink" title="2D Convolution"></a>2D Convolution</h1><p>Kernels move from left to right, from top to the bottom. (2 directions)<br><img src="/images/CS5242/5/2D.png" alt="2D"></p>
<h2 id="Single-channel-single-kernel"><a href="#Single-channel-single-kernel" class="headerlink" title="Single channel, single kernel"></a>Single channel, single kernel</h2><p>Convert each receptive field to a column.<br>Input $\mathbf{X}$ : $(n_h, n_w)$<br>Kernel $\mathbf{W}$ : $(k_h, k_w)$<br>Output $\mathbf{Y}$ : $(o_h, o_w)= (\lfloor\frac{n_h+p_h-k_h}{s_h}\rfloor+1,\lfloor\frac{n_w+p_w-k_w}{s_w}\rfloor+1)$<br>Computation cost: $O(k_h\times k_w\times o_h\times o_w)$<br><img src="/images/CS5242/5/2d_single_kernel.png" alt="2d_single_kernel"></p>
<h2 id="Single-channel-multiple-c-o-kernels"><a href="#Single-channel-multiple-c-o-kernels" class="headerlink" title="Single channel, multiple( $c_o$ ) kernels"></a>Single channel, multiple( $c_o$ ) kernels</h2><p>Applying multiple kernels (filters) $c_o$, all of the same stride and padding.<br>Convert each receptive field and each kernel to a column.<br>Parameter size: $c_o\times k_h\times k_w$<br>Output shape: $(c_0,o_h,o_w)=(c_o,\lfloor\frac{n_h+p_h-k_h}{s_h}\rfloor+1,\lfloor\frac{n_w+p_w-k_w}{s_w}\rfloor+1)$<br>Computation cost: $O((c_o\times k_h\times k_w)\times (o_h\times o_w))$<br><img src="/images/CS5242/5/2d_s_m.png" alt="2d_s_m"></p>
<h2 id="Multiple-c-i-channels-multiple-c-o-kernels"><a href="#Multiple-c-i-channels-multiple-c-o-kernels" class="headerlink" title="Multiple( $c_i$ ) channels, multiple( $c_o$ ) kernels"></a>Multiple( $c_i$ ) channels, multiple( $c_o$ ) kernels</h2><p>Slide the window from left to right, top to bottom.<br>Receptive fields across feature maps are concatenated into to one column.<br><img src="/images/CS5242/5/2d_m_m.png" alt="2d_m_m"></p>
<ul>
<li>Forward<br>Convert $\mathbf{X}$ into $\mathbf{\hat{X}}$ of size $(c_ik_hk_w\times o_ho_w)$<br>Reshape $\mathbf{W}$ into $(c_0\times c_ik_hk_w)$<br>$\mathbf{Y}=\mathbf{W\hat{X}}+\boldsymbol{b}$<br>Output shape: $(c_0,o_h,o_w)=(c_o,\lfloor\frac{n_h+p_h-k_h}{s_h}\rfloor+1,\lfloor\frac{n_w+p_w-k_w}{s_w}\rfloor+1)$<br>Computational cost: $O(c_o\times c_ik_hk_w\times o_h o_w)$<br><img src="/images/CS5242/5/2d_m_m1.png" alt="2d_m_m1"></li>
<li>Backward<br>Given $\frac{\partial L}{\partial \mathbf{Y}}$<br>Compute $\frac{\partial L}{\partial \mathbf{W}}=\frac{\partial L}{\partial \mathbf{Y}}\mathbf{\hat{X}}^T\quad \frac{\partial L}{\partial \mathbf{\hat{X}}}=\mathbf{W}^T\frac{\partial L}{\partial \mathbf{Y}}$<br>Column to receptive field transformation to get gradient w.r.t original $\mathbf{X}$</li>
</ul>
<h1 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h1><p>No parameter, applied to each channel respectively.<br>Padding and stride can be applied.<br># input channels = # output channels<br>i.e., $c_i=c_o=c$</p>
<h2 id="Max-pooling"><a href="#Max-pooling" class="headerlink" title="Max pooling"></a>Max pooling</h2><p><img src="/images/CS5242/5/max_p.png" alt="max_p"></p>
<h2 id="Average-pooling"><a href="#Average-pooling" class="headerlink" title="Average pooling"></a>Average pooling</h2><p><img src="/images/CS5242/5/avg_p.png" alt="avg_p"></p>
<h2 id="Effect-of-pooling"><a href="#Effect-of-pooling" class="headerlink" title="Effect of pooling"></a>Effect of pooling</h2><p>Reduce the feature size and model size<br>Max pooling: invariant to rotation of the input image<br>Average pooling: is usually skipped, can be replaced by convolution (sum of weighted input); much cheaper(no weights)</p>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Linear Regression</title>
    <url>/2020/08/11/ML_LR/</url>
    <content><![CDATA[<h1 id="神经网络发展历程"><a href="#神经网络发展历程" class="headerlink" title="神经网络发展历程"></a>神经网络发展历程</h1><p>神经网络主要依赖于特征转换(feature transformation)，通过准备数据、提取特征来构造模型。</p>
<p><img src="/images/ML_history.png" alt="history"><a id="more"></a><br>神经网络中需要非线性处理单元(nonlinear processing unit)用来产生非线性边界(boundary)，以处理线性不可分的数据(dataset)。<br>CV的主要任务有：图像分类，实体识别，场景识别，图像生成。<br>NLP的主要任务有：问答系统，机器翻译<br>Speech：对话识别</p>
<h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h1><p>线性回归(linear regression)解决的是一个回归任务，即：给定若干实例$(x,y)$，需要求得一个系统，以$x_i$为输入，输出对$y_i$的预测值$\widetilde{y_i}$。<br>在单变量线性回归中，$x_i$、$y_i$、$\widetilde{y_i}$都是标量(<strong>scalar</strong>)，该系统输出的$\widetilde{y_i}$是$x_i$的线性函数：$\widetilde{y_i}=w*x_i+b$。<br>使用损失函数(loss function)来衡量$w$、$b$对模型的模拟能力。损失函数值越小，拟合效果越好。<br>数据集$S_{train}=\{x^{(i)}，y^{(i)}\}, i = 1…m,|S_{train}|=m$<br>在参数 $w, b$ 的条件下的常用损失函数可以表示为：</p>
<ul>
<li>${\mathcal L}1：L(x,y|w,b)=|\widetilde{y} - y|$</li>
<li>${\mathcal L}2：L(x,y|w,b)=\frac{1}{2}||\widetilde{y} - y||^2=\frac{1}{2}(\widetilde{y} - y)^2$<br>选择 ${\mathcal L}2$ 因为它可微(differentiable)，更易进行优化和训练；系数$\frac{1}{2}$是为了让导数形式更简单。</li>
</ul>
<p>定义training objective（此处使用平均误差）：$J(w,b)=\frac{1}{m}\sum_{i=1}^{m}{\mathcal L}(x_i,y_i|w,b)$<br>累加：获得全局(overall)信息以达到全局优化<br>平均：消除数据集大小对loss值的影响</p>
<p>优化和训练过程：</p>
<script type="math/tex; mode=display">\min_{w,b}J(w,b)</script><script type="math/tex; mode=display">\to\min_{w,b}\frac{1}{m}\sum_{i=1}^{m}{\mathcal L}(x_i,y_i|w,b)</script><script type="math/tex; mode=display">\to \min_{w,b}\frac{1}{2m}\sum_{i=1}^{m}(w*x_i+b - y_i)^2</script><p>当$w,b$不收敛（固定更新轮数or设定阈值判断收敛）时，重复以下步骤：</p>
<ul>
<li>固定b，学习w<script type="math/tex; mode=display">\min_{w}\frac{1}{2m}\sum_{i=1}^{m}(w*x_i+b - y_i)^2</script><script type="math/tex; mode=display">\to \min_wC_1w^2+C_2w+C_3</script>由于b被固定了，w前的系数$C_1,C_2$和后面的偏置$C_3$都是常数(constant)。此时优化函数是一个关于w的一元二次函数，很容易求解当该函数J取得最小值时$w = -\frac{C_2}{2C_1}$。<br>当优化函数复杂时可以通过梯度下降作为优化函数进行求解：$w=w-\alpha\frac{\partial J}{\partial w}$</li>
<li>固定w，学习b<br>b的学习过程与w类似，将w看作常量，优化函数转换为关于b的一元二次函数，将这个函数通过优化函数（如梯度下降等）求解最优处的b值。</li>
</ul>
<h1 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h1><p>将一个实例$\boldsymbol{x}$的n个特征表示为一个$n\times 1$大小的列向量，其中$x_i$代表实例x的第i个特征，每个实例x对应一个目标值y。待学习的权重w是一个向量，$w_i$代表了特征$x_i$的重要程度。<br>即：单一实例$\boldsymbol{x}=\begin{pmatrix} x_1 \\ x_2 \\ … \\ x_n\\\end{pmatrix}$<br>建立线性回归模型（将输入映射到输出）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
   \widetilde{y}=\boldsymbol{w^Tx}+b &=\sum_{i=1}^{n}w_ix_i+b\\  &=\begin{pmatrix} w_1,w_2...w_n\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ ... \\ x_n\\\end{pmatrix} + b\\\\
   &= \begin{pmatrix} w_1,w_2...w_n, b\end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ ... \\ x_n\\1\end{pmatrix}\\
   \to \widetilde{y}=\bar w^T\bar x
\end{aligned}</script><p>对于单一实例求梯度：</p>
<script type="math/tex; mode=display">J(w)=\frac{1}{2}||\widetilde{y} - y||^2=\frac{1}{2}( \boldsymbol{w^Tx}-y)^2</script><script type="math/tex; mode=display">\frac{\partial J(\boldsymbol{w})}{\partial \boldsymbol{w}}=(\boldsymbol{w^Tx}-y)\boldsymbol{x}</script><p>我们可以选择对每个实例$x^{(i)}$，$y^{(i)}$分别求loss值并累加求平均，再对每个实例的梯度求累加，但这样对实例逐个操作的效率不如将所有实例合并视为矩阵再进行矩阵操作代表效率高。<br>将每个特征向量实例(列向量)通过转置放在矩阵的一行：</p>
<script type="math/tex; mode=display">
\mathbf{X} = \begin{pmatrix} {\boldsymbol{x^{(1)}}}^T \\ {\boldsymbol{x^{(2)}}}^T \\ ... \\ {\boldsymbol{x^{(m)}}}^T\end{pmatrix} \qquad
\boldsymbol{y} = \begin{pmatrix} {y^{(1)}}^T \\ {y^{(2)}}^T \\ ... \\ {y^{(m)}}^T\end{pmatrix} 
\qquad \boldsymbol{\widetilde{y}}=\mathbf{X}\boldsymbol{w}</script><script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{1}{2m}||\widetilde{\boldsymbol{y}} - \boldsymbol{y}||^2=\frac{1}{2m}(\widetilde{\boldsymbol{y}} - \boldsymbol{y})^T(\widetilde{\boldsymbol{y}} - \boldsymbol{y})\qquad \boldsymbol{u}=\widetilde{\boldsymbol{y}} - \boldsymbol{y}</script><p>由于：</p>
<script type="math/tex; mode=display">\frac{\partial(u(x)\cdot v(x))}{\partial x} = \frac{\partial u^Tv}{\partial x} = \frac{\partial u}{\partial x}\cdot v+\frac{\partial v}{\partial x}\cdot u</script><script type="math/tex; mode=display">\frac{\partial {\boldsymbol{u}}}{\partial {\boldsymbol{w}}} = {\mathbf{X}}^T</script><p>可以得到：</p>
<script type="math/tex; mode=display">
\frac{\partial J(\boldsymbol{w})}{\partial \boldsymbol{w}}=\frac{1}{2m}(\frac{\partial {\boldsymbol{u}}}{\partial {\boldsymbol{w}}}\cdot {\boldsymbol{u}} + \frac{\partial {\boldsymbol{u}}}{\partial {\boldsymbol{w}}}\cdot {\boldsymbol{u}})=\frac{1}{m}{\mathbf{X}}^T{\boldsymbol{u}}=\frac{1}{m}{\mathbf{X}}^T(\widetilde{\boldsymbol{y}} - \boldsymbol{y})</script><p>显然在当前条件下导数取0时损失函数$J(w)$值最小，此时令$\frac{\partial J(w)}{\partial w}=0$有：</p>
<script type="math/tex; mode=display">{\mathbf{X}}^T({\mathbf{X}}{\boldsymbol{w}}-{\boldsymbol{y}})=0</script><script type="math/tex; mode=display">\to {\mathbf{X}}^T{\mathbf{X}}{\boldsymbol{w}}={\mathbf{X}}^T{\boldsymbol{y}}</script><script type="math/tex; mode=display">\to {\boldsymbol{w}}=({\mathbf{X}}^T{\mathbf{X}})^{-1}{\mathbf{X}}^T{\boldsymbol{y}}</script><p>同样也可以使用梯度下降进行训练：${\boldsymbol{w}}={\boldsymbol{w}}-\alpha\frac{\partial J({\boldsymbol{w}})}{\partial {\boldsymbol{w}}}$</p>
<ul>
<li><p>w.r.t.: with respect to</p>
</li>
<li><p>分母布局(denominator layout)：求导结果的维度以分母为主，即结果的第一维度与分母相同。分子布局和分母布局的结果是互为转置的关系。</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Matrix_calculus#cite_note-2">https://en.wikipedia.org/wiki/Matrix_calculus#cite_note-2</a><br><img src="/images/denominator_layout.png" alt="denominator layout"></p>
</blockquote>
</li>
<li><p>Pipeline of ML: Dataset, modeling, optimization, validation and inference</p>
</li>
<li><p>使用 $L$ 来表示单一实例上的损失值<br>使用 $J$ 来表示实例集合上的平均损失值</p>
</li>
<li><p>np.multiply()函数 等价于 星号（*）：每个元素对应相乘</p>
</li>
<li>在矢量乘矢量的內积运算中，np.matmul 与np.dot没有区别，np.matmul中禁止矩阵与标量的乘法。<blockquote>
<p><a href="https://blog.csdn.net/guaidoukx/article/details/97659074?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">https://blog.csdn.net/guaidoukx/article/details/97659074?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Training Deep Networks</title>
    <url>/2020/09/01/ML_Training_Deep_Networks/</url>
    <content><![CDATA[<h1 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h1><h2 id="Various-GD’s-problems-and-advantages"><a href="#Various-GD’s-problems-and-advantages" class="headerlink" title="Various GD’s problems and advantages"></a>Various GD’s problems and advantages</h2><ol>
<li>Forward computer the average loss among:<ul>
<li>all training samples (GD)</li>
<li>randomly pick a single training sample (SGD)</li>
<li>randomly pick b training samples (mini-batch SGD)</li>
</ul>
</li>
<li>Backword compute the gradient of each parameter</li>
<li>Update every parameter according to its gradient</li>
</ol>
<p>Problems of GD: </p>
<ul>
<li>local optimum: stop updating when every parameter is at its local optimum. (This is a rare case) </li>
<li>saddle point: we may not even arrive at a local optimum.</li>
<li>Efficiency: has to load all data to compute the update, with high memory cost and high computation cost per iteration.</li>
</ul>
<p>Advantage of SGD:</p>
<ul>
<li>more efficient in terms of memory cost and speed per iteration.</li>
<li>avoid local optimum efficiently: although $\frac{\partial J^{(k)}}{\partial w}=0$ , $\frac{\partial J^{(i)}}{\partial w}$ (i != k) may not be zero.</li>
</ul>
<p>Problems of SGD: </p>
<ul>
<li>not moving in the optimal direction for every step due to the stochastic process(minimizing loss for a single example but not for the whole training dataset and may not converge at all).</li>
</ul>
<p>Advantages of mini-batch SGD:</p>
<ul>
<li>the derivation direction is more stable than SGD.</li>
<li>Averaged on b samples, leading the faster convergence in terms of #iterations.</li>
</ul>
<p>Efficiency compare:</p>
<ul>
<li>R for convergence rate: $R_{GD}&gt;R_{mini-batch SGD}&gt;R_{SGD}$</li>
<li>T for time per iteration: $T_{GD}\leq T_{mini-batch SGD}\leq T_{SGD}$</li>
<li>Tc for time to converage: $Tc=$#$\times T$ (number of iterations $\times$ time per iteration), it depends case by case. By experience, mini-batch SGD is the fastest to converge.</li>
</ul>
<h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><ul>
<li>larger batches are more accurate at estimating the gradient, better gradient does not equal to better performance due to the possibility of overfitting.</li>
<li>GPU has limited memory, if all batch samples are processed in parallel, we should reduce batch size or use more GPU.</li>
<li>Some hardware have better runtime with specific array sizes, e.g., for GPUs, powers of 2 are usually optimal.</li>
<li>Training with small batch need small learning rate to reduce the noise and maintain stability due to the high variance in the estimated gradient, this trade-off may slow down the overall learning speed.</li>
<li>Small batches act as regularizers with noisy approximated gradients.</li>
<li>Using extreme small batches may cause the underutilization of multicore architectures and there is a fixed time cost below a certain batch size.</li>
</ul>
<h2 id="Loss-Contour"><a href="#Loss-Contour" class="headerlink" title="Loss Contour"></a>Loss Contour</h2><p>Every point in the contour is a specific assignment of parameters, points on the same circle have the same loss value, points on the innermost circle have the minimum loss.<br><img src="/images/CS5242/4/loss-contour.png" alt="loss-contour"></p>
<h2 id="Evolution-of-mini-batch-SGD"><a href="#Evolution-of-mini-batch-SGD" class="headerlink" title="Evolution of mini-batch SGD"></a>Evolution of mini-batch SGD</h2><h3 id="mini-batch-SGD-with-momentum"><a href="#mini-batch-SGD-with-momentum" class="headerlink" title="mini-batch SGD with momentum"></a>mini-batch SGD with momentum</h3><p>Motivation：</p>
<ul>
<li>Gradients may dramatically change on a complex loss function.</li>
<li>Hard to make progress on flat regions.</li>
<li>Momentum smooths the gradients over past time, i.e., balance thes history and new gradient.</li>
</ul>
<p>Algorithm:</p>
<script type="math/tex; mode=display">v=\beta v+\alpha g</script><script type="math/tex; mode=display">w=w-v</script><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Motivation：</p>
<ul>
<li>Different dimensions have different gradients.</li>
<li>We want a larger lr for small gradients and small lr for large gradients.</li>
<li>Adagrad makes learning rates adapt to each dimension.</li>
</ul>
<p>Algorithm:</p>
<script type="math/tex; mode=display">s=s+g^2</script><script type="math/tex; mode=display">w=w-\frac{\alpha}{\sqrt{s+\epsilon}}g</script><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>Motivation：</p>
<ul>
<li>Adagrad decays the learning rate that may lead slow progress at end.</li>
<li>RMSprop guarantees that learning rate will not end up to 0 as time goes by.</li>
</ul>
<p>Algorithm:</p>
<script type="math/tex; mode=display">s=\beta s+(1-\beta)g^2\qquad moving\quad average</script><script type="math/tex; mode=display">w=w-\frac{\alpha}{\sqrt{s+\epsilon}}g\qquad nomalize\quad the\quad gradient</script><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Motivation：</p>
<ul>
<li>Conbine the momentum and adapted learning rate to smooth the gradients and adapt to various dimensions.</li>
</ul>
<p>Algorithm:</p>
<script type="math/tex; mode=display">v=\beta_1 v+(1-\beta_1)g\quad \hat v = \frac{v}{1-\beta_1^t}</script><script type="math/tex; mode=display">s=\beta_2 s+(1-\beta_2)g^2\quad \hat s = \frac{s}{1-\beta_2^t}</script><script type="math/tex; mode=display">w=w-\frac{\alpha}{\sqrt{s+\epsilon}} v=w-\frac{\alpha}{\sqrt{\hat s+\epsilon}}\hat v</script><p>Adam offers fast convergence.<br>Mainly for the first few iterations, when t gets large, the denominator goes to 1.</p>
<h2 id="Convergence"><a href="#Convergence" class="headerlink" title="Convergence"></a>Convergence</h2><p>Usually, convergence is considered w.r.t the optimized variable, i.e. weights of the network.</p>
<ul>
<li>For learning, however, because we’re not really interested in finding the optimum but having a good model that will generalize, we are interested in convergence of the loss.</li>
<li>Convergence then refers to the loss plateauing.</li>
</ul>
<h1 id="Training-tricks"><a href="#Training-tricks" class="headerlink" title="Training tricks"></a>Training tricks</h1><h2 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h2><p>Initialize lr with a large value and then decrease it gradually.<br>decay learning rate:</p>
<ul>
<li>step</li>
<li>1/t</li>
<li>exponential $\alpha e^{-kt}$</li>
</ul>
<p><img src="/images/CS5242/4/lr-2.png" alt="lr-2"></p>
<h2 id="Random-parameter-initialization"><a href="#Random-parameter-initialization" class="headerlink" title="Random parameter initialization"></a>Random parameter initialization</h2><p>We want each unit to compute a different function and we do not want redundant parameters.<br><strong>Random initializations can break symmetry.</strong></p>
<ul>
<li>Gaussian, N(0, 0.01) </li>
<li>Uniform, U(-0.05, 0.05) </li>
<li>Glorot/Xavier : Gaussian  N(0, sqrt(2/(fan_in+ fan_out)) </li>
<li>He/MSRA : Gaussian  N(0, sqrt(2/fan_in)) </li>
</ul>
<p>General idea: keep variance of random values within some reasonable range. We want various parameters have the same scale/range.</p>
<ul>
<li>Too small variance: Z vanishing  (Z = X W + b,)</li>
<li>Too large variance: Z exploding</li>
</ul>
<p>For bias vector:<br><img src="/images/CS5242/4/bias.png" alt="bias"></p>
<h2 id="Data-normalization"><a href="#Data-normalization" class="headerlink" title="Data normalization"></a>Data normalization</h2><p><img src="/images/CS5242/4/normalization.png" alt="normalization"></p>
<h1 id="Overfitting-and-Underfitting"><a href="#Overfitting-and-Underfitting" class="headerlink" title="Overfitting and Underfitting"></a>Overfitting and Underfitting</h1><h2 id="Underfitting"><a href="#Underfitting" class="headerlink" title="Underfitting"></a>Underfitting</h2><p>Low model capacity / complexity $\to$ model too simple to fit training data<br><strong>High bias</strong></p>
<ul>
<li>definition of bias (error): An algorithm’s tendency to consistently learn wrong things by not taking into account sufficient information found in the data</li>
</ul>
<h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p>High capacity/complexity $\to$ fits well to seen data, i.e. training data<br><strong>High variance</strong><br>Cannot generalize well onto new data, so performs poorly on unseen data, i.e. test</p>
<ul>
<li>definition of Variance (errors): Amount that the estimates (in our case the parameters w) will change if we had different of training data (but still drawn from the same source).</li>
</ul>
<h2 id="A-good-model"><a href="#A-good-model" class="headerlink" title="A good model"></a>A good model</h2><p>Uses the “right” capacity / complexity to model the data and can <strong>generalize to unseen data</strong>.<br>Strikes a balance between under-and over-fitting, as well as bias and variance.<br>Because bias and variance derived from the same factor(model capacity), we cannot have both low bias AND low variance.<br><img src="/images/CS5242/4/model.png" alt="model"></p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><h3 id="Add-L2-Norm"><a href="#Add-L2-Norm" class="headerlink" title="Add L2 Norm"></a>Add L2 Norm</h3><p>$\Phi$ is the flatten vector of all parameters:</p>
<script type="math/tex; mode=display">J_{reg}=J+\frac{\lambda}{2}||\Phi||^2</script><script type="math/tex; mode=display">\frac{\partial J_{reg}}{\partial w}=\frac{\partial J}{\partial w} + \lambda w</script><h3 id="Stop-early"><a href="#Stop-early" class="headerlink" title="Stop early"></a>Stop early</h3><p><img src="/images/CS5242/4/stop.png" alt="stop"></p>
<h2 id="Model-tuning-and-data-splitting"><a href="#Model-tuning-and-data-splitting" class="headerlink" title="Model tuning and data splitting"></a>Model tuning and data splitting</h2><p>Degree is a hyper-parameter or configuration knob.<br>Tuning the degree is called <strong>hyper-parameter tuning or model selection</strong>.<br>For complex models, there could be many such hyper-parameters, e.g. Learning rate of the gradient descent algorithm $\alpha$, Number of layers for a neural network, etc.<br><img src="/images/CS5242/4/Hyper-parameter.png" alt="Hyper-parameter"> </p>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>正则化方法</title>
    <url>/2020/07/13/Normalization%20Methods/</url>
    <content><![CDATA[<h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p>点估计point estimator：<br>偏差bias：预测值和真实值之间的差异<br>方差variance：描述预测值作为随机变量的离散程度<br>泛化generalization：在先前未观察的输入上表现良好的能力<a id="more"></a><br>准确率accuracy：<br>错误率errorrate：该模型输出错误结果的样本比率，是0-1损失的期望<br>泛化误差：即测试误差<br>模型的容量capacity：拟合各种函数的能力<br>奥卡姆剃刀原则Occam‘s razor：在同样能够解释已有观测现象的假设中，我们应该选择最“简单”的那个。<br>过拟合overfitting：训练误差和测试误差之间差距较大<br>欠拟合underfitting：模型不能再训练集上获得足够低的误差<br>验证集：用来训练超参数<br>k-fold validation：<br>鲁棒性Robust：算法对数据变化的容忍度<br>正则化：对学习算法的修改，旨在减少泛化误差而不是训练误差。</p>
<h2 id="参数范数惩罚（权重衰减weight-decay）"><a href="#参数范数惩罚（权重衰减weight-decay）" class="headerlink" title="参数范数惩罚（权重衰减weight decay）"></a>参数范数惩罚（权重衰减weight decay）</h2><p>正则化方法对目标函数添加一个参数惩罚项得到正则化后的目标函数：<br>$J (\theta ;x,y)=J(\theta ;x,y)+\alpha \Omega(\theta)$<br>其中α是权衡范数惩罚项和标准目标函数相对贡献的超参数。<br>神经网络中参数包括权重和偏置，通常只对权重进行惩罚而不对偏置做惩罚。每个权重会指定两个变量如何相互作用，而每个偏置仅控制一个变量，对偏置不进行正则化也不会导致较大方差，对其正则化反而会导致欠拟合。<br>$w$：应受范数惩罚影响的权重参数<br>$\theta$：所有参数</p>
<h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>各个参数绝对值之和：<br>$\Omega (\theta) = ||w||_1=\sum_i|w_i|$<br>L1范数损失函数<br>$L=\sum_{i=1}^n|y_i-f(x_i)|$<br>鲁棒性更好，因为L2将误差平方化，误差会放大很多。<br>L1正则通常被用来进行特征选择，L1范数会使得很多参数的最优解变成0，产生稀疏解。</p>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>岭回归、Tikhonov正则<br>$\Omega (\theta)=\frac{1}{2} ||w||_2^2=\sum_{i=1}^nw_i^2$<br>L2范数损失函数<br>$L=\sum_{i=1}^n(y_i-f(x_i))^2$<br>使用在LR任务中：最小二乘法。<br>许多线性模型，包括线性回归和PCA，都依赖于对矩阵$x^T x$求逆。一旦该矩阵奇异，这个方法就失效了。而加入正则化后对应求逆$x^T x+\alpha I$，这个正则化矩阵是可以保证可逆的。</p>
<h2 id="数据集增强"><a href="#数据集增强" class="headerlink" title="数据集增强"></a>数据集增强</h2><p>创造假数据，注入噪声等。对于对象识别任务使用数据集增强格外有效</p>
<h2 id="噪声鲁棒性"><a href="#噪声鲁棒性" class="headerlink" title="噪声鲁棒性"></a>噪声鲁棒性</h2><p>向输入添加方差极小的噪声等价于对权重施加范数惩罚<br>将噪声添加到隐藏单元的效果比简单的收缩参数更大<br>施加于权重的噪声可以被解释为与传统正则形式相同<br>显式地对标签上的噪声进行建模可以获得更好的模型效果</p>
<h2 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h2><p>通过合并不同任务的样例来提高泛化能力。这些不同任务中包含共享的因素。</p>
<h2 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h2><p>使用算法确定最佳训练时间or使用提前终止确定训练步数or使用提前终止确定将会过拟合的目标值。</p>
<h2 id="参数共享与参数绑定"><a href="#参数共享与参数绑定" class="headerlink" title="参数共享与参数绑定"></a>参数共享与参数绑定</h2><p>强迫某些参数相等。<br>与范数惩罚相比明显优点在于显著减少模型所占内存。</p>
<h2 id="稀疏表示"><a href="#稀疏表示" class="headerlink" title="稀疏表示"></a>稀疏表示</h2><p>惩罚神经网络中的激活单元。<br>稀疏性，即很多参数是0，相当于对模型做了一次特征选择，只留下了比较重要的特征，提高模型泛化能力。</p>
<h2 id="集成学习Ensemble-learning"><a href="#集成学习Ensemble-learning" class="headerlink" title="集成学习Ensemble learning"></a>集成学习Ensemble learning</h2><p>用多个弱分类器组合成一个强分类器</p>
<h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>从训练集进行子抽样得到每个基模型需要的子训练集，对所有基模型的预测结果进行综合产生的最终的预测结果。结合几个模型降低泛化误差，被称为模型平均(model averaging)。Bagging允许重复使用同一个模型、训练算法和目标函数。随机森林random forest是典型的基于bagging框架的模型。Bagging均匀随机重采样，各分类器是不相关的，可以并行。<strong>减少方差</strong>：各子模型之间有一定相关性，可以减少方差。</p>
<h3 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h3><p>训练过程为阶梯状，基模型按次序一一训练，下一个分类器依赖于上一个分类器，只能串行。（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化，对所有基模型的预测结果进行综合产生的最终的预测结果。Boosting根据错误率采样，因此Boosting分类精度优于Bagging。<strong>减少偏差</strong>：序列化最小化损失函数，由于各子模型强相关，不能显著减少方差。</p>
<h3 id="stacking"><a href="#stacking" class="headerlink" title="stacking"></a>stacking</h3><p>将训练好的所有基模型对训练集进行预测，第j个基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第j个训练值，最后基于新的训练集机型训练。同理，预测的过程也要先经过所有的基模型的预测形成新的测试集，最后再对测试集进行预测。</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout被认为是集成大量深度神经网络的使用Bagging方法。Dropout训练的集成包括所有从基础网络除去非输出单元后形成的子网络。<br>在bagging中所有模型都是独立的，在dropout中所有模型共享参数。</p>
<h2 id="批标准化"><a href="#批标准化" class="headerlink" title="批标准化"></a>批标准化</h2><h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2>]]></content>
      <categories>
        <category>学习笔记：深度学习</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for &quot;Federated Machine Learning -- Concept and Applications&quot;</title>
    <url>/2020/10/30/Reading_Note_FL/</url>
    <content><![CDATA[<p>Paper: <a href="https://arxiv.org/abs/1902.04885">https://arxiv.org/abs/1902.04885</a></p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>As the technology grows rapidly, more and more attentions are paid to Artificial Intelligence, one of the most famous examples is Alpha Go. It used 30000 games to train the model and finally defeated the top human Go players. The current rapid development of AI largely depends on the <em>availability of Big Data</em>.<br><a id="more"></a></p>
<p>The real-world situations, however are less encouraging. We have the following troubles such as limited amount of data, data without high quality, and it may cost a lot to exchange data, we should meet the requirements of laws and go through administrative procedures. Sometimes, considering the industry competition and privacy security, it is impossible to get more data at all. In this case, most of time we are confronted with a big issue: how can we fuse enough high-quality data for training a machine learning model legally with a low cost.</p>
<p>In general, all the problems above can be concluded as two parts: <strong>data island</strong> and <strong>data security</strong>, which lead to federated learning framework to achieve model training regardless of these troubles.</p>
<h1 id="Federated-Learning"><a href="#Federated-Learning" class="headerlink" title="Federated Learning"></a>Federated Learning</h1><p>Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections.</p>
<p>Here are some privacy protect methods for FL:</p>
<ul>
<li><p>The First is <strong>Secure Multi-party Computation</strong>. SMC models naturally involve multiple parties, each party knows nothing except its input and output. But this setting requires complex computation protocols and be may hard to achieve effectively.</p>
</li>
<li><p>As for <strong>Differential privacy</strong>, The basic idea is to add noise to the data, or use other methods to obscure certain sensitive information until the individual cannot be distinguished. We can see this method leads to a trade-off between accuracy and privacy.</p>
</li>
<li><p>Also, we can use <strong>Homomorphic Encryption</strong> to guarantee privacy through parameter exchange under the encryption mechanism, while the data and the model itself are not transmitted, so there’s risks in raw data leakage.</p>
</li>
</ul>
<p>Federated Learning can be divided to three categories:</p>
<ol>
<li><p><strong>Horizontal federated learning</strong><br>The first kind of FL is Horizontal federated learning.<br><img src="/images/CS6203/FL-1.png" alt="HFL"><br>Each row of the matrix represents a sample, and each column represents a feature. In HFL, datasets share the same feature space but different in samples. For example, people in different cities leads to different samples, while the banks in different cities have similar functions, i.e., similar features. Therefore, we can train a ML model using decentralized data.<br><img src="/images/CS6203/FL-2.png" alt="HFL"><br>This figure shows a typical architecture of server/client horizontal federated learning system. In the first step, the clients locally train the model, mask the gradients using differential privacy or other encryption techniques and send their encrypted gradients to the server. Then the server aggregates the gradients and updates the model. Next, the server sends the model parameters back to clients and clients update their local model. This process will loop until every party gets a satisfying model.<br>In the whole process, there’re are no information leakage between any parties.<br>And this architecture is independent from specific ML algorithms that the clients choose to train and all participants will share the final model parameters.</p>
</li>
<li><p><strong>Vertical Federated Learning</strong><br><img src="/images/CS6203/FL-3.png" alt="VFL"><br>We can see from the figure that in this framework, datasets have different features but similar samples. For example, the bank and shopping mall in the same city, they have the same target clients while their services are quite different. In this case we can jointly train a model for product purchase based on user and product information.<br><img src="/images/CS6203/FL-4.png" alt="VFL"><br>As for the architecture of vertical federated learning, suppose A has training data while B has training data and labels. We want to model how the data in A and data in B jointly influence the value of label. Since A and B cannot exchange data directly, we need a third party, C to help with the model training.<br>To start with, we need to confirm the common users between A and B without exposing users that do not overlap. This process is called entity alignment.<br>Then we’ll begin model training. Firstly, the collaborator C send public key to A and B. Then A and B exchange encrypted intermediate result. After exchanging, A and B compute encrypted gradient and loss and send to C. C decrypts and send gradient and loss back to A and B, the clients updates their model locally.</p>
</li>
<li><p><strong>Federated Transfer Learning</strong><br>Federated Transfer Learning applies to datasets differ in both sample space and feature space. For instance, the datasets from bank in China and hospital in Singapore. Finding similarities (invariants) is the core of transfer learning. So we learn a representation between the two feature space using the overlapped sample sets and apply this model to obtain predictions for samples.<br><img src="/images/CS6203/FL-5.png" alt="FTL"><br>In the scenario of TFL, datasets have only a little set of overlapping samples. It does not change the architecture of vertical federated learning, but different in detail which tries to find the common representation among the parties.<br>Incentive mechanism means after the model is built, the local model’s performance depends on how much this party contribute to the whole federated system. Combining the incentive mechanism may improve the entire architecture, encouraging more organizations to participant and provide more data.</p>
</li>
</ol>
<h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><p>Federated learning can be considered as <strong>privacy-preserving decentralized collaborative machine learning</strong> and most of the privacy protection techniques using in privacy-preserving machine learning can be applied in Federated learning</p>
<p>Horizontal federated learning is similar to <strong>Distributed Machine Learning</strong> to some extent. This figure shows an example of distributed machine learning using distributed storage of training data. In distributed machine learning, the parameter server allocates data on distributed working nodes and compute model parameters in a scheduled way. However in Federated learning, each working node which is the data holder, can independently decide when and how to join federated learning. What’s more, federated learning focus on the privacy protection during parameter transferring while distributed machine learning makes no achievements in this security issue.</p>
<p><strong>Edge computing</strong> is also closely connected to federated learning. Federated learning can provide protocols of implementation details that helps to guarantee data security as well as the improvement in accuracy of global model. In this case federated learning can work as an operation system for edge computing.</p>
<p><strong>Federated Database Systems</strong> are systems that integrate multiple database units and manage the integrated system as a whole. Compare to distributed data system, working nodes in federated database system store heterogenous data. So federated database has similar type and storage of data compared to federated learning. But as a database system, federated database system mainly focuses on the basic operations of data such as inserting, deleting and searching while the goal of federated learning is to jointly train a model that works well in each independent unit. Meanwhile federated database system does not have a privacy protection mechanism.</p>
<h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><p>The first common application of federated learning is <strong>smart retail</strong>. Smart retail aims at providing personal recommendation and services based on user’s preference and purchasing power. However, these features may be stored in different places: purchasing power can be analyzed from bank saving, user’s preference can be indicated from social media and information of product is recorded in the e-shops. </p>
<p>In this case we are facing the problem that <em>these three kinds of organizations cannot exchange data directly so the data are scattered and cannot be aggregated</em>.</p>
<p>The other problem is <em>data Heterogeneity</em>.<br>Heterogeneous data are any data with high variability of data types and formats. They are possibly ambiguous and low quality due to missing values, high data redundancy, and untruthfulness. It is difficult to integrate heterogeneous data to meet the business information demands.</p>
<p>The solution is Federated learning and Transfer learning. We can use federated learning to break the data island and use transfer learning to handle the Heterogeneous data, in this way we can train a model using data in various parties to achieve personal recommendation and break through the limitations of traditional artificial intelligence techniques Therefore, federated learning provides a cross-enterprise, cross-data, and cross-domain ecosphere of big data and artificial intelligence. </p>
<p><img src="/images/CS6203/FL-6.png" alt="Smart Healthcare"><br><strong>Smart Healthcare</strong> is obviously a direct application of horizontal federated learning. Different hospitals treat different patients, but are doing research for the treating of the same diseases. Due to the limitation of people treated in one single hospital, each hospital may not get enough data for a certain disease. Medical information is definitely sensitive information that involves patient privacy. Therefore, hospitals cannot exchange data directly. In this case we can use federated learning to jointly make use of data in different hospitals to train a global model that works well in each single hospital and protect patients’ privacy at the same time.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>It is expected that in the near future, federated learning would break the barriers between industries and establish a community where data and knowledge could be shared together with safety, and the benefits would be fairly distributed according to the contribution of each participant.</p>
<p>Here are some of my insights.</p>
<p>One of the challenges for federated learning is the Non-IID data as introduced in the section of distributed machine learning. So the main problem is that when we are using SGD in local models, we’ll face the problem of weight divergence, as we can see in the figure, the global model does not work well on each local datasets. So <strong>I think we can use adaptive optimizer as it can learn from the history gradient, which means it can make use of local data distribution when computing local gradients and can solve the problem of weight divergence to some extent</strong>.<br><img src="/images/CS6203/FL-7.png" alt="Non-i.i.d"></p>
<blockquote>
<p>Yue Zhao,Meng Li,Liang zhen Lai,Naveen Suda,Damon Civin,and Vikas Chandra.2018. Federated Learning with Non-IID Data</p>
</blockquote>
<p>And the other thing I notice is that all the framework above assume no delay for each nodes’ model transferring to the server, i.e, the server has to collect all gradients before updating global model. This lead to the fact that server may wait for too much time for collecting all gradients in one single round. So, <strong>I think the federated model can work asynchronously</strong>. Every time the server receives a model from A, the server updates the global model and immediately sends the new model back to A. Therefore, there’re less time waste in one round and more efficiency. </p>
]]></content>
      <categories>
        <category>Readings</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>DB</tag>
      </tags>
  </entry>
  <entry>
    <title>NNDL Summary</title>
    <url>/2020/10/05/ML_Summary/</url>
    <content><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><h2 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear regression"></a>Linear regression</h2><p>Univariate: single feature $x\in R$<br>Multivariate: multiple features $\boldsymbol{x}\in R^m$<br>Linear Transformation: $\widetilde{y}=\boldsymbol{w^Tx}$<br>Loss: measures the difference between the prediction and the ground truth<br>Training: to optimize(i.e. minimize) the loss w.r.t parameters($\boldsymbol{w}$)<br><a id="more"></a></p>
<h2 id="Gradient-descent-algorithm"><a href="#Gradient-descent-algorithm" class="headerlink" title="Gradient descent algorithm"></a>Gradient descent algorithm</h2><p>minimize the target loss iterately, for each iteration:<br>conpute the gradient of average loss</p>
<ul>
<li>for single feature multiple examples: <script type="math/tex; mode=display">J(w)=\frac{1}{2m}\sum_{i=1}^m(wx^{(i)}-y^{(i)})^2</script></li>
<li>for multiple features one single instance: <script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{1}{2}(\boldsymbol{w^Tx}-y)^2</script></li>
<li>for multiple features multiple examples: <script type="math/tex; mode=display">J(\boldsymbol{w})=\frac{1}{2m}(\mathbf{X}\boldsymbol{w}-\boldsymbol{y})^T(\mathbf{X}\boldsymbol{w}-\boldsymbol{y})</script></li>
</ul>
<p>update parameter in the opposite of the gradient direction: $\boldsymbol{w}=\boldsymbol{w}-\alpha\frac{\partial J}{\partial \boldsymbol{w}}$</p>
<ul>
<li>for single feature multiple examples: <script type="math/tex; mode=display">\frac{\partial J}{\partial w}=\sum_{i=1}^m(wx^{(i)}-y^{(i)})</script></li>
<li>for multiple features one single instance: <script type="math/tex; mode=display">\frac{\partial J}{\partial \boldsymbol{w}}=(\boldsymbol{w^Tx}-y)\boldsymbol{x}</script></li>
<li>for multiple features multiple examples: <script type="math/tex; mode=display">\frac{\partial J}{\partial \boldsymbol{w}}=\frac{1}{m}\mathbf{X}^T(\mathbf{X}\boldsymbol{w}-\boldsymbol{y})</script></li>
</ul>
<p>stop until paremeters converge or reach a certain number of iterations.</p>
<ul>
<li>$f(x)=g(x)+h(x)\quad\to\quad f’(x)=g’(x)+h’(x)$</li>
<li>$f(x)=g(x)h(x)\quad\to\quad f’(x)=g’(x)h(x)+g(x)h’(x)$</li>
<li>$f(x)=\frac{g(x)}{h(x)}\quad\to\quad f’(x)=\frac{g’(x)h(x)-g(x)h’(x)}{h(x)^2}$</li>
<li>$f(x)=g(u),g(u)=h(x)\quad\to\quad f’(x)=g’(u)h’(x)$</li>
</ul>
<p><img src="/images/CS5242/Summary/vecBYvec0.png" alt="vecBYvec0"><br><img src="/images/CS5242/Summary/vecBYvec.png" alt="vecBYvec"><br><img src="/images/CS5242/Summary/scalBYvec.png" alt="scalBYvec"></p>
<p><strong>Example:</strong><br>$y=(\mathbf{A}\boldsymbol{x})^T(2\boldsymbol{x+z})$ , $\mathbf{A}$ is a square matrix, $\boldsymbol{x}$ and $\boldsymbol{z}$ are vectors, y is a scalar, what is $\frac{\partial y}{\partial \boldsymbol{x}}$ ?</p>
<script type="math/tex; mode=display">\boldsymbol{u}=\mathbf{A}\boldsymbol{x}\quad \boldsymbol{v}=2\boldsymbol{x+z}</script><script type="math/tex; mode=display">\begin{aligned}\frac{\partial y}{\partial \boldsymbol{x}}&=\frac{\partial \boldsymbol{u^Tv}}{\partial \boldsymbol{x}}=\frac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}}\cdot\boldsymbol{v}+\frac{\partial \boldsymbol{v}}{\partial \boldsymbol{x}}\cdot\boldsymbol{u}\\&=\mathbf{A}^T(2\boldsymbol{x+z})+(2\boldsymbol{I})\cdot(\mathbf{A}\boldsymbol{x})\\&=2\mathbf{A}^T\boldsymbol{x}+\mathbf{A}^T\boldsymbol{z}+2\mathbf{A}\boldsymbol{x}\end{aligned}</script><p>$L=\frac{1}{2}(\boldsymbol{w^Tx}-y)^2$, $\boldsymbol{x}=(1,2)^T$, $\boldsymbol{w}=(2,1)^T$, $y=0$, compute the gradient of $\frac{\partial L}{\partial \boldsymbol{w}}$</p>
<script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial \boldsymbol{w}}&=(\boldsymbol{w^Tx}-y)\boldsymbol{x}\\&=(\begin{pmatrix}2,1\end{pmatrix}\cdot\begin{pmatrix}1\\2\end{pmatrix}-0)\cdot\begin{pmatrix}1\\2\end{pmatrix}\\&=(4-0)\cdot\begin{pmatrix}1\\2\end{pmatrix}\\&=\begin{pmatrix}4\\8\end{pmatrix}\end{aligned}</script><h1 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h1><h2 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h2><p>From Linear regression to Logistic regression:<br>The function <script type="math/tex">\widetilde{y}=\begin{cases} 1,\qquad\qquad if\boldsymbol{w^Tx}>0 \\ 0,\qquad\qquad else\end{cases}</script> is not differentiable, therefore we use Logistic function as the probability: <script type="math/tex">p=\sigma (z) = \sigma(\boldsymbol{w^Tx})=\frac{1}{1+e^{-\boldsymbol{w^Tx}}}</script><br>$L2$ loss function: <script type="math/tex">L=\frac{1}{2}||p-y||^2=\frac{1}{2}||\sigma(z)-y||^2</script><br>thus the gradient is: <script type="math/tex">\frac{\partial L}{\partial \boldsymbol{w}}=(\sigma(z)-y)\sigma(z)(1-\sigma(z))\boldsymbol{x}</script><br>There exists gradient vanishing. In order to solve this problem, we use the <strong>binary cross entropy</strong> to compute the loss: <script type="math/tex">L=-ylogp-(1-y)log(1-p)</script><br>The gradient now is : <script type="math/tex">\frac{\partial L}{\partial \boldsymbol{w}}=(p-y)\boldsymbol{x}</script></p>
<script type="math/tex; mode=display">z=\boldsymbol{w^Tx},\boldsymbol{x}\in R^{n\times 1}, \boldsymbol{w}\in R^{n\times 1}</script><script type="math/tex; mode=display">p=\sigma (z)=\frac{1}{1+e^{-z}}</script><script type="math/tex; mode=display">L(p, y)=-ylogp-(1-y)log(1-p)</script><script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol{w}}=(p-y)\boldsymbol{x}</script><h2 id="Multi-label-classification"><a href="#Multi-label-classification" class="headerlink" title="Multi-label classification"></a>Multi-label classification</h2><p>对第 $i$ 个类别有：<br>$z_i=\boldsymbol{W}_i\boldsymbol{x}+b_i\quad p_i=\sigma(z_i)\quad L_{ce}=-y_ilogp_i-(1-y_i)log(1-p_i)$<br>对于所有类别向量化表示为:<br>$\boldsymbol{z}=\boldsymbol{W}\boldsymbol{x}+\boldsymbol{b}\quad \boldsymbol{p}=\sigma(\boldsymbol{z})\quad L_{ce}=-\boldsymbol{y}^Tlog\boldsymbol{p}-(1-\boldsymbol{y})^Tlog(1-\boldsymbol{p})$</p>
<h2 id="Softmax-multinomial-classification"><a href="#Softmax-multinomial-classification" class="headerlink" title="Softmax/multinomial classification"></a>Softmax/multinomial classification</h2><p>Multi-class single-label: classes are not independent, they are exclusive. </p>
<ul>
<li><strong>Softmax function</strong> <script type="math/tex; mode=display">p_i=\frac{e^{z_i}}{\sum_j e^{z_j}}=softmax(z_i)</script></li>
<li><strong>Cross-entropy loss</strong><script type="math/tex; mode=display">L_{ce}(\boldsymbol{x},\boldsymbol{y})=\sum_i-y_ilogp_i=-\boldsymbol{y}^Tlog\boldsymbol{p}</script><script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{W}}=\frac{1}{m}(\mathbf{P}-\mathbf{Y})\mathbf{X}^T\in R^{k\times n}, \mathbf{X}\in R^{(m\times n)} ,\mathbf{Y}\in R^{(m\times k)}</script></li>
</ul>
<p><strong>Detail:</strong><br>$L=\frac{1}{2}(\sigma(\boldsymbol{w^Tx})-y)^2$</p>
<script type="math/tex; mode=display">z=\boldsymbol{w^Tx}</script><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial \boldsymbol{w}}&=(\sigma(z)-y)\frac{\partial \sigma(z)}{\partial z}\frac{\partial z}{\boldsymbol{w}}\\&=(\sigma(z)-y)\frac{\partial (1-e^{-z})^{-1}}{\partial z}\boldsymbol{x}\\&=(\sigma(z)-y)(-1(1-e^{-z})^{-2})\frac{\partial (1-e^{-z})}{\partial z}\boldsymbol{x}\\&=(\sigma(z)-y)(-\sigma(z)^2)\frac{\partial (1-e^{-z})}{\partial z}\boldsymbol{x}\\&=(\sigma(z)-y)(-\sigma(z)^2)e^{-z}\boldsymbol{x}\\&=(\sigma(z)-y)(-\sigma(z)^2)(1-\frac{1}{\sigma(z)})\boldsymbol{x}\\&=(\sigma(z)-y)(-\sigma(z))(\sigma(z)-1)\boldsymbol{x}\\&=(\sigma(z)-y)\sigma(z)(1-\sigma(z))\boldsymbol{x}\end{aligned}</script><p>$L_{ce}=-ylogp-(1-y)log(1-p)\quad p=\sigma(z)$</p>
<script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial \boldsymbol{w}}&=\frac{\partial L}{\partial p}\frac{\partial p}{\partial z}\frac{\partial z}{\partial \boldsymbol{w}}\\&=(-\frac{y}{p}+\frac{1-y}{1-p})p(1-p)\boldsymbol{x}\\&=(-y+py+p-py)\boldsymbol{x}\\&=(p-y)\boldsymbol{x}\end{aligned}</script><p>$L_{ce}(\boldsymbol{x},\boldsymbol{y})=\sum_i-y_ilogp_i=-\boldsymbol{y^T}log\boldsymbol{p}$</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial \mathbf{W}}=\frac{1}{m}(\mathbf{P}-\mathbf{Y})\mathbf{X}^T\in R^{k\times n}</script><script type="math/tex; mode=display">\mathbf{X}\in R^{(m\times n)} ,\mathbf{Y}\in R^{(m\times k)}</script><h1 id="From-Shallow-to-Deep-Neural-Network"><a href="#From-Shallow-to-Deep-Neural-Network" class="headerlink" title="From Shallow to Deep Neural Network"></a>From Shallow to Deep Neural Network</h1><h2 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h2><p>A net with multiple layers that transform input features into hidden features and then make predictions .</p>
<ul>
<li>At least one non-linear hidden layer</li>
<li>A linear function is always followed by a non-linear function</li>
</ul>
<script type="math/tex; mode=display">\boldsymbol{z}^{[i]}=a^{[i]}(\boldsymbol{h}^{[i-1]})=W^{[i]}\boldsymbol{h}^{[i-1]}+\boldsymbol{b}^{[i]}</script><script type="math/tex; mode=display">W^{[i]}\in R^{n_i\times n_{i-1}}\quad \boldsymbol{b}^{[i]}\in R^{n_i}</script><p><img src="/images/CS5242/3/MLP.png" alt="MLP"></p>
<h2 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h2><p><img src="/images/CS5242/Summary/chain-rule.png" alt="chain-rule"></p>
<h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p><img src="/images/CS5242/Summary/Backpropagation.png" alt="Backpropagation"></p>
<ul>
<li><p>Add bias<br>$\boldsymbol{A}\in R^{m\times n}\quad b\in R^{1\times n}$<br>Forward($\boldsymbol{A},b$): $\boldsymbol{C}=\boldsymbol{A}+b$<br>Backward($d\boldsymbol{C},\boldsymbol{A},b$): $d\boldsymbol{A}=d\boldsymbol{C}\quad db=1^Td\boldsymbol{C}$</p>
</li>
<li><p>Array and scalar multiplication<br>$\boldsymbol{v}$ is an array, $k$ is a scalar(usually hyperparameter)<br>Forward($\boldsymbol{v},k$): $\boldsymbol{c}=k\boldsymbol{v}$<br>Backward($d\boldsymbol{c},\boldsymbol{v},k$): $d\boldsymbol{v}=kd\boldsymbol{c}$</p>
</li>
<li><p>Matmul matrix multiplication operation<br>$\boldsymbol{A}\in R^{m\times k}\quad \boldsymbol{B}\in R^{k\times n}$ (including matrix with a single column or row)<br>Forward($\boldsymbol{A},\boldsymbol{B}$):$\boldsymbol{C=A\cdot B^T}\in R^{m\times n}$<br>Backward($d\boldsymbol{C},\boldsymbol{A},\boldsymbol{B}$): $d\boldsymbol{A}=d\boldsymbol{C}\cdot \boldsymbol{B^T}\quad d\boldsymbol{B}=\boldsymbol{A^T}\cdot d\boldsymbol{C}$</p>
</li>
<li><p>Logistic operation<br>$a$ is an array of any shape<br>Forward($a$): $b=\sigma(a)$<br>Backward($db,a$): $da=db\times b\times (1-b)$</p>
</li>
<li><p>Softmax-Cross-entropy operation<br>Forward($\boldsymbol{Z},\boldsymbol{P}$): $\boldsymbol{P}=softmax(\boldsymbol{Z})\quad L=\frac{1}{m}sum(-YlogP) $<br>Backward($\boldsymbol{Z},\boldsymbol{P}$): $d\boldsymbol{Z}=\frac{1}{m}(\boldsymbol{P}-\boldsymbol{Y})$</p>
</li>
</ul>
<h1 id="Training-Deep-Networks"><a href="#Training-Deep-Networks" class="headerlink" title="Training Deep Networks"></a>Training Deep Networks</h1><h2 id="Mini-batch-stochastic-gradient-descent-SGD"><a href="#Mini-batch-stochastic-gradient-descent-SGD" class="headerlink" title="Mini-batch stochastic gradient descent (SGD)"></a>Mini-batch stochastic gradient descent (SGD)</h2><p>• Reduces the chance of local optimal points and saddle points from GD<br>• More stable and smooth than standard SGD<br>• Extensions: Momentum, RMSProp, Adam</p>
<ul>
<li>Smooth the gradients by preserving historical gradients</li>
<li>Adaptive learning rate per parameter</li>
</ul>
<p>Momentum:</p>
<script type="math/tex; mode=display">v=\beta v+\alpha g</script><script type="math/tex; mode=display">w=w-v</script><p>RMSprop:</p>
<script type="math/tex; mode=display">s=\beta s+(1-\beta)g^2\qquad moving\quad average</script><script type="math/tex; mode=display">w=w-\frac{\alpha}{\sqrt{s+\epsilon}}g\qquad nomalize\quad the\quad gradient</script><p>Adam:</p>
<script type="math/tex; mode=display">v=\beta_1 v+(1-\beta_1)g\quad \hat v = \frac{v}{1-\beta_1^t}</script><script type="math/tex; mode=display">s=\beta_2 s+(1-\beta_2)g^2\quad \hat s = \frac{s}{1-\beta_2^t}</script><script type="math/tex; mode=display">w=w-\frac{\alpha}{\sqrt{s+\epsilon}} v=w-\frac{\alpha}{\sqrt{\hat s+\epsilon}}\hat v</script><h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><ul>
<li>Learning rate decay: start large and decrease gradually</li>
<li>Randomly initialize parameters: break symmetry<br>and avoid gradient vanishing / exploding</li>
<li>Data normalization</li>
</ul>
<h2 id="Overfitting-and-underfitting"><a href="#Overfitting-and-underfitting" class="headerlink" title="Overfitting and underfitting"></a>Overfitting and underfitting</h2><ul>
<li>Underfitting means high bias, Overfitting means high variance.</li>
<li>Regularization: Early stop and L2 Norm.</li>
<li>Model capacity: the ability for the model to fit different functions or datasets; with more unconstrained parameters, the model can fit more functions and datasets.</li>
<li>Hyper-parameter tuning:<ul>
<li>Tune parameters using training data.</li>
<li>Tune hyper-parameters using validation data.</li>
<li>Report model performance using test data.</li>
</ul>
</li>
</ul>
<h1 id="Convolution-and-Pooling"><a href="#Convolution-and-Pooling" class="headerlink" title="Convolution and Pooling"></a>Convolution and Pooling</h1><h2 id="2D-Convolution"><a href="#2D-Convolution" class="headerlink" title="2D Convolution"></a>2D Convolution</h2><p>Convolution is an affine transformation.<br>Each receptive field generates aone output value: sparse connection<br># kernels = # output feature maps<br>Output size: $(c_0,o_h,o_w)=(c_o,\lfloor\frac{n_h+p_h-k_h}{s_h}\rfloor+1,\lfloor\frac{n_w+p_w-k_w}{s_w}\rfloor+1)$</p>
<p>Share the same parameters across different locations: reduce parameter, location invariant, </p>
<p>Implementation: Receptive fields across feature maps are concatenated into to one column.<br><img src="/images/CS5242/Summary/conv2D.png" alt="conv2D"></p>
<h2 id="3D-Convolution"><a href="#3D-Convolution" class="headerlink" title="3D Convolution"></a>3D Convolution</h2><p><img src="/images/CS5242/Summary/conv3D.png" alt="conv3D"></p>
<h2 id="Average-amp-max-pooling"><a href="#Average-amp-max-pooling" class="headerlink" title="Average &amp; max pooling"></a>Average &amp; max pooling</h2><ul>
<li>Aggregate the information from each receptive field.</li>
<li>The output is invariant to some variations of the input. eg. rotation.</li>
<li>Stride is usually &gt; 1 to reduce dimensionality </li>
<li>Pooling is applied to each channel / feature map separately</li>
</ul>
<p><img src="/images/CS5242/Summary/pooling.png" alt="pooling"></p>
<ul>
<li>Information will be lost from pooling but local “useful” information remains.</li>
<li>We don’t need to keep as many features as possible because we want to derive higher levels of abstraction. e.g. from simple edges to parts of objects</li>
</ul>
<h1 id="ConvNet-Architecture"><a href="#ConvNet-Architecture" class="headerlink" title="ConvNet Architecture"></a>ConvNet Architecture</h1><h2 id="Neocognitron"><a href="#Neocognitron" class="headerlink" title="Neocognitron"></a>Neocognitron</h2><ul>
<li><strong>Hierarchical Feature Extraction + Multi-Layer + Hand-crafted weights.</strong></li>
</ul>
<h2 id="LeNet-1-5"><a href="#LeNet-1-5" class="headerlink" title="LeNet 1-5"></a>LeNet 1-5</h2><ul>
<li><strong>Small CNN with convolution + pooling (subsampling)</strong></li>
</ul>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><ul>
<li><strong>GPUs</strong> (instead of CPUS): Fast training </li>
<li><strong>Ensemble modelling</strong></li>
<li><strong>ReLU</strong>: Reduce the chance of gradient vanishing</li>
<li><strong>Dropout</strong>: Multiple the outputs (h) with scale 1/(1-p); Regularization (Similar to L2 norm) </li>
<li><strong>Image augmentation</strong>: Done on-the-fly during training, so no need to store entire (augmented) dataset and occupy memory. Random operation for training; No random operations during test,  Make predictions by aggregating the results from all augmented images.(Voting)</li>
</ul>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><ul>
<li><strong>Uniform kernel size</strong></li>
<li><strong>Consecutive convolution layers</strong></li>
</ul>
<h2 id="InceptionNetV1"><a href="#InceptionNetV1" class="headerlink" title="InceptionNetV1"></a>InceptionNetV1</h2><ul>
<li><strong>Parallel paths</strong>: inception block (1x1 convolution),  insert the 1x1 convolution layer with a small number of kernels to <em>fuse the channels</em> from the input tensor and <em>reduce the computational cost</em> for the next convolution</li>
<li><strong>Complexity optimization</strong></li>
<li><strong>Average pooling</strong>: Reduce model size (less overfitting); Reduce time complexity </li>
<li><strong>New image augmentation methods</strong></li>
</ul>
<h2 id="InceptionNetV2"><a href="#InceptionNetV2" class="headerlink" title="InceptionNetV2"></a>InceptionNetV2</h2><ul>
<li><strong>Batch Normalization</strong><br>Computes the mean and variance over <em>mini-batch samples’s features</em>.<br>Normalize every neuron of each sample.<br>Applied after linear transformations and before activation.<br>(z = relu(BN(conv/ fc(x))) or z = BN(relu(conv/ fc)))<br>Accumulates the mean and variance from every batch during training, and then apply them during test</li>
</ul>
<h2 id="InceptionNetV3"><a href="#InceptionNetV3" class="headerlink" title="InceptionNetV3"></a>InceptionNetV3</h2><ul>
<li><strong>Spatially separable convolutions</strong>(Factorization of the kernel): Reduce computation cost $\to$ train faster ( 7x7 == 1x7 and 7x1 (receptive field))</li>
<li><strong>Label smoothing</strong>: prevents network from becoming over-confident and optimizing towards values that cannot be achieved</li>
</ul>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><ul>
<li><strong>Skip/Residual-connection</strong>: Reduct gradient vanishing. </li>
</ul>
<h2 id="XceptionNet"><a href="#XceptionNet" class="headerlink" title="XceptionNet"></a>XceptionNet</h2><ul>
<li><strong>Depth-wise spatial convolution</strong>: 2D Convolution applied to each channel independently; Number of input channels = number of kernels = number of output channels: $c_i=c_o$, Each kernel has a single channel </li>
<li><strong>Pointwise convolution</strong>: Normal 2D convolution with kernel height and width: 1x1</li>
</ul>
<h2 id="Neural-Architecture-Search-NAS"><a href="#Neural-Architecture-Search-NAS" class="headerlink" title="Neural Architecture Search (NAS)"></a>Neural Architecture Search (NAS)</h2><ul>
<li><strong>Model Compression</strong>: Prune layers, Low-precision representation<br><img src="/images/CS5242/7/model-compression.png" alt="model-compression"></li>
</ul>
<h2 id="Transfer-Learning-with-DCNN"><a href="#Transfer-Learning-with-DCNN" class="headerlink" title="Transfer Learning with DCNN"></a>Transfer Learning with DCNN</h2><ol>
<li>Train model (or use someone else’s model) on ImageNet 2012 training set </li>
<li>Re-train classifier on new dataset //Just the top layer (softmax or SVM) </li>
<li>Classify test set of new dataset<br><img src="/images/CS5242/7/transfer-learning.png" alt="transfer-learning"></li>
</ol>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>PPT for &quot;The Case For Learned Index Structures&quot;</title>
    <url>/2020/09/11/Reading_ppt_Learned_Index/</url>
    <content><![CDATA[<p>Paper: <a href="https://www.arxiv-vanity.com/papers/1712.01208/">https://www.arxiv-vanity.com/papers/1712.01208/</a><br><a id="more"></a></p>
<object data="/pdf/nus/CS6203/Learned_Index.pdf" type="application/pdf" width="100%" height="877px"></object>]]></content>
      <categories>
        <category>Readings</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>DB</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for &quot;The Case For Learned Index Structures&quot;</title>
    <url>/2020/09/11/Reading_Note_Learned_Index/</url>
    <content><![CDATA[<p>Paper: <a href="https://www.arxiv-vanity.com/papers/1712.01208/">https://www.arxiv-vanity.com/papers/1712.01208/</a> </p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>There exists various data access patterns, and correspondingly various choices of index structure.<br>B-tree: range request  (from sorted array)<br>HashMap: single key look-ups (from unsorted array)<br>Bloom filters: check for record existence<br><a id="more"></a></p>
<p>If data is from 1 to 1000, we don’t actually need a tree structure, we just need to use a direct look-up for the key. This example shows that <strong>One size does not fit all</strong>.<br>Traditional data structures make no assumptions about data, they only rely on the size of data.<br>While using ML, we try to learn the distribution of data,  We use data complexity instead of data size to construct a suitable index. In this way, we can achieve huge performance improvement.</p>
<p>There are also many other data structures and algorithms in database system, we can apply the idea of using ML to change the design of them.</p>
<p>Basically, we want to use ML techniques to enhance traditional index structures or even replace them to get a better performance.</p>
<h1 id="Range-Index-B-tree"><a href="#Range-Index-B-tree" class="headerlink" title="Range Index (B-tree)"></a>Range Index (B-tree)</h1><h2 id="Range-Index-Models-are-CDF-Models"><a href="#Range-Index-Models-are-CDF-Models" class="headerlink" title="Range Index Models are CDF Models"></a>Range Index Models are CDF Models</h2><p>B-tree can be viewed as a model which takes the feature’s key as input and predict the position of the feature. Usually a B-tree maps a key to a page, and then searches within this page. Which means B-tree outputs a position, and searches within $[pos, pos+page_ size]$. i.e., by normal B-trees, the error is defined by the pagesize.</p>
<p>model:f(key)-&gt;pos, searches from $[pos-err_{min},pos+err_{max}]$<br>we can search in this range to find the exact right record.<br><img src="/images/CS6203/index-1.png" alt="history"></p>
<p><strong>We don’t care about generalization, we only care about the data we have.</strong></p>
<p>So B-tree is actually building a model of Cumulative Distribution Function:<br>$p=P(x\leq key)*nums_of_Keys$<br>p is the predict position from key.<br><img src="/images/CS6203/index-2.png" alt="history"><br>For example, if your data is a Gaussian Distribution, most of your data is around the middle, and the learned index can allocate more trees in that area because of this information.</p>
<p>Potantial Advantages of learned b-tree models:<br>smaller index: less main-memory storage<br>faster lookup<br>more parallelism: sequential if-statement is exchanged for multiplications</p>
<p>assume read-only</p>
<h2 id="A-First-Naive-Learned-Index"><a href="#A-First-Naive-Learned-Index" class="headerlink" title="A First Naive Learned Index"></a>A First Naive Learned Index</h2><p>200 M records of timestamps (sorted)<br>two-layer fully-connected neural network<br>32 neurons per layer<br>ReLu activation functions<br>train this model to predict the position of the timestamp<br>measure the look-up time for a randomly selected key (averaged over several runs)</p>
<p>look-up with B-tree: 300ns<br>look-up with binary search: 900ns<br>look-up with Model: 80000ns (1250 predictions per second)</p>
<p>This model is a naive version and doesn’t work well and we find this model is limited in the following ways:</p>
<ol>
<li>Tensorflow is designed for large models, not small models like this.</li>
<li>B-tree can easily overfit the training data and can predict the position precisely. For database system, overfitting is not that bad, you get a database, you know the data you’re trying to search, and there are no other data outside to generalize,<br>While learned index can easily narrow the space from 100M to 10k but have trouble in narrowing the space from 100M to hundreds, which is “the last mile of learned index”, in other words, learned index has problems being accurate at the individual data instance level.</li>
<li>B-trees are cache efficient, they keep the top nodes always in cache and they access other pages if needed. In contrast NN need to load the all parameters into memory to compute a prediction and have a high cost in the number of multiplications.</li>
<li>For classic machine learning problems, the ultimate goal is the desired average average error, but we look for the index, hoping to get the best prediction, and finally hoping to find the true position of the key. Search does not make advantage of this prediction, the question is what is the right search style given an approximate answer</li>
</ol>
<h1 id="RM-Index-B-tree"><a href="#RM-Index-B-tree" class="headerlink" title="RM-Index (B-tree)"></a>RM-Index (B-tree)</h1><h2 id="The-Learning-Index-Framework-LIF"><a href="#The-Learning-Index-Framework-LIF" class="headerlink" title="The Learning Index Framework (LIF)"></a>The Learning Index Framework (LIF)</h2><p>LIF is an index optimization system. Given a set of index data, LIF automatically generates multiple sets of hyperparameters, and automatically optimizes and tests to find the optimal set of configurations.<br>When applied in a real database, LIF will generate an efficient index structure based on C++. LIF focuses on small models, so many unnecessary costs for managing large models must be reduced(Use a specific model structure to reduce runtime overhead, such as pruning, removing redundant if-else, direct memory access, etc.), now we can optimize the model prediction to within 30ns.<br><em>LIF is still an experimental framework and is instumentalized to quickly evaluate different index configurations.</em></p>
<h2 id="Recursive-Model-Index"><a href="#Recursive-Model-Index" class="headerlink" title="Recursive Model Index"></a>Recursive Model Index</h2><p>The real data distribution function is often between the two: in general, it satisfies a change trend, but the part is a piecewise function (the piecewise function of daily data may be different)<br><img src="/images/CS6203/index-2.png" alt="history"></p>
<p>In that case we use the idea of Mixture-of-Experts and propose Recursive Model Index to solve the problem of “the last mile of learned index”</p>
<p>We use a hierarchical experts, i.e. a hierarchy of models.<br>At each stage, take the key as input and based on it to pick another model, until final stage predicts the position.</p>
<p>Every model is like an expert for a certain range and may has a better prediction.</p>
<ul>
<li>Such a multi-layer model structure is not a search tree, and multiple stage 2 models may point to the same stage 3 model. In addition, at the lowest level, in order to ensure that the smallest and largest deviations can be controlled, we can also use B-Tree to complete the final index.</li>
<li>Each model does not need to cover the same amount of records like B-trees do.</li>
<li>predictions between stages don’t need to be the estimate of position, but can be considered as picking an expert which has a better knowledge about certain keys.</li>
</ul>
<p><img src="/images/CS6203/index-3.png" alt="history"><br>RMI has several benefits:</p>
<ol>
<li>It separates model size and complexity from execution cost</li>
<li>It is easy to learn the overall data distribution</li>
<li>Divide the entire space into smaller subranges. Each subranges is similar to a B-tree or decision tree, making it easier to solve the last-mile accuracy problem.</li>
<li>No search process is required in-between stages. For example, the output y of model 1.1 is an offset, which can be directly used to select the model of the next stage.</li>
</ol>
<h2 id="Hybrid-Indexes"><a href="#Hybrid-Indexes" class="headerlink" title="Hybrid Indexes"></a>Hybrid Indexes</h2><p>Another advantage of the recursive model index is that we are able to build mixtures of models.<br>top-layer: a small ReLU neural net —- they can learn a wide range of  complex data<br>bottom: simple models like linear regression(inexpensive in space and execution), or B-trees if the data is particularly hard to learn(worst case)</p>
<p>Such a recursive model has several advantages: </p>
<ol>
<li>It can learn the shape of the output data well. </li>
<li>Through stratification, you can basically use B-Tree or decision tree to obtain very high search efficiency. </li>
<li>Different stages can use penetration to initialize the model instead of searching, which makes it easier to refactor and optimize the programming language.</li>
</ol>
<h2 id="Search-Strategy"><a href="#Search-Strategy" class="headerlink" title="Search Strategy"></a>Search Strategy</h2><ol>
<li>Model Biased Search：the default search strategy, based on binary search, the new intermediate point is based on the standard deviation $\sigma$ of the last layer of model settings</li>
<li>Biased Quaternary Search：search three points at the same time, $pos-\sigma, pos, pos+\sigma$ . Requires CPU to obtain multiple data addresses in parallel from main memory</li>
</ol>
<ul>
<li>how to deal with large address spaces</li>
<li>how do we deal with not able to assume that the end location is ordered<br>if pages are in disks or other seperate devices they may not be in order<br>it’s an engineering issue</li>
</ul>
<h2 id="Indexing-Strings"><a href="#Indexing-Strings" class="headerlink" title="Indexing Strings"></a>Indexing Strings</h2><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>trade-off between memory and latency</p>
<p>Map data: relatively linear<br>Web data: (timestamp) complex, a worst case<br>Log Normal Data: non-linear integers, heavy tail<br>2nd stage: linear models have the best performance.<br>For the last mile we don’t need to execute complex models.</p>
<p><img src="/images/CS6203/index-4.png" alt="history"></p>
<ul>
<li>Histogram: B-Trees approximate the CDF of the underlying data distribution. When we use  histograms as a CDF,this histogram must be a low error approximation of the CDF. However,  this requires a large number of buckets, which makes it expensive to search the histogram itself. And almost every bucket has data insides, only few buckets are empty or too full. The solution to this case is B-Trees, so we’ll not discuss Histogram individually.</li>
<li>Lookup-Table: Lookup-table is another simple alternative to B-trees. A lookup table is an array that replaces runtime computation with a simpler array indexing operation(Wikipedia).<blockquote>
<p>Performance comparison: linear search vs binary search.<br><a href="https://dirtyhandscoding.wordpress.com/2017/08/25/performance-comparison-linear-search-vs-binary-search/">https://dirtyhandscoding.wordpress.com/2017/08/25/performance-comparison-linear-search-vs-binary-search/</a></p>
</blockquote>
</li>
<li>FAST: a highly SIMD optimized data structure, allocate memory in the power of 2, leading to significantly larger indexes</li>
<li>Fixed-size B-Tree &amp; interpolation search</li>
<li>Learned indexes without overhead: a multivariate linear regression model at the top and simple linear models at bottom<br><img src="/images/CS6203/index-5.png" alt="history"></li>
</ul>
<p>Because the binary search of string will be more time-consuming, the search strategy proposed above(learned QS with quaternary search) can improve efficiency more significantly<br><img src="/images/CS6203/index-6.png" alt="history"></p>
<h1 id="Point-Index-hash"><a href="#Point-Index-hash" class="headerlink" title="Point Index (hash)"></a>Point Index (hash)</h1><p>Point Index, as the name suggests, is to search for the position corresponding to the key, which is Hashmap. Hashmap is naturally a model, the input is the key and the output is the position. For Hashmap, in order to reduce conflicts, it is often necessary to use a much larger number of slots than the number of keys (the paper mentions that a hashmap usually has 78% extra slots in Google). For conflicts, although there are linked-list or secondary probing methods to deal with, their overhead is not small. Even using sparse-hashmap to optimize memory usage, the performance is 3-7 times slower than hashmap. </p>
<p>Therefore,  the learned model can reduce the additional memory overhead on the one hand, and reduce the overhead of some conflicts on the other hand (Hashmap can avoid conflicts by using more slots. If there are few conflicts, it will be difficult to optimize the index Time Because Hashmap is inherently constant complexity, the goal here is to reduce the memory overhead of Hashmap while ensuring indexing time). As shown in the figure below, the Learned Hashmap can not only reduce conflicts, but also make better use of memory space under the same number of slots.</p>
<p>The point index does not need to ensure that the records are continuous (maxerr and minerr do not need to be considered), so the trained model F fit CDF is easier to meet<br><img src="/images/CS6203/index-7.png" alt="history"><br>After the hash function is replaced with the model, the index time is not shortened, but the key space is indeed effectively reduced, which improves the space utilization<br><img src="/images/CS6203/index-8.png" alt="history"></p>
<h1 id="Existence-Index-Bloom-Filters"><a href="#Existence-Index-Bloom-Filters" class="headerlink" title="Existence Index (Bloom-Filters)"></a>Existence Index (Bloom-Filters)</h1><p>Although Bloom filters allow false positives,<br>for many applications the space savings outweigh this drawback</p>
<p>Considering the usage scenario of the existence index is usually to first determine whether the record exists in the cold storage, and then access the data through disk or network. So the latency can be relatively high, so that we can use more complex RNN or CNN models as classifiers.</p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><h2 id="B-Tree-and-Variants"><a href="#B-Tree-and-Variants" class="headerlink" title="B-Tree and Variants"></a>B-Tree and Variants</h2><p>Over the last few years, a variety of different index structures have been proposed, such as B+-trees, red-black trees, all aiming at improving the performance of original main-memory trees. However, none of the structures have learned from the data distribution.</p>
<p>While using our hybrid model, we can combine the existing index structures with the learned index for further performance gains.</p>
<h2 id="Perfect-hashing"><a href="#Perfect-hashing" class="headerlink" title="Perfect hashing"></a>Perfect hashing</h2><p>Perfect hashing tried to avoid conflicts without learning techniques.<br>The size of perfect hash function relies on data size.<br>In contrast, Learned hash function can be indenpendent of data size.<br>What’s more, perfect hash is not suitable for B-trees and Bloom filters.</p>
<h2 id="Bloom-filters"><a href="#Bloom-filters" class="headerlink" title="Bloom filters"></a>Bloom filters</h2><p>Our existence indexes directly make use of the current bloom filters by adding a ML model before the simple bloom filter and creating a classification model.<br>And in addition we can also use our learning model as a special hash function to seperate the keys and non-keys.</p>
<h2 id="Modeling-CDFs"><a href="#Modeling-CDFs" class="headerlink" title="Modeling CDFs"></a>Modeling CDFs</h2><p>Our models for both range and point indexes are closely tied to models of CDF.<br>How to mostly model CDF remanins an open question.</p>
<h2 id="Mixture-of-Experts"><a href="#Mixture-of-Experts" class="headerlink" title="Mixture of Experts"></a>Mixture of Experts</h2><p>Our RMI architecture builds experts for subsets of the data and in this way we can make our model size indepentent on model computation.<br>So we can create models that are complex but still cheap to execute.</p>
<h1 id="Conclusion-and-future-work"><a href="#Conclusion-and-future-work" class="headerlink" title="Conclusion and future work"></a>Conclusion and future work</h1><h2 id="Other-ML-Models"><a href="#Other-ML-Models" class="headerlink" title="Other ML Models"></a>Other ML Models</h2><p>We combine some simple neural networks with the existing data structures, and there are definitely more ways to do the combination which are worth exploring.</p>
<h2 id="Multi-Dimensional-Indexes"><a href="#Multi-Dimensional-Indexes" class="headerlink" title="Multi-Dimensional Indexes"></a>Multi-Dimensional Indexes</h2><p>Models like NN are extremely good at handling multi-dimensional data, and the learned model could be used to estimate the position given various attributes.</p>
<h2 id="Learned-Algorithms-beyond-indexing"><a href="#Learned-Algorithms-beyond-indexing" class="headerlink" title="Learned Algorithms beyond indexing"></a>Learned Algorithms beyond indexing</h2><p>Learned Algorithms can also be applied to other data structures and CDF models are beneficial to join and sorting operations as well.</p>
<h2 id="GPU-TPU"><a href="#GPU-TPU" class="headerlink" title="GPU/TPU"></a>GPU/TPU</h2><p>All we have been doing is to learn data pattern and use it in data structure design.<br>We are now having TPU/GPU which are vector processors and vector multiplies become very cheap.<br>We can make use of this property and construct new data structure along with ML techniques to speed up the look-up time and get better performance.</p>
<p><strong>In summary, we have demonstrated that machine learned models have the potential to provide significant benefits over state-of-the-art indexes, and we believe this is a fruitful direction for future research.</strong></p>
<blockquote>
<p>B-tree indexes and CPU caches<br><a href="https://www.researchgate.net/search.Search.html?type=publication&amp;query=%20B-tree%20indexes%20and%20CPU%20caches">https://www.researchgate.net/search.Search.html?type=publication&amp;query=%20B-tree%20indexes%20and%20CPU%20caches</a><br>Making B+-Trees Cache Conscious in Main Memory<br><a href="http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2006/Papers/cache-b-tree.pdf">http://www.cse.iitb.ac.in/infolab/Data/Courses/CS632/2006/Papers/cache-b-tree.pdf</a><br>Adaptive Range Filters for Cold Data: Avoiding Trips to Siberia<br><a href="http://www.vldb.org/pvldb/vol6/p1714-kossmann.pdf">http://www.vldb.org/pvldb/vol6/p1714-kossmann.pdf</a><br>Cuckoo Filter: Practically Better Than Bloom<br><a href="https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf">https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Readings</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>DB</tag>
      </tags>
  </entry>
  <entry>
    <title>PPT for &quot;Federated Machine Learning -- Concept and Applications&quot;</title>
    <url>/2020/10/30/Reading_ppt_FL/</url>
    <content><![CDATA[<p>Paper: <a href="https://arxiv.org/abs/1902.04885">https://arxiv.org/abs/1902.04885</a><br><a id="more"></a></p>
<object data="/pdf/nus/CS6203/Federated_Learning.pdf" type="application/pdf" width="100%" height="877px"></object>]]></content>
      <categories>
        <category>Readings</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>DB</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/07/12/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><a id="more"></a>
<p>ss<br> <strong>BP算法</strong></p>
<p>  训练集    $\left\{\left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right)\right\}$</p>
<p>  设    $\Delta_{i j}^{(l)}=0(\text { for all } l, i, j)$</p>
<p>  $\begin{array}{l}{\text {For } i=1 \text { to } m}\end{array}$</p>
<script type="math/tex; mode=display">
  \begin{array}{l}{\text { Set } a^{(1)}=x^{(i)}} \\ {\text { Perform forward propagation to compute } a^{(l)} \text { for } l=2,3, \ldots, L} \\ {\text { Using } y^{(i)}, \text { compute } \delta^{(L)}=a^{(L)}-y^{(i)}} \\ {\text { Compute } \delta^{(L-1)}, \delta^{(l+1)}, \ldots, \delta^{(2)}} \\ {\Delta_{i j}^{(l)} :=\Delta_{i j}^{(l)}+a_{j}^{(l)} \delta_{i}^{(l+1)}}\end{array}</script><p>  $\begin{array}{l}{D_{i j}^{(l)} :=\frac{1}{m} \Delta_{i j}^{(l)}+\lambda \Theta_{i j}^{(l)}} &amp; {\text { if } j \neq 0} \\ {D_{i j}^{(l)} :=\frac{1}{m} \Delta_{i j}^{(l)}} &amp; {\text { if } j=0}\end{array}$</p>
<p>  其中    $\frac{\partial}{\partial \Theta_{i j}^{(l)}} J(\Theta)=D_{i j}^{(l)}$</p>
<h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>树结构 Tree</title>
    <url>/2020/07/20/Tree%20Structure/</url>
    <content><![CDATA[<h2 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h2><p>一棵搜索树既可以作为一个字典又可以作为一个优先队列。</p>
<h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h2><h2 id="B树"><a href="#B树" class="headerlink" title="B树"></a>B树</h2><h2 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B+树</h2>]]></content>
      <categories>
        <category>学习笔记：数据结构</category>
      </categories>
      <tags>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Python</title>
    <url>/2020/07/28/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%EF%BC%9APython/</url>
    <content><![CDATA[<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h3 id="Python可变参数args，-kwargs"><a href="#Python可变参数args，-kwargs" class="headerlink" title="Python可变参数args，*kwargs"></a>Python可变参数<em>args，*</em>kwargs</h3><p>当函数的参数前面有一个星号的时候表示这是一个可变的位置参数<br>两个星号表示这个是一个可变的关键词参数。<br>星号<em>把序列或者集合解包（unpack）成位置参数，两个星号*</em>把字典解包成关键词参数。</p>
<h3 id="Python内置容器及其容器及其使用场景"><a href="#Python内置容器及其容器及其使用场景" class="headerlink" title="Python内置容器及其容器及其使用场景 "></a>Python内置容器及其容器及其使用场景 <a id="more"></a></h3><h3 id="python多线程"><a href="#python多线程" class="headerlink" title="python多线程"></a>python多线程</h3><h3 id="对Python了解吗？说说它底层的实现原理。"><a href="#对Python了解吗？说说它底层的实现原理。" class="headerlink" title="对Python了解吗？说说它底层的实现原理。"></a>对Python了解吗？说说它底层的实现原理。</h3><h3 id="什么是Python生成器，迭代器；"><a href="#什么是Python生成器，迭代器；" class="headerlink" title="什么是Python生成器，迭代器；"></a>什么是Python生成器，迭代器；</h3><h3 id="Python中的dict底层怎么实现的"><a href="#Python中的dict底层怎么实现的" class="headerlink" title="Python中的dict底层怎么实现的"></a>Python中的dict底层怎么实现的</h3><h3 id="Python多线程用了几个CPU"><a href="#Python多线程用了几个CPU" class="headerlink" title="Python多线程用了几个CPU"></a>Python多线程用了几个CPU</h3><h3 id="什么是装饰器；讲一讲装饰器，能手写实现一个装饰器么？"><a href="#什么是装饰器；讲一讲装饰器，能手写实现一个装饰器么？" class="headerlink" title="什么是装饰器；讲一讲装饰器，能手写实现一个装饰器么？"></a>什么是装饰器；讲一讲装饰器，能手写实现一个装饰器么？</h3><p>本质上，decorator就是一个返回函数的高阶函数。<br>@符号用做函数的修饰符，可以在模块或者类的定义层内对函数进行修饰，出现在函数定义的前一行，不允许和函数定义在同一行。一个修饰符就是一个函数，它将被修饰的函数作为参数，并返回修饰后的同名函数或其他可调用的东西（如果返回不是一个可调用的对象那么会报错）。每一层调用装饰都传递的是函数对象，每一层Wrap返回对象也是一个可调用的对象（如上例中的decorator）</p>
<h2 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h2><h3 id="C-中const与-define的区别与优势"><a href="#C-中const与-define的区别与优势" class="headerlink" title="C++中const与#define的区别与优势"></a>C++中const与#define的区别与优势</h3><h3 id="C-11新特性auto的用法"><a href="#C-11新特性auto的用法" class="headerlink" title="C++11新特性auto的用法"></a>C++11新特性auto的用法</h3><h3 id="C-虚函数，读程序写结果"><a href="#C-虚函数，读程序写结果" class="headerlink" title="C++虚函数，读程序写结果"></a>C++虚函数，读程序写结果</h3><h3 id="C-关键字static、extern"><a href="#C-关键字static、extern" class="headerlink" title="C++关键字static、extern"></a>C++关键字static、extern</h3><h3 id="C-关键字this，C-类的实例方法中能否写delete-this，这种情况的使用场景"><a href="#C-关键字this，C-类的实例方法中能否写delete-this，这种情况的使用场景" class="headerlink" title="C++关键字this，C++类的实例方法中能否写delete this，这种情况的使用场景"></a>C++关键字this，C++类的实例方法中能否写delete this，这种情况的使用场景</h3><h3 id="C-中new和delete语句的底层实现"><a href="#C-中new和delete语句的底层实现" class="headerlink" title="C++中new和delete语句的底层实现"></a>C++中new和delete语句的底层实现</h3><h2 id="编程语言之间的对比"><a href="#编程语言之间的对比" class="headerlink" title="编程语言之间的对比"></a>编程语言之间的对比</h2><h3 id="Java和C-相比有什么特点？"><a href="#Java和C-相比有什么特点？" class="headerlink" title="Java和C++相比有什么特点？"></a>Java和C++相比有什么特点？</h3><h3 id="C-为什么比Java运行得快？"><a href="#C-为什么比Java运行得快？" class="headerlink" title="C++为什么比Java运行得快？"></a>C++为什么比Java运行得快？</h3><h3 id="java和python的异同？"><a href="#java和python的异同？" class="headerlink" title="java和python的异同？"></a>java和python的异同？</h3><h2 id="计算机系统"><a href="#计算机系统" class="headerlink" title="计算机系统"></a>计算机系统</h2><h3 id="了解Linux么？用过哪些命令？"><a href="#了解Linux么？用过哪些命令？" class="headerlink" title="了解Linux么？用过哪些命令？"></a>了解Linux么？用过哪些命令？</h3><h3 id="SSO了解吗？"><a href="#SSO了解吗？" class="headerlink" title="SSO了解吗？"></a>SSO了解吗？</h3><h3 id="负载均衡了解吗？"><a href="#负载均衡了解吗？" class="headerlink" title="负载均衡了解吗？"></a>负载均衡了解吗？</h3><h3 id="MD5加密可逆吗？会被破解吗-如何破解？"><a href="#MD5加密可逆吗？会被破解吗-如何破解？" class="headerlink" title="MD5加密可逆吗？会被破解吗?如何破解？"></a>MD5加密可逆吗？会被破解吗?如何破解？</h3><h3 id="DDOS攻击（我答了SYN攻击，深入问了如何解决，关键点是什么）"><a href="#DDOS攻击（我答了SYN攻击，深入问了如何解决，关键点是什么）" class="headerlink" title="DDOS攻击（我答了SYN攻击，深入问了如何解决，关键点是什么）"></a>DDOS攻击（我答了SYN攻击，深入问了如何解决，关键点是什么）</h3><h3 id="并发用什么"><a href="#并发用什么" class="headerlink" title="并发用什么"></a>并发用什么</h3><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><h3 id="设计一个线程池"><a href="#设计一个线程池" class="headerlink" title="设计一个线程池"></a>设计一个线程池</h3><h3 id="解释下内存中堆和栈，写一个stack-overflow的代码"><a href="#解释下内存中堆和栈，写一个stack-overflow的代码" class="headerlink" title="解释下内存中堆和栈，写一个stack overflow的代码"></a>解释下内存中堆和栈，写一个stack overflow的代码</h3><h3 id="元组和list的区别；"><a href="#元组和list的区别；" class="headerlink" title="元组和list的区别；"></a>元组和list的区别；</h3><h3 id="list的底层怎么实现的；"><a href="#list的底层怎么实现的；" class="headerlink" title="list的底层怎么实现的；"></a>list的底层怎么实现的；</h3><h3 id="双等于和is有什么区别"><a href="#双等于和is有什么区别" class="headerlink" title="双等于和is有什么区别"></a>双等于和is有什么区别</h3><h3 id="栈的默认大小（1M）"><a href="#栈的默认大小（1M）" class="headerlink" title="栈的默认大小（1M）"></a>栈的默认大小（1M）</h3><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="说一说各种排序算法原理，及其时间复杂度。"><a href="#说一说各种排序算法原理，及其时间复杂度。" class="headerlink" title="说一说各种排序算法原理，及其时间复杂度。"></a>说一说各种排序算法原理，及其时间复杂度。</h3><h3 id="一道二叉树的算法题：给出一棵二叉树，求出所有左叶子结点的值的和"><a href="#一道二叉树的算法题：给出一棵二叉树，求出所有左叶子结点的值的和" class="headerlink" title="一道二叉树的算法题：给出一棵二叉树，求出所有左叶子结点的值的和"></a>一道二叉树的算法题：给出一棵二叉树，求出所有左叶子结点的值的和</h3><h3 id="栈，队列是什么？区别是什么？"><a href="#栈，队列是什么？区别是什么？" class="headerlink" title="栈，队列是什么？区别是什么？"></a>栈，队列是什么？区别是什么？</h3><h3 id="设计一个可以满足高效率获取第k大和前k个大的元素的数据结构"><a href="#设计一个可以满足高效率获取第k大和前k个大的元素的数据结构" class="headerlink" title="设计一个可以满足高效率获取第k大和前k个大的元素的数据结构"></a>设计一个可以满足高效率获取第k大和前k个大的元素的数据结构</h3><h3 id="手写求两个链表第一个交叉节点"><a href="#手写求两个链表第一个交叉节点" class="headerlink" title="手写求两个链表第一个交叉节点"></a>手写求两个链表第一个交叉节点</h3><h3 id="求x的y次方，想出比直接for循环更好的方案"><a href="#求x的y次方，想出比直接for循环更好的方案" class="headerlink" title="求x的y次方，想出比直接for循环更好的方案"></a>求x的y次方，想出比直接for循环更好的方案</h3><h3 id="求绝对众数"><a href="#求绝对众数" class="headerlink" title="求绝对众数"></a>求绝对众数</h3><h3 id="二叉树，输出所有和为n的路径（可以从中间结点到中间结点）"><a href="#二叉树，输出所有和为n的路径（可以从中间结点到中间结点）" class="headerlink" title="二叉树，输出所有和为n的路径（可以从中间结点到中间结点）"></a>二叉树，输出所有和为n的路径（可以从中间结点到中间结点）</h3><h3 id="如何用两个栈实现一个队列。"><a href="#如何用两个栈实现一个队列。" class="headerlink" title="如何用两个栈实现一个队列。"></a>如何用两个栈实现一个队列。</h3><h3 id="如何开启一个进程。"><a href="#如何开启一个进程。" class="headerlink" title="如何开启一个进程。"></a>如何开启一个进程。</h3><h3 id="实现一个-hashtable"><a href="#实现一个-hashtable" class="headerlink" title="实现一个 hashtable"></a>实现一个 hashtable</h3><h3 id="写代码：一个二叉树，每个节点除了有左右子节点外，还有指向父节点的引用。给出一个节点，返回它在二叉树中中序遍历的下一个节点。"><a href="#写代码：一个二叉树，每个节点除了有左右子节点外，还有指向父节点的引用。给出一个节点，返回它在二叉树中中序遍历的下一个节点。" class="headerlink" title="写代码：一个二叉树，每个节点除了有左右子节点外，还有指向父节点的引用。给出一个节点，返回它在二叉树中中序遍历的下一个节点。"></a>写代码：一个二叉树，每个节点除了有左右子节点外，还有指向父节点的引用。给出一个节点，返回它在二叉树中中序遍历的下一个节点。</h3><h3 id="最长公共连续子串"><a href="#最长公共连续子串" class="headerlink" title="最长公共连续子串"></a>最长公共连续子串</h3><h3 id="几十G的数据都是URL，内存空间只有1G，磁盘空间无限，统计频率最高的Top-10；"><a href="#几十G的数据都是URL，内存空间只有1G，磁盘空间无限，统计频率最高的Top-10；" class="headerlink" title="几十G的数据都是URL，内存空间只有1G，磁盘空间无限，统计频率最高的Top 10；"></a>几十G的数据都是URL，内存空间只有1G，磁盘空间无限，统计频率最高的Top 10；</h3><h3 id="判断平衡二叉树。"><a href="#判断平衡二叉树。" class="headerlink" title="判断平衡二叉树。"></a>判断平衡二叉树。</h3><h3 id="1亿数据取top10-，-1亿数据取出现频率top10，1亿URL取出重复URL"><a href="#1亿数据取top10-，-1亿数据取出现频率top10，1亿URL取出重复URL" class="headerlink" title="1亿数据取top10 ， 1亿数据取出现频率top10，1亿URL取出重复URL"></a>1亿数据取top10 ， 1亿数据取出现频率top10，1亿URL取出重复URL</h3><h3 id="有序数组转二叉搜索树"><a href="#有序数组转二叉搜索树" class="headerlink" title="有序数组转二叉搜索树"></a>有序数组转二叉搜索树</h3><h3 id="设计阻塞队列（不能用JDK的api"><a href="#设计阻塞队列（不能用JDK的api" class="headerlink" title="设计阻塞队列（不能用JDK的api"></a>设计阻塞队列（不能用JDK的api</h3><h3 id="了解哪些设计模式？手写一个工厂方法模式"><a href="#了解哪些设计模式？手写一个工厂方法模式" class="headerlink" title="了解哪些设计模式？手写一个工厂方法模式"></a>了解哪些设计模式？手写一个工厂方法模式</h3><h3 id="手撕sql"><a href="#手撕sql" class="headerlink" title="手撕sql"></a>手撕sql</h3><h3 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h3><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2>]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>programming language</tag>
      </tags>
  </entry>
  <entry>
    <title>JAVA</title>
    <url>/2020/07/28/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%EF%BC%9AJAVA/</url>
    <content><![CDATA[<h2 id="面向对象有哪些特征？解释一下"><a href="#面向对象有哪些特征？解释一下" class="headerlink" title="面向对象有哪些特征？解释一下"></a>面向对象有哪些特征？解释一下</h2><p>面向对象主要有四大特性：<em>抽象、封装、继承和多态</em>。<br>OO开发范式大致为：<strong>划分对象→抽象类→将类组织成为层次化结构(继承和合成) →用类与实例进行设计和实现几个阶段。</strong><a id="more"></a></p>
<blockquote>
<p>  <a href="https://juejin.im/entry/59f292635188254115701364">https://juejin.im/entry/59f292635188254115701364</a><br>    <a href="https://blog.csdn.net/cancan8538/article/details/8057095">https://blog.csdn.net/cancan8538/article/details/8057095</a></p>
</blockquote>
<ol>
<li>抽象就是将一类实体的共同特性抽象出来，封装在一个新的概念(类) 中，所以抽象是面向对象语言的基础。</li>
<li>封装特性是由类来体现的。将现实生活中的一类实体定义成类，其中包括属性和行为（在Java中就是方法）。</li>
<li>一个类可以继承另一个类的一些特性，从而可以代码重用，其实继承体现的是is-a关系，父类和子类在本质上还是一类实体。继承是一个对象获得另一个对象的属性的过程，使一个对象成为一个更具通用类的一个特定实例（可传递可扩展、可复用、可维护）通过继承创建的新类称为“子类”或“派生类”；被继承的类称为“基类”、“父类”或“超类”。</li>
<li>多态是允许一个接口被多个同类动作使用的特性，具体使用哪个动作与应用场合有关。多态就是通过传递给父类对象引用不同的子类对象从而表现出不同的行为，多态可为程序提供更好的可扩展性，同样也可以代码重用。<br>“向上转型”: Animal a = new Dog(); 定义了一个Animal类型的引用，指向新建的Dog类型的对象。<strong>父类引用a可以直接调用Animal中未被dog重写(override)的方法，调用被重写的方法时会调用子类中的这个方法，这就是动态连接。</strong>这样的函数调用在编译期间是无法确定的（调用的子类的虚函数的地址无法给出）。当需要在子类中调用父类的被重写方法时，要使用 super 关键字。</li>
</ol>
<p>注意：重载(overloading) 是在一个类里面，方法名字相同，而参数不同。返回类型可以相同也可以不同。编译器根据函数不同的参数表，对同名函数的名称做修饰，然后这些同名函数就成了不同的函数（对于编译器来说）。对于这两个函数的调用，在编译器间就已经确定了，是静态的（记住：是静态）。也就是说，它们的地址在编译期就绑定了（早绑定），因此，重载只是一种语言特性，与多态无关，与面向对象也无关。<strong>无法以返回值类型作为重载函数的区分标准。</strong></p>
<h2 id="Java集合框架"><a href="#Java集合框架" class="headerlink" title="Java集合框架"></a>Java集合框架</h2><blockquote>
<p>  <a href="https://www.runoob.com/java/java-collections.html">https://www.runoob.com/java/java-collections.html</a></p>
</blockquote>
<p>集合框架是一个用来代表和操纵集合的统一架构。所有的集合框架都包含如下内容：<br>    接口：是代表集合的抽象数据类型。例如 Collection、List、Set、Map 等。之所以定义多个接口，是为了以不同的方式操作集合对象<br>    实现（类）：是集合接口的具体实现。从本质上讲，它们是可重复使用的数据结构，例如：ArrayList、LinkedList、HashSet、HashMap。<br>    算法：是实现集合接口的对象里的方法执行的一些有用的计算，例如：搜索和排序。这些算法被称为多态，那是因为相同的方法可以在相似的接口上有着不同的实现。</p>
<h3 id="接口和抽象类区别"><a href="#接口和抽象类区别" class="headerlink" title="接口和抽象类区别"></a>接口和抽象类区别</h3><blockquote>
<p>  <a href="https://www.cnblogs.com/dolphin0520/p/3811437.html">https://www.cnblogs.com/dolphin0520/p/3811437.html</a><br>    <a href="https://www.jianshu.com/p/038f0b356e9a">https://www.jianshu.com/p/038f0b356e9a</a></p>
</blockquote>
<p>抽象方法是一种特殊的方法：它只有声明，而没有具体的实现：abstract void fun();因为抽象类中含有无具体实现的方法，所以不能用抽象类创建对象。包含抽象方法的类称为<em>抽象类</em>，抽象方法必须为public或者protected（因为如果为private，则不能被子类继承，子类便无法实现该方法），缺省情况下默认为public；抽象类不能用来创建对象；如果一个类继承于一个抽象类，则子类必须实现父类的抽象方法。如果子类没有实现父类的抽象方法，则必须将子类也定义为为abstract类。<br><em>接口</em>泛指供别人调用的方法或者函数。接口中的变量会被隐式地指定为public static final变量，方法会被隐式地指定为public abstract方法，并且接口中所有的方法不能有具体的实现，也就是说，<strong>接口中的方法必须都是抽象方法。</strong>可以看出，接口是一种极度抽象的类型。<br><strong>允许一个类实现(implements)多个特定的接口。</strong><br><strong>如果一个非抽象类实现(implements)某个接口，就必须实现该接口中的所有方法。</strong><br>对于实现(implements)某个接口的抽象类，可以不实现该接口中的抽象方法。</p>
<ol>
<li>抽象类可以有默认的方法实现完全是抽象的。接口根本不存在方法的实现。</li>
<li>抽象类使用extends关键字来继承抽象类。如果子类不是抽象类的话，它需要提供抽象类中所有声明的方法的实现。子类使用关键字implements来实现接口。它需要提供接口中所有声明的方法的实现。</li>
<li>抽象类可以有构造器，而接口不能有构造器。</li>
<li>抽象方法可以有public、protected和default这些修饰符 ;接口方法默认修饰符是public，不可以使用其它修饰符。</li>
<li>抽象类在java语言中所表示的是一种继承关系，一个子类只能存在一个父类，但是可以存在多个接口。</li>
<li>抽象方法比接口速度要快，接口是稍微有点慢的，因为它需要时间去寻找在类中实现的方法。</li>
<li>如果你往抽象类中添加新的方法，你可以给它提供默认的实现。因此你不需要改变你现在的代码。如果你往接口中添加方法，那么你必须改变实现该接口的类。</li>
</ol>
<h3 id="【高频】-类加载过程"><a href="#【高频】-类加载过程" class="headerlink" title="【高频】 类加载过程"></a>【高频】 类加载过程</h3><p> 加载，连接，初始化 </p>
<h3 id="有哪些类加载器，能否自定义-Java-Object-String-的类加载器-？"><a href="#有哪些类加载器，能否自定义-Java-Object-String-的类加载器-？" class="headerlink" title="有哪些类加载器，能否自定义 Java.Object.String 的类加载器 ？"></a>有哪些类加载器，能否自定义 Java.Object.String 的类加载器 ？</h3><h3 id="双亲委派机制介绍-amp-作用"><a href="#双亲委派机制介绍-amp-作用" class="headerlink" title="双亲委派机制介绍 &amp; 作用"></a>双亲委派机制介绍 &amp; 作用</h3><p>类加载器加载类时先委派给父类加载，只有父类无法加载时，自己才尝试加载。<br>保证java类库中的类不受用户类影响，防止用户自定义一个类库中的同名类，引起问题。</p>
<h2 id="源码-amp-原理-amp-对比：arraylist、linkedlist、vector"><a href="#源码-amp-原理-amp-对比：arraylist、linkedlist、vector" class="headerlink" title="源码&amp;原理&amp;对比：arraylist、linkedlist、vector"></a>源码&amp;原理&amp;对比：arraylist、linkedlist、vector</h2><h2 id="JVM相关问题"><a href="#JVM相关问题" class="headerlink" title="JVM相关问题"></a>JVM相关问题</h2><h3 id="JOOM-如何定位"><a href="#JOOM-如何定位" class="headerlink" title="JOOM 如何定位"></a>JOOM 如何定位</h3><h3 id="说几个虚拟机指令以及虚拟机栈可能会发生什么错误"><a href="#说几个虚拟机指令以及虚拟机栈可能会发生什么错误" class="headerlink" title="说几个虚拟机指令以及虚拟机栈可能会发生什么错误"></a>说几个虚拟机指令以及虚拟机栈可能会发生什么错误</h3><h3 id="四种引用类型"><a href="#四种引用类型" class="headerlink" title="四种引用类型"></a>四种引用类型</h3><h3 id="jvm-的局部变量表是做什么的？"><a href="#jvm-的局部变量表是做什么的？" class="headerlink" title="jvm 的局部变量表是做什么的？"></a>jvm 的局部变量表是做什么的？</h3><p>存储局部变量、函数调用时传递参数，很多字节码指令都是对局部变量表和操作数栈进行操作的。</p>
<h2 id="static变量什么作用，放在哪里"><a href="#static变量什么作用，放在哪里" class="headerlink" title="static变量什么作用，放在哪里"></a>static变量什么作用，放在哪里</h2><h2 id="线程池的执行过程、核心参数以及常用的几个线程池"><a href="#线程池的执行过程、核心参数以及常用的几个线程池" class="headerlink" title="线程池的执行过程、核心参数以及常用的几个线程池"></a>线程池的执行过程、核心参数以及常用的几个线程池</h2><h2 id="Java内存区域-："><a href="#Java内存区域-：" class="headerlink" title="Java内存区域   ："></a>Java内存区域   ：</h2><p>程序计数器 ， 虚拟机栈 ，本地方法栈 ， 堆 ，方法区 ，元空间 </p>
<h3 id="讲一下-volatile-。"><a href="#讲一下-volatile-。" class="headerlink" title="讲一下 volatile 。"></a>讲一下 volatile 。</h3><p>内存可见性、指令重排、32位jvm对64数据的原子操作什么的</p>
<h3 id="volatile-可以保证并发计数正确性？"><a href="#volatile-可以保证并发计数正确性？" class="headerlink" title="volatile 可以保证并发计数正确性？"></a>volatile 可以保证并发计数正确性？</h3><p>不能</p>
<h3 id="如果需要保证并发计数正确怎么办，只能加锁吗？"><a href="#如果需要保证并发计数正确怎么办，只能加锁吗？" class="headerlink" title="如果需要保证并发计数正确怎么办，只能加锁吗？"></a>如果需要保证并发计数正确怎么办，只能加锁吗？</h3><p>计数是个轻量级的操作，java 有原子变量，于是说 AtomicInteger。</p>
<h3 id="为什么原子变量能保证高效正确计数？"><a href="#为什么原子变量能保证高效正确计数？" class="headerlink" title="为什么原子变量能保证高效正确计数？"></a>为什么原子变量能保证高效正确计数？</h3><h2 id="垃圾回收：JVM-GC的一整套-算法、分区、判断方法、GC器"><a href="#垃圾回收：JVM-GC的一整套-算法、分区、判断方法、GC器" class="headerlink" title="垃圾回收：JVM GC的一整套(算法、分区、判断方法、GC器)"></a>垃圾回收：JVM GC的一整套(算法、分区、判断方法、GC器)</h2><p>为新生成的对象分配内存,识别那些垃圾对象，并且从垃圾对象那回收内存</p>
<h3 id="【高频】-判断对象死亡的方法-：-引用计数法-，可达分析算法"><a href="#【高频】-判断对象死亡的方法-：-引用计数法-，可达分析算法" class="headerlink" title="【高频】 判断对象死亡的方法 ： { 引用计数法 ，可达分析算法 }"></a>【高频】 判断对象死亡的方法 ： { 引用计数法 ，可达分析算法 }</h3><h3 id="垃圾收集算法-："><a href="#垃圾收集算法-：" class="headerlink" title="垃圾收集算法  ："></a>垃圾收集算法  ：</h3><p> { 标记清除算法 、标记整理算法 、 复制算法、 分代收集算法 }<br>深入一些： 各个算法的优点和适用场景</p>
<h3 id="垃圾收集器-："><a href="#垃圾收集器-：" class="headerlink" title="垃圾收集器 ："></a>垃圾收集器 ：</h3><p> { serial  、 parallel 、 CMS 、 G1  }</p>
<h3 id="垃圾回收怎么判断哪些对象应该回收？"><a href="#垃圾回收怎么判断哪些对象应该回收？" class="headerlink" title="垃圾回收怎么判断哪些对象应该回收？"></a>垃圾回收怎么判断哪些对象应该回收？</h3><p>可达性分析，从 GCRoot 开始遍历，不能遍历到的对象就可以认为已经死了，可以回收。</p>
<h3 id="什么可以作为GCRoot？"><a href="#什么可以作为GCRoot？" class="headerlink" title="什么可以作为GCRoot？"></a>什么可以作为GCRoot？</h3><p>方法区的数据引用、当前代码处的局部变量，基本就是用户能通过代码引用到的。</p>
<h3 id="Minor-GC-触发条件-："><a href="#Minor-GC-触发条件-：" class="headerlink" title="Minor GC 触发条件 ："></a>Minor GC 触发条件 ：</h3><p>eden区剩余内存是否足够 两种情况分开分析</p>
<h3 id="FULL-GC-触发条件-："><a href="#FULL-GC-触发条件-：" class="headerlink" title="FULL GC  触发条件 ："></a>FULL GC  触发条件 ：</h3><p>Minor GC 平均晋升空间大小 &gt; 老年代连续剩余空间，则触发FULL GC</p>
<h3 id="讲讲了解的垃圾回收算法和回收器，什么时候执行STOP-THE-WORLD？"><a href="#讲讲了解的垃圾回收算法和回收器，什么时候执行STOP-THE-WORLD？" class="headerlink" title="讲讲了解的垃圾回收算法和回收器，什么时候执行STOP THE WORLD？"></a>讲讲了解的垃圾回收算法和回收器，什么时候执行STOP THE WORLD？</h3><h3 id="CMS-、G1-重点-，-介绍工作流程和优缺点"><a href="#CMS-、G1-重点-，-介绍工作流程和优缺点" class="headerlink" title="CMS 、G1 重点 ， 介绍工作流程和优缺点"></a>CMS 、G1 重点 ， 介绍工作流程和优缺点</h3><h3 id="内存泄漏"><a href="#内存泄漏" class="headerlink" title="内存泄漏"></a>内存泄漏</h3><p>例子： { 单例 ， 容器 等等}<br>原因 ： 长生命周期持有短生命周期引用</p>
<h3 id="引用类型"><a href="#引用类型" class="headerlink" title="引用类型"></a>引用类型</h3><p>强引用、 软引用、 弱引用 、 虚引用  </p>
<h3 id="如何高效进行数组拷贝（其实是System-arraycopy-的原理"><a href="#如何高效进行数组拷贝（其实是System-arraycopy-的原理" class="headerlink" title="如何高效进行数组拷贝（其实是System.arraycopy()的原理"></a>如何高效进行数组拷贝（其实是System.arraycopy()的原理</h3><h3 id="equals-和-区别"><a href="#equals-和-区别" class="headerlink" title="equals 和 == 区别"></a>equals 和 == 区别</h3><h3 id="为啥重写equals要重写hashCode"><a href="#为啥重写equals要重写hashCode" class="headerlink" title="为啥重写equals要重写hashCode()"></a>为啥重写equals要重写hashCode()</h3><p>hash值相等，而两个对象不一定equals</p>
<h3 id="String-StringBuffer-StringBuilder-区别-和各自使用场景"><a href="#String-StringBuffer-StringBuilder-区别-和各自使用场景" class="headerlink" title="String StringBuffer StringBuilder  区别 和各自使用场景"></a>String StringBuffer StringBuilder  区别 和各自使用场景</h3><p>深入一些 ： String 是如何实现它不可变的？ 为什么要设置String为不可变对象  ?  </p>
<h3 id="深拷贝和浅拷贝区别"><a href="#深拷贝和浅拷贝区别" class="headerlink" title="深拷贝和浅拷贝区别"></a>深拷贝和浅拷贝区别</h3><h3 id="Object的方法"><a href="#Object的方法" class="headerlink" title="Object的方法"></a>Object的方法</h3><p>{ finalize 、 clone、 getClass 、 equals 、 hashCode }</p>
<h3 id="【高频】-设计模式"><a href="#【高频】-设计模式" class="headerlink" title="【高频】 设计模式"></a>【高频】 设计模式</h3><p> {    单例模式 、 工厂模式 、 装饰者模式 、 代理模式 、 策略模式 等等} （此处我的掌握也不是很好）</p>
<h3 id="深入一些"><a href="#深入一些" class="headerlink" title="深入一些"></a>深入一些</h3><p>单例模式为什么采用双检测机制 ？ 单例为什么用Volatile修饰？ 装饰模式和代理模式区别？</p>
<h3 id="B树怎么保证每次操作完都平衡的？"><a href="#B树怎么保证每次操作完都平衡的？" class="headerlink" title="B树怎么保证每次操作完都平衡的？"></a>B树怎么保证每次操作完都平衡的？</h3><p>跟红黑树还是差不太多</p>
]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>programming language</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库</title>
    <url>/2020/07/29/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93/</url>
    <content><![CDATA[<h3 id="expoll了解吗"><a href="#expoll了解吗" class="headerlink" title="expoll了解吗"></a>expoll了解吗</h3><h3 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h3><h3 id="数据库事务隔离级别"><a href="#数据库事务隔离级别" class="headerlink" title="数据库事务隔离级别"></a>数据库事务隔离级别</h3><h3 id="多人同时使用数据库的注意事项"><a href="#多人同时使用数据库的注意事项" class="headerlink" title="多人同时使用数据库的注意事项"></a>多人同时使用数据库的注意事项</h3><h3 id="MySQL与其他主流数据库相比有什么特点？"><a href="#MySQL与其他主流数据库相比有什么特点？" class="headerlink" title="MySQL与其他主流数据库相比有什么特点？"></a>MySQL与其他主流数据库相比有什么特点？</h3><h3 id="事务的四大特性"><a href="#事务的四大特性" class="headerlink" title="事务的四大特性"></a>事务的四大特性</h3><h3 id="四种隔离级别"><a href="#四种隔离级别" class="headerlink" title="四种隔离级别"></a>四种隔离级别</h3><h3 id="什么是幻读"><a href="#什么是幻读" class="headerlink" title="什么是幻读"></a>什么是幻读</h3><h3 id="InnoDB-怎么防止幻读"><a href="#InnoDB-怎么防止幻读" class="headerlink" title="InnoDB 怎么防止幻读"></a>InnoDB 怎么防止幻读</h3><h3 id="B-树原理，为什么使用B-而不是二叉平衡树"><a href="#B-树原理，为什么使用B-而不是二叉平衡树" class="headerlink" title="B+树原理，为什么使用B+而不是二叉平衡树"></a>B+树原理，为什么使用B+而不是二叉平衡树</h3><h3 id="为什么要分用户态和内核态"><a href="#为什么要分用户态和内核态" class="headerlink" title="为什么要分用户态和内核态"></a>为什么要分用户态和内核态</h3><h3 id="用户态和内核态的区别"><a href="#用户态和内核态的区别" class="headerlink" title="用户态和内核态的区别"></a>用户态和内核态的区别</h3><h3 id="Git-切换分支，提交，具体如何合并分支"><a href="#Git-切换分支，提交，具体如何合并分支" class="headerlink" title="Git 切换分支，提交，具体如何合并分支"></a>Git 切换分支，提交，具体如何合并分支</h3><h3 id="数据库部分知识，手写一个-SQL-（子查询-感觉主要看-group-by-和-having）"><a href="#数据库部分知识，手写一个-SQL-（子查询-感觉主要看-group-by-和-having）" class="headerlink" title="数据库部分知识，手写一个 SQL （子查询 感觉主要看 group by 和 having）"></a>数据库部分知识，手写一个 SQL （子查询 感觉主要看 group by 和 having）</h3><h3 id="MySQL的索引-B-和B树的区别，为啥用B-；"><a href="#MySQL的索引-B-和B树的区别，为啥用B-；" class="headerlink" title="MySQL的索引(B+和B树的区别，为啥用B+)；"></a>MySQL的索引(B+和B树的区别，为啥用B+)；</h3><h3 id="让你设计数据库会注意什么；"><a href="#让你设计数据库会注意什么；" class="headerlink" title="让你设计数据库会注意什么；"></a>让你设计数据库会注意什么；</h3><h3 id="【高频】MyISAM-和-InnoDB的区别"><a href="#【高频】MyISAM-和-InnoDB的区别" class="headerlink" title="【高频】MyISAM 和 InnoDB的区别"></a>【高频】MyISAM 和 InnoDB的区别</h3><p>{是否支持行锁 、 是否支持十五 、 是否支持 MVCC 、 底层索引结构不同 }</p>
<h3 id="update时什么锁-next-key-lock，但唯一性索引时降级为行锁，"><a href="#update时什么锁-next-key-lock，但唯一性索引时降级为行锁，" class="headerlink" title="update时什么锁 (next-key lock，但唯一性索引时降级为行锁，"></a>update时什么锁 (next-key lock，但唯一性索引时降级为行锁，</h3><h3 id="Mysql-比较熟悉是吧？说一下底层数据存储原理？"><a href="#Mysql-比较熟悉是吧？说一下底层数据存储原理？" class="headerlink" title="Mysql 比较熟悉是吧？说一下底层数据存储原理？"></a>Mysql 比较熟悉是吧？说一下底层数据存储原理？</h3><p>如果直接线性存储的话，每次查找数据都要整个遍历一遍，那么复杂度就是log(n)，于是可以用二叉树来存储，把复杂度降低到约log(n)，但是二叉树有个特点就是，它有可能因为插入顺序的问题，变得不平衡，最坏情况就是都在节点一边，又变成了log(n)，所有就通过改进插入和删除等操作，保证每次操作完后树都是平衡的，就有了B树……</p>
<h3 id="说一下建表时，建索引有哪些要注意的。"><a href="#说一下建表时，建索引有哪些要注意的。" class="headerlink" title="说一下建表时，建索引有哪些要注意的。"></a>说一下建表时，建索引有哪些要注意的。</h3><p>选区分度比较大的，选数据类型比较小的比如整数而不要选长字符串，选where子句中出现的，覆盖索引 balabala……</p>
<h3 id="事务特性ACID"><a href="#事务特性ACID" class="headerlink" title="事务特性ACID"></a>事务特性ACID</h3><p>深入一些 ： 为什么要有一致性 ？ AID不是已经保证了一致性了吗 ？</p>
<h3 id="并发事务带来的问题"><a href="#并发事务带来的问题" class="headerlink" title="并发事务带来的问题"></a>并发事务带来的问题</h3><p>{    脏读 、 修改丢失 、 不可重复读 、 幻影读    }</p>
<h3 id="【高频】事务的隔离级别"><a href="#【高频】事务的隔离级别" class="headerlink" title="【高频】事务的隔离级别"></a>【高频】事务的隔离级别</h3><h3 id="【高频】-MVCC机制"><a href="#【高频】-MVCC机制" class="headerlink" title="【高频】 MVCC机制"></a>【高频】 MVCC机制</h3><h3 id="【高频】索引"><a href="#【高频】索引" class="headerlink" title="【高频】索引"></a>【高频】索引</h3><h3 id="为什么索引使用B-树结构，而不是B树"><a href="#为什么索引使用B-树结构，而不是B树" class="headerlink" title="为什么索引使用B+树结构，而不是B树"></a>为什么索引使用B+树结构，而不是B树</h3><h3 id="为什么索引使用B-树结构，而不是红黑树-："><a href="#为什么索引使用B-树结构，而不是红黑树-：" class="headerlink" title="为什么索引使用B+树结构，而不是红黑树 ："></a>为什么索引使用B+树结构，而不是红黑树 ：</h3><p>{  磁盘预读取 、红黑树高度 }</p>
<h3 id="聚簇索引和非聚簇索引区别？-主键索引和二级索引了解吗？"><a href="#聚簇索引和非聚簇索引区别？-主键索引和二级索引了解吗？" class="headerlink" title="聚簇索引和非聚簇索引区别？ 主键索引和二级索引了解吗？"></a>聚簇索引和非聚簇索引区别？ 主键索引和二级索引了解吗？</h3><h3 id="为什么不对每个列创建索引呢？"><a href="#为什么不对每个列创建索引呢？" class="headerlink" title="为什么不对每个列创建索引呢？"></a>为什么不对每个列创建索引呢？</h3><h3 id="【高频】SQL语句优化-，SQL题目（字节要求撸代码）"><a href="#【高频】SQL语句优化-，SQL题目（字节要求撸代码）" class="headerlink" title="【高频】SQL语句优化 ，SQL题目（字节要求撸代码）"></a>【高频】SQL语句优化 ，SQL题目（字节要求撸代码）</h3><h3 id="explain中-rows-type-key-extra字段的含义？"><a href="#explain中-rows-type-key-extra字段的含义？" class="headerlink" title="explain中 rows type key extra字段的含义？"></a>explain中 rows type key extra字段的含义？</h3><h3 id="count-1-count-count-列值-的区别"><a href="#count-1-count-count-列值-的区别" class="headerlink" title="count(1) count(*) count(列值)的区别"></a>count(1) count(*) count(列值)的区别</h3><h3 id="数据库的关系有哪些？"><a href="#数据库的关系有哪些？" class="headerlink" title="数据库的关系有哪些？"></a>数据库的关系有哪些？</h3><h3 id="一对多怎么实现？多对多怎么实现？"><a href="#一对多怎么实现？多对多怎么实现？" class="headerlink" title="一对多怎么实现？多对多怎么实现？"></a>一对多怎么实现？多对多怎么实现？</h3><h3 id="MySQL数据库有哪些类型？这些类型有什么区别？"><a href="#MySQL数据库有哪些类型？这些类型有什么区别？" class="headerlink" title="MySQL数据库有哪些类型？这些类型有什么区别？"></a>MySQL数据库有哪些类型？这些类型有什么区别？</h3>]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络</title>
    <url>/2020/07/27/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h1><p>锁(lock)或互斥(mutex)是一种同步机制，用于在有许多执行线程的环境中强制对资源的访问限制。锁旨在强制实施互斥排他、并发控制策略。</p>
<h2 id="常见锁机制"><a href="#常见锁机制" class="headerlink" title="常见锁机制"></a>常见锁机制</h2><p>读写锁、可重入锁、乐观锁、悲观锁、公平锁、非公平锁<br><a id="more"></a></p>
<h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><p>Java中的死锁是一种编程情况，其中两个或多个线程被永久阻塞，Java死锁情况出现至少两个线程和两个或更多资源。<br>Java发生死锁的根本原因是：在申请锁时发生了交叉闭环申请。</p>
<ul>
<li>是多个线程涉及到多个锁，这些锁存在着交叉，所以可能会导致了一个锁依赖的闭环。<br>例如：线程在获得了锁A并且没有释放的情况下去申请锁B，这时，另一个线程已经获得了锁B，在释放锁B之前又要先获得锁A，因此闭环发生，陷入死锁循环。</li>
<li>默认的锁申请操作是阻塞的。<br>所以要避免死锁，就要在一遇到多个对象锁交叉的情况，就要仔细审查这几个对象的类中的所有方法，是否存在着导致锁依赖的环路的可能性。总之是尽量避免在一个同步方法中调用其它对象的延时方法和同步方法。</li>
</ul>
<h2 id="独享锁-共享锁"><a href="#独享锁-共享锁" class="headerlink" title="独享锁/共享锁"></a>独享锁/共享锁</h2><p>独享锁是指该锁一次只能被一个线程所持有。 (ReentrantLock、 Synchronized)<br>共享锁是指该锁可被多个线程所持有。 (ReadWriteLock)<br>互斥锁/读写锁<br>独享锁/共享锁这是广义上的说法，互斥锁/读写锁就分别对应具体的实现。在Java中如ReentrantLock就是互斥锁(独享锁)， ReadWriteLock就是读写锁(共享锁)。 独享锁与共享锁也是通过AQS来实现的<br>锁升级：读锁到写锁 (不支持)<br>锁降级：写锁到读锁 (支持)</p>
<h2 id="公平锁-amp-非公平锁"><a href="#公平锁-amp-非公平锁" class="headerlink" title="公平锁 &amp; 非公平锁"></a>公平锁 &amp; 非公平锁</h2><p>公平锁是指多个线程按照申请锁的顺序来获取锁。<br>非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能会造成饥饿现象。<br>对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。<br>对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的控制线程对锁的获取， 所以并没有任何办法使其变成公平锁。</p>
<h2 id="乐观锁-amp-悲观锁"><a href="#乐观锁-amp-悲观锁" class="headerlink" title="乐观锁 &amp; 悲观锁"></a>乐观锁 &amp; 悲观锁</h2><p>乐观锁：就像它的名字一样，对于并发间操作产生的线程安全问题持乐观状态，乐观锁认为竞争不总是会发生，因此它不需要持有锁，将比较-替换这两个动作作为一个原子操作尝试去修改内存中的变量，如果失败则表示发生冲突，那么就应该有相应的重试逻辑。<br>悲观锁：还是像它的名字一样，对于并发间操作产生的线程安全问题持悲观状态，悲观锁认为竞争总是会发生，因此每次对某资源进行操作时，都会持有一个独占的锁，就像synchronized，不管三七二十一，直接上了锁就操作资源了。</p>
<h2 id="实现读写锁ReentrantReadWriteLock"><a href="#实现读写锁ReentrantReadWriteLock" class="headerlink" title="实现读写锁ReentrantReadWriteLock"></a>实现读写锁ReentrantReadWriteLock</h2><p>低16位代表写锁，高16位代表读锁</p>
<h2 id="可重入锁ReetrantLock原理"><a href="#可重入锁ReetrantLock原理" class="headerlink" title="可重入锁ReetrantLock原理"></a>可重入锁ReetrantLock原理</h2><p>可重入锁又名递归锁，是指同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。<br>ReentrantLock和Synchronized都是可重入锁。<strong>可重入锁的一个好处是可一定程度避免死锁。</strong><br>ReentrantLock主要利用CAS+AQS队列来实现。它支持公平锁和非公平锁，两者的实现类似。<br><strong>CAS：Compare and Swap，比较并交换。</strong>CAS有3个操作数：内存值V、预期值A、要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。该操作是一个原子操作，被广泛的应用在Java的底层实现中。在Java中，CAS主要是由sun.misc.Unsafe这个类通过JNI调用CPU底层指令实现<br><strong>AbstractQueuedSynchronizer简称AQS，是一个用于构建锁和同步容器的框架。</strong>事实上concurrent包内许多类都是基于AQS构建，例如ReentrantLock，Semaphore，CountDownLatch，ReentrantReadWriteLock，FutureTask等。AQS解决了在实现同步容器时设计的大量细节问题。</p>
<h2 id="synchronized和ReentrantLock的区别"><a href="#synchronized和ReentrantLock的区别" class="headerlink" title="synchronized和ReentrantLock的区别"></a>synchronized和ReentrantLock的区别</h2><p>synchronized是和if、else、for、while一样的关键字，ReentrantLock是类，这是二者的本质区别。既然ReentrantLock是类，那么它就提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量，ReentrantLock比synchronized的扩展性体现在几点上：</p>
<ul>
<li>ReentrantLock可以对获取锁的等待时间进行设置，这样就避免了死锁</li>
<li>ReentrantLock可以获取各种锁的信息</li>
<li>ReentrantLock可以灵活地实现多路通知</li>
</ul>
<h2 id="如果有多个变量要更新，要保证一致性，怎样加锁来保证正确性，效率又比较高？"><a href="#如果有多个变量要更新，要保证一致性，怎样加锁来保证正确性，效率又比较高？" class="headerlink" title="如果有多个变量要更新，要保证一致性，怎样加锁来保证正确性，效率又比较高？"></a>如果有多个变量要更新，要保证一致性，怎样加锁来保证正确性，效率又比较高？</h2><p>只在写时加锁，读不加锁</p>
<h2 id="那怎么解决一个写操作修改了部分变量，读操作，读取了这个中间状态的问题？"><a href="#那怎么解决一个写操作修改了部分变量，读操作，读取了这个中间状态的问题？" class="headerlink" title="那怎么解决一个写操作修改了部分变量，读操作，读取了这个中间状态的问题？"></a>那怎么解决一个写操作修改了部分变量，读操作，读取了这个中间状态的问题？</h2><p>写操作时，先锁住锁1，在线程本地也就是函数局部计算完所有结果后，锁住锁2，一次更新完后再释放2个锁，读操作只锁住锁2</p>
<h2 id="synchronized-amp-lock"><a href="#synchronized-amp-lock" class="headerlink" title="synchronized &amp; lock"></a>synchronized &amp; lock</h2><p>synchronized原始采用的是CPU悲观锁机制，即线程获得的是独占锁。独占锁意味着其他线程只能依靠阻塞来等待线程释放锁。而在CPU转换线程阻塞时会引起线程上下文切换，当有很多线程竞争锁的时候，会引起CPU频繁的上下文切换导致效率很低。<br>Lock用的是乐观锁方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。乐观锁实现的机制就是CAS操作（Compare and Swap）。</p>
<blockquote>
<p>  <a href="https://blog.csdn.net/tanmomo/article/details/99671622">https://blog.csdn.net/tanmomo/article/details/99671622</a><br>    <a href="https://blog.csdn.net/fuyuwei2015/article/details/83719444">https://blog.csdn.net/fuyuwei2015/article/details/83719444</a><br>    <a href="https://zhuanlan.zhihu.com/p/108224026?from_voters_page=true">https://zhuanlan.zhihu.com/p/108224026?from_voters_page=true</a><br>    <a href="https://blog.csdn.net/natian306/article/details/18504111">https://blog.csdn.net/natian306/article/details/18504111</a></p>
</blockquote>
<h1 id="TCP-amp-UDP"><a href="#TCP-amp-UDP" class="headerlink" title="TCP &amp; UDP"></a>TCP &amp; UDP</h1><h2 id="TCP和UDP有什么区别？"><a href="#TCP和UDP有什么区别？" class="headerlink" title="TCP和UDP有什么区别？"></a>TCP和UDP有什么区别？</h2><ol>
<li>TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接</li>
<li>TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付</li>
<li>TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的。UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）</li>
<li>每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信</li>
<li>TCP首部开销20字节;UDP的首部开销小，只有8个字节</li>
<li>TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道</li>
</ol>
<p>具体编程时的区别</p>
<ol>
<li>socket()的参数不同   </li>
<li>UDP Server不需要调用listen和accept </li>
<li>UDP收发数据用sendto/recvfrom函数 </li>
<li>TCP：地址信息在connect/accept时确定  </li>
<li>UDP：在sendto/recvfrom函数中每次均 需指定地址信息 </li>
<li>UDP：shutdown函数无效</li>
</ol>
<h2 id="TCP可靠性机制"><a href="#TCP可靠性机制" class="headerlink" title="TCP可靠性机制"></a><strong>TCP可靠性机制</strong></h2><h3 id="检验和"><a href="#检验和" class="headerlink" title="检验和"></a>检验和</h3><p>TCP检验和的计算与UDP一样，在计算时要加上12byte的伪首部，检验范围包括TCP首部及数据部分，但是UDP的检验和字段为可选的，而TCP中是必须有的。计算方法为：在发送方将整个报文段分为多个16位的段，然后将所有段进行反码相加，将结果存放在检验和字段中，接收方用相同的方法进行计算，如最终结果为检验字段所有位是全1则正确（UDP中为0是正确），否则存在错误。 </p>
<h3 id="序列号"><a href="#序列号" class="headerlink" title="序列号"></a>序列号</h3><p>TCP将每个字节的数据都进行了编号，这就是序列号。<br>序列号的作用：<br>a. 保证可靠性（当接收到的数据总少了某个序号的数据时，能马上知道）<br>b. 保证数据的按序到达<br>c. 提高效率，可实现多次发送，一次确认<br>d. 去除重复数据<br>数据传输过程中的确认应答处理、重发控制以及重复控制等功能都可以通过序列号来实现 </p>
<h3 id="确认应答机制（ACK）"><a href="#确认应答机制（ACK）" class="headerlink" title="确认应答机制（ACK）"></a>确认应答机制（ACK）</h3><p>TCP通过确认应答机制实现可靠的数据传输。在TCP的首部中有一个标志位——<strong>ACK，此标志位表示确认号是否有效</strong>。接收方对于按序到达的数据会进行确认，当标志位ACK=1时确认首部的确认字段有效。进行确认时，确认字段值表示这个值之前的数据都已经按序到达了。而发送方如果收到了已发送的数据的确认报文，则继续传输下一部分数据；而如果等待了一定时间还没有收到确认报文就会启动重传机制。 </p>
<h3 id="超时重传机制"><a href="#超时重传机制" class="headerlink" title="超时重传机制"></a>超时重传机制</h3><p>当报文发出后在一定的时间内未收到接收方的确认，发送方就会进行重传（通常是在发出报文段后设定一个闹钟，到点了还没有收到应答则进行重传）<br>当接收方接收到重复的数据时就将其丢掉，重新发送ACK。而要识别出重复的数据，就要用到前面提到的序列号了，利用序列号很容易就可以做到去重的效果。 </p>
<h3 id="连接管理机制"><a href="#连接管理机制" class="headerlink" title="连接管理机制"></a>连接管理机制</h3><p>连接管理机制即TCP建立连接时的三次握手和断开连接时的四次挥手。 </p>
<h4 id="三次握手："><a href="#三次握手：" class="headerlink" title="三次握手："></a>三次握手：</h4><ul>
<li>客户端发出：syn包（seq=j），并进入SYN_SENT状态，等待服务器确认。</li>
<li>服务端发出：ack=j+1，同时自己也发送一个SYN包（seq=k），此时服务器进入SYN_RECV状态。</li>
<li>客户端发出：确认包ACK(ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。</li>
</ul>
<h4 id="四次挥手："><a href="#四次挥手：" class="headerlink" title="四次挥手："></a>四次挥手：</h4><ul>
<li>主机1发出：主机1（可以使客户端，也可以是服务器端），设置序列号Seq和确认号ACK，向主机2发送一个FIN报文段；此时，主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了； </li>
<li>主机2发出：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段，ACK = Seq + 1；主机1进入FIN_WAIT_2状态；主机2告诉主机1，我“同意”你的关闭请求；</li>
<li>主机2发出：FIN报文段，请求关闭连接，同时主机2进入LAST_ACK状态；</li>
<li>主机1发出：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。</li>
</ul>
<p>FIN_WAIT_1: 其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。（主动方）<br>FIN_WAIT_2：实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你(ACK信息)，稍后再关闭连接。（主动方）<br><strong>CLOSE_WAIT：表示在等待关闭。当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。</strong>接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。（被动方）<br>LAST_ACK: 被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。（被动方）<br><strong>TIME_WAIT: 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。</strong>如果FIN WAIT1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。（主动方）<br>CLOSED: 表示连接中断。</p>
<h3 id="停止等待协议"><a href="#停止等待协议" class="headerlink" title="停止等待协议"></a>停止等待协议</h3><p>停止等待协议是一种ARQ协议。(Automatic Repeat reQuest自动重传请求)<br>A向B每发送一个分组，都要停止发送，等待B的确认应答；<strong>A只有收到了B的确认应答后才能发送下一个分组</strong>。当分组丢失或出现差错 的情况下，A都会超时重传分组。TCP会给每个字节都打上序号，用于判断该分组是否已经接收。 </p>
<ul>
<li>必须设置超时计时器。每发送一个分组就要启动计时器，超时就要重发分组。计时器的超时时间要大于应答的平均返回时间，否则会出现很多不必要的重传，降低传输效率。但超时时间也不能太长。</li>
</ul>
<h3 id="流量控制-滑动窗口（发送窗口和接收窗口）"><a href="#流量控制-滑动窗口（发送窗口和接收窗口）" class="headerlink" title="流量控制  滑动窗口（发送窗口和接收窗口）"></a>流量控制  滑动窗口（发送窗口和接收窗口）</h3><p>在ARQ协议发送者每次只能发送一个分组，在应答到来前必须等待。而连续ARQ协议的发送者拥有一个发送窗口，发送者可以在没有得到应答的情况下连续发送窗口中的分组。这样降低了等待时间，提高了传输效率。<br>接收端处理数据的速度是有限的，如果发送方发送数据的速度过快，导致接收端的缓冲区满，而发送方继续发送，就会造成丢包，继而引起丢包重传等一系列连锁反应。<br><strong>TCP支持根据接收端的处理能力，来决定发送端的发送速度，这个机制叫做流量控制。</strong><br>在TCP报文段首部中有一个16位窗口长度，当接收端接收到发送方的数据后，在应答报文ACK中就将自身缓冲区的剩余大小，放入16窗口大小中。这个大小随数据传输情况而变，窗口越大，网络吞吐量越高，而一旦接收方发现自身的缓冲区快满了，就将窗口设置为更小的值通知发送方。如果缓冲区满，就将窗口置为0，发送方收到后就不再发送数据，但是需要定期发送一个窗口探测数据段，使接收端把窗口大小告诉发送端。 </p>
<ul>
<li>已经发送并且对端确认（Sent/ACKed）———————-发送窗外 缓冲区外<br>已经发送但未收到确认数据（Sent/UnACKed） ————-发送窗内 缓冲区内<br>允许发送但尚未防的数据（Unsent/Inside）—————发送窗内 缓冲区内<br>未发送暂不允许（Unsent/Outside）————————-发送窗外 缓冲区内<br>第二、三为<strong>发送窗口，存放在TCP header中有一个Window Size字段，用来告知发送端自己所能接收的数据量，从而达到一部分流控的目的。</strong></li>
<li>对于TCP的接收方，在某一时刻在它的接收缓存内存在3种。<br>“已接收”，“未接收准备接收”，“未接收并未准备接收”（由于ACK直接由TCP协议栈回复，默认无应用延迟，不存在“已接收未回复ACK”）。<br><strong>其中“未接收准备接收”称之为接收窗口。</strong></li>
<li>TCP是<strong>双工</strong>的协议，会话的双方都可以同时接收、发送数据。<strong>TCP会话的双方都各自维护一个“发送窗口”和一个“接收窗口”。</strong>其中各自的“接收窗口”大小取决于应用、系统、硬件的限制（TCP传输速率不能大于应用的数据处理速率）。各自的“发送窗口”则要求取决于对端通告的“接收窗口”，要求相同。</li>
</ul>
<h3 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h3><p>流量控制解决了两台主机之间因传送速率而可能引起的丢包问题，在一方面保证了TCP数据传送的可靠性。然而如果网络非常拥堵，此时再发送数据就会加重网络负担，那么发送的数据段很可能超过了最大生存时间也没有到达接收方，就会产生丢包问题。<br>为此TCP引入慢启动机制，先发出少量数据，就像探路一样，先摸清当前的网络拥堵状态后，再决定按照多大的速度传送数据。<br>此处引入一个<strong>拥塞窗口</strong>：<br>发送开始时定义拥塞窗口大小为1；每次收到一个ACK应答，拥塞窗口加1；而在每次发送数据时，发送窗口取拥塞窗口与接送段接收窗口最小者。<br>慢启动：在启动初期以指数增长方式增长；设置一个慢启动的阈值，当以指数增长达到阈值时就停止指数增长，按照线性增长方式增加；线性增长达到网络拥塞时立即“乘法减小”，拥塞窗口置回1，进行新一轮的“慢启动”，同时新一轮的阈值变为原来的一半。 </p>
<h2 id="TCP长连接-amp-短连接"><a href="#TCP长连接-amp-短连接" class="headerlink" title="TCP长连接 &amp; 短连接"></a>TCP长连接 &amp; 短连接</h2><ul>
<li><p>长连接，也叫持久连接，在TCP层握手成功后，不立即断开连接，并在此连接的基础上进行多次消息（包括心跳）交互，直至连接的任意一方（客户端OR服务端）主动断开连接，此过程称为一次完整的长连接。<strong>HTTP 1.1相对于1.0最重要的新特性就是引入了长连接。</strong><br>短连接</p>
</li>
<li><p>短连接，客户端收到服务端的响应后，立刻发送FIN消息，主动释放连接。也有服务端主动断连的情况，凡是在一次消息交互（发请求-收响应）之后立刻断开连接的情况都称为短连接。<strong>短连接是建立在TCP协议上的，有完整的握手挥手流程，区别于UDP协议。</strong></p>
</li>
</ul>
<h2 id="UDP有拥塞控制吗？如何解决？"><a href="#UDP有拥塞控制吗？如何解决？" class="headerlink" title="UDP有拥塞控制吗？如何解决？"></a>UDP有拥塞控制吗？如何解决？</h2><p>UDP没有拥塞控制，可以根据丢包率来判断网络的拥塞情况,如果网络拥塞,接收方通知发送方调整发送速率从而有效解决公平性问题以及UDP的拥塞控制问题。</p>
<h2 id="为什么要有-time-wait"><a href="#为什么要有-time-wait" class="headerlink" title="为什么要有 time_wait"></a>为什么要有 time_wait</h2><p>假设最终的ACK丢失，主机2将重发FIN，主机1必须维护TCP状态信息以便可以重发最终的ACK，否则会发送RST，结果主机2认为发生错误。TCP实现必须可靠地终止连接的两个方向(全双工关闭)，主机1必须进入 TIME_WAIT 状态，因为主机1可能面临重发最终ACK的情形。</p>
<blockquote>
<p>  <a href="https://www.zhihu.com/question/47378601/answer/276353285">https://www.zhihu.com/question/47378601/answer/276353285</a><br>    <a href="https://blog.csdn.net/xuzhangze/article/details/80490362">https://blog.csdn.net/xuzhangze/article/details/80490362</a><br>    <a href="https://www.cnblogs.com/xdyixia/p/9294885.html">https://www.cnblogs.com/xdyixia/p/9294885.html</a><br>    <a href="https://www.cnblogs.com/hongdada/p/11171068.html">https://www.cnblogs.com/hongdada/p/11171068.html</a><br>    <a href="https://www.cnblogs.com/Paul-watermelon/p/11141422.html">https://www.cnblogs.com/Paul-watermelon/p/11141422.html</a><br>    <a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=6576ba05a97ec447bcf1111cbd0485e9&amp;site=xueshu_se">https://xueshu.baidu.com/usercenter/paper/show?paperid=6576ba05a97ec447bcf1111cbd0485e9&amp;site=xueshu_se</a><br>    <a href="https://blog.51cto.com/11859650/1917938">https://blog.51cto.com/11859650/1917938</a></p>
</blockquote>
<h1 id="http-amp-https"><a href="#http-amp-https" class="headerlink" title="http &amp; https"></a>http &amp; https</h1><h2 id="http与https的区别"><a href="#http与https的区别" class="headerlink" title="http与https的区别"></a>http与https的区别</h2><p>HTTPS是在HTTP上建立SSL加密层，并对传输数据进行加密，是HTTP协议的安全版。HTTPS协议，它比HTTP协议相比多了以下优势:<br>数据隐私性：内容经过对称加密，每个连接生成一个唯一的加密密钥<br>数据完整性：内容传输经过完整性校验<br>身份认证：第三方无法伪造服务端（客户端）身份（数字签名）<br>发送密文的一方使用对方的公钥进行加密处理“对称的密钥”，然后对方用自己的私钥解密拿到“对称的密钥”，这样可以确保交换的密钥是安全的前提下，使用对称加密方式进行通信。所以，<strong>HTTPS采用对称加密和非对称加密两者并用的混合加密机制。</strong></p>
<ul>
<li>HTTPS比HTTP更加安全，对搜索引擎更友好，利于SEO，谷歌、百度优先索引HTTPS网页;</li>
<li>HTTPS需要用到SSL证书，而HTTP不用;</li>
<li>HTTPS标准端口443，HTTP标准端口80;</li>
<li>HTTPS基于传输层，HTTP基于应用层;</li>
<li>HTTPS在浏览器显示绿色安全锁，HTTP没有显示;s</li>
<li>HTTPS需要使用证书，必须向认证机构（CA）购买。</li>
<li>HTTPS加密通信会消耗更多的CPU及内存资源</li>
</ul>
<h2 id="浏览器中输入网址到获得页面的全过程"><a href="#浏览器中输入网址到获得页面的全过程" class="headerlink" title="浏览器中输入网址到获得页面的全过程"></a>浏览器中输入网址到获得页面的全过程</h2><ol>
<li>浏览器中输入域名www.baidu.com</li>
<li>域名解析DNS得到对应的ip<br>找浏览器缓存-&gt;查看本机的host文件-&gt;路由器缓存-&gt;对本地DNS服务器进行递归查询-&gt;本地域名服务器采用迭代查询-&gt;根域名服务器（告诉本地顶级ip）-&gt;顶级域名服务器（告诉本地权威ip）-&gt;权威域名服务器（告诉本地查找结果）-&gt;本地域名服务器把查询结果告诉本机。</li>
<li>浏览器与目标服务器建立TCP连接：3次握手连接</li>
<li>浏览器通过http协议向目标服务器发送请求<br>浏览器向主机发起一个HTTP-GET方法报文请求。请求中包含访问的URL，也就是<a href="http://www.baidu.com/等信息。Cookies如果是首次访问，会提示服务器建立用户缓存信息。">http://www.baidu.com/等信息。Cookies如果是首次访问，会提示服务器建立用户缓存信息。</a></li>
<li>服务器给出响应，将指定文件发送给浏览器<br>状态行，响应头，响应实体内容，返回状态码200 OK，表示服务器可以响应请求，返回报文，由于在报头中Content-type为“text/html”，浏览器以HTML形式呈现，而不是下载文件。</li>
<li>TCP释放链接<br>浏览器所在主机向服务器发出连接释放报文，然后停止发送数据；<br><strong>服务器接收到释放报文后发出确认报文，然后将服务器上未传送完的数据发送完；</strong><br>服务器数据传输完毕后，向客户机发送连接释放报文；<br>客户机接收到报文后，发出确认，然后等待一段时间后，释放TCP连接；</li>
<li>浏览器显示页面中所有文本。<br>浏览器接收到返回的数据包，根据浏览器的渲染机制对相应的数据进行渲染。渲染后的数据，进行相应的页面呈现和脚步的交互。</li>
</ol>
<ul>
<li>涉及到的协议<br>  应用层：HTTP(WWW访问协议)，DNS(域名解析服务)<br>  传输层：TCP(为HTTP提供可靠的数据传输)，UDP(DNS使用UDP传输)<br>  网络层：IP(IP数据数据包传输和路由选择)，ICMP(提供网络传输过程中的差错检测)，ARP(将本机的默认网关IP地址映射成物理MAC地址)</li>
</ul>
<h2 id="输入-www-baidu-com，怎么变成-https-www-baidu-com-的，怎么确定用HTTP还是HTTPS？"><a href="#输入-www-baidu-com，怎么变成-https-www-baidu-com-的，怎么确定用HTTP还是HTTPS？" class="headerlink" title="输入 www.baidu.com，怎么变成 https://www.baidu.com 的，怎么确定用HTTP还是HTTPS？"></a>输入 www.baidu.com，怎么变成 <a href="https://www.baidu.com">https://www.baidu.com</a> 的，怎么确定用HTTP还是HTTPS？</h2><p>一种是原始的302跳转，服务器把所有的HTTP流量跳转到HTTPS。但这样有一个漏洞，就是中间人可能在第一次访问站点的时候就劫持。<br>解决方法是引入HSTS机制，用户浏览器在访问站点的时候强制使用HTTPS。</p>
<h2 id="HTTP可以使用UDP吗"><a href="#HTTP可以使用UDP吗" class="headerlink" title="HTTP可以使用UDP吗"></a>HTTP可以使用UDP吗</h2><p>HTTP3。对传输的报文进行充分的加密</p>
<h2 id="HTTP协议的报文格式"><a href="#HTTP协议的报文格式" class="headerlink" title="HTTP协议的报文格式"></a>HTTP协议的报文格式</h2><p>客户端请求request报文格式：请求行（请求方法，URL，HTTP协议版本）、请求头部（header）、空行、请求数据<br>服务器响应response报文格式：状态行（HTTP协议版本， 状态码， 状态消息）、消息报头、空行、响应正文</p>
<h2 id="Http1-0-amp-HTTP1-1"><a href="#Http1-0-amp-HTTP1-1" class="headerlink" title="Http1.0 &amp; HTTP1.1"></a>Http1.0 &amp; HTTP1.1</h2><ol>
<li>缓存处理，在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略。</li>
<li>带宽优化及网络连接的使用，HTTP1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了，并且不支持断点续传功能，HTTP1.1则在请求头引入了range头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接。</li>
<li>错误通知的管理，在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除。</li>
<li>Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。</li>
<li>长连接，HTTP 1.1支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。</li>
</ol>
<h2 id="HTTP2-0和HTTP1-X相比的新特性"><a href="#HTTP2-0和HTTP1-X相比的新特性" class="headerlink" title="HTTP2.0和HTTP1.X相比的新特性"></a>HTTP2.0和HTTP1.X相比的新特性</h2><ol>
<li>新的二进制格式（Binary Format），HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。</li>
<li>多路复用（MultiPlexing），即连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。</li>
<li>header压缩，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。</li>
<li>服务端推送（server push），同SPDY一样，HTTP2.0也具有server push功能。</li>
</ol>
<h2 id="HTTP请求方法"><a href="#HTTP请求方法" class="headerlink" title="HTTP请求方法"></a>HTTP请求方法</h2><ol>
<li><strong>GET方法意思是获取URL指定的资源</strong>，使用GET方法时，可以将请求参数和对应的值附加在 URI 后面，利用一个问号(“?”)将资源的URI和请求参数隔开，参数之间使用与符号(“&amp;”)隔开，因此传递参数长度也受到了限制，而且与隐私相关的信息也直接暴露在URI中。比如/index.jsp?username=holmofy&amp;password=123123</li>
<li>HEAD 方法与GET用法相同，但没有响应体，使用场合没有GET多。比如下载前使用HEAD发送请求，通过ContentLength响应字段，来了解网络资源的大小；或者通过LastModified响应字段来判断本地缓存资源是否要更新。 </li>
<li><strong>POST 方法一般用提交信息或数据，请求服务器进行处理（例如提交表单或者上传文件）。</strong>表单使用POST相对GET来说还是比较隐秘的，而且GET的URL有长度限制，而上传大文件就必须要使用POST了。</li>
<li>OPTIONS方法比较少见，该方法用于请求服务器告知其支持哪些其他的功能和方法。通过OPTIONS 方法，可以询问服务器具体支持哪些方法，或者服务器会使用什么样的方法来处理一些特殊资源。可以说这是一个探测性的方法，客户端通过该方法可以在不访问服务器上实际资源的情况下就知道处理该资源的最优方式。这个选项在跨域HTTP请求的情况出现的比较多，这里有一片关于跨域请求的文章，其中有一张图很好的解释了什么是跨域HTTP请求。 </li>
</ol>
<h2 id="GET与POST的区别"><a href="#GET与POST的区别" class="headerlink" title="GET与POST的区别"></a>GET与POST的区别</h2><p>GET是幂等的，即读取同一个资源，总是得到相同的数据，POST不是幂等的；<br>GET一般用于从服务器获取资源，而POST有可能改变服务器上的资源；<br>请求形式上：GET请求的数据附在URL之后，在HTTP请求头中；POST请求的数据在请求体中；<br>安全性：GET请求可被缓存、收藏、保留到历史记录，且其请求数据明文出现在URL中。POST的参数不会被保存，安全性相对较高；<br>GET只允许ASCII字符，POST对数据类型没有要求，也允许二进制数据；<br>GET的长度有限制（操作系统或者浏览器），而POST数据大小无限制</p>
<h2 id="常见的状态码"><a href="#常见的状态码" class="headerlink" title="常见的状态码"></a>常见的状态码</h2><p>1XX：信息提示。表示请求已被服务器接受，但需要继续处理，范围为100~101。<br>2XX：请求成功。服务器成功处理了请求。范围为200~206。<br>3XX：客户端重定向。重定向状态码用于告诉客户端浏览器，它们访问的资源已被移动，并告诉客户端新的资源位置。客户端收到重定向会重新对新资源发起请求。范围为300~305。<br>4XX：客户端信息错误。客户端可能发送了服务器无法处理的东西，比如请求的格式错误，或者请求了一个不存在的资源。范围为400~415。<br>5XX：服务器出错。客户端发送了有效的请求，但是服务器自身出现错误，比如Web程序运行出错。范围是500~505。<br>200: ok<br>304: not modified<br>400: bad request<br>404: not found</p>
<blockquote>
<p>  <a href="https://blog.csdn.net/Goligory/article/details/104513317">https://blog.csdn.net/Goligory/article/details/104513317</a><br>    <a href="https://blog.csdn.net/qiuchaoxi/article/details/79415400">https://blog.csdn.net/qiuchaoxi/article/details/79415400</a><br>    <a href="https://www.cnblogs.com/jpfss/p/10984966.html">https://www.cnblogs.com/jpfss/p/10984966.html</a><br>    <a href="https://blog.csdn.net/qq_38128179/article/details/85068195">https://blog.csdn.net/qq_38128179/article/details/85068195</a><br>    <a href="https://blog.csdn.net/qq_40638598/article/details/105293250">https://blog.csdn.net/qq_40638598/article/details/105293250</a><br>    <a href="https://www.cnblogs.com/heluan/p/8620312.html">https://www.cnblogs.com/heluan/p/8620312.html</a></p>
</blockquote>
<h1 id="Cookie-amp-Session"><a href="#Cookie-amp-Session" class="headerlink" title="Cookie &amp; Session"></a>Cookie &amp; Session</h1><h2 id="Cookie的参数有哪些？"><a href="#Cookie的参数有哪些？" class="headerlink" title="Cookie的参数有哪些？"></a>Cookie的参数有哪些？</h2><p>setcookie()函数向客户端发送一个 HTTP cookie。<br>cookie的名称指定为相同名称的变量。例如，如果被发送的 cookie 名为 “name”，会自动创建名为 $user 的变量，包含cookie 的值。<br>setcookie(name,value,expire,path,domain,secure)<br>名称、值、有效期、服务器路径、域名、是否用安全的https来传输</p>
<h2 id="Cookie被仿造怎么办？"><a href="#Cookie被仿造怎么办？" class="headerlink" title="Cookie被仿造怎么办？"></a>Cookie被仿造怎么办？</h2><p>建议对客户端标识的用户敏感信息数据，使用Session会话认证方式，避免被他人仿冒身份。<br>敏感的信息如账号密码等尽量不要写到Cookie中。最好是像Google、Baidu那样将Cookie信息加密，提交到服务器后再进行解密，保证Cookie中的信息只要本人能读得懂。而假如选择Session就省事多了，反正是放在服务器上，Session里任何隐私都能够有效的保护。</p>
<h2 id="Cookie和Session的区别"><a href="#Cookie和Session的区别" class="headerlink" title="Cookie和Session的区别"></a>Cookie和Session的区别</h2><ol>
<li>数据存放位置不同：cookie数据存放在客户的浏览器上，session数据放在服务器上。</li>
<li>安全程度不同：cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗，考虑到安全应当使用session。</li>
<li>性能使用程度不同：session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用cookie；cookie会附加在每个http请求中，无形增加了流量，对于大data不适用。</li>
<li>数据存储大小不同：单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie，而session则存储与服务端，浏览器对其没有限制。</li>
<li>浏览器支持不同：Cookie是需要客户端浏览器支持的。假如客户端禁用了Cookie，或者不支持Cookie，则会话跟踪会失效。关于WAP上的应用，常规的Cookie就派不上用场了。</li>
<li>跨域支持不同：Cookie支持跨域名访问，例如将domain属性设置为“.biaodianfu.com”，则以“.biaodianfu.com”为后缀的一切域名均能够访问该Cookie。跨域名Cookie如今被普遍用在网络中，例如Google、Baidu、Sina等。而Session则不会支持跨域名访问。Session仅在他所在的域名内有效。</li>
<li>存放数据类型不同：cookie存放的树ASCLL码表示的数据，而Session中可以保存任意类型的数据。</li>
</ol>
<h2 id="Session校验服务器该如何设计"><a href="#Session校验服务器该如何设计" class="headerlink" title="Session校验服务器该如何设计"></a>Session校验服务器该如何设计</h2><p>session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串。<br>每个客户端请求都会和服务器建立一个会话，会有一个唯一的SessionId, 客户端在收到SessionId之后可以将seesionId保存(比如保存在cookie中)，然后每次通过cookie的sessionId去验证。<br>缺点：时间较短，需要服务器设置；服务器挂了需要重新登录。<br>1、用户向服务器发送用户名和密码。<br>2、服务器验证通过后，在当前session里面保存相关数据，比如用户角色、登录时间等等。<br>3、服务器向用户返回一个 session_id，写入用户的 Cookie。<br>4、用户随后的每一次请求，都会通过 Cookie，将 session_id 传回服务器。<br>5、服务器收到 session_id，找到前期保存的数据，由此得知用户的身份。</p>
<h2 id="Session劫持"><a href="#Session劫持" class="headerlink" title="Session劫持"></a>Session劫持</h2><p>Session劫持攻击通过窃取或预测有效的Session令牌来获得未经授权Web服务器访问权限。<br>Session ID应至少为128位长，以防止蛮力Session猜测攻击。</p>
<blockquote>
<p>  <a href="https://blog.csdn.net/qq_36031499/article/details/54573461">https://blog.csdn.net/qq_36031499/article/details/54573461</a><br>    <a href="https://zhidao.baidu.com/question/575343552.html">https://zhidao.baidu.com/question/575343552.html</a><br>    <a href="https://www.cnblogs.com/hnzheng/p/12732047.html">https://www.cnblogs.com/hnzheng/p/12732047.html</a></p>
</blockquote>
<h1 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h1><h2 id="解释下Cache的运行过程，怎么保证cache一致性"><a href="#解释下Cache的运行过程，怎么保证cache一致性" class="headerlink" title="解释下Cache的运行过程，怎么保证cache一致性"></a>解释下Cache的运行过程，怎么保证cache一致性</h2><p>三级缓存结构cpu -&gt; cache -&gt; memory<br>缓存一致性：用于保证多个CPU cache之间缓存共享数据的一致。<br>MESI，则是缓存一致性协议中的一个，到底怎么实现，还是得看具体的处理器指令集。<br><strong>读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。更新的时候，先更新数据库，然后再删除缓存。</strong><br><strong>更新的时候，先更新数据库，然后再删除缓存。</strong><br>如果不删除而是使用更新，在并发的情况下可能导致数据不一致。</p>
<h2 id="Cache的算法"><a href="#Cache的算法" class="headerlink" title="Cache的算法"></a>Cache的算法</h2><h3 id="FIFO算法"><a href="#FIFO算法" class="headerlink" title="FIFO算法"></a>FIFO算法</h3><p><strong>FIFO算法的思想是先进先出（FIFO，队列）</strong>，这是最简单、最公平的一种思想，即如果一个数据是最先进入的，那么可以认为在将来它被访问的可能性很小。<strong>空间满的时候，最先进入的数据会被最早置换（淘汰）掉</strong>。<br>FIFO 算法的描述：设计一种缓存结构，该结构在构造时确定大小，假设大小为 K，并有两个功能：<br>set(key,value)：将记录(key,value)插入该结构。当缓存满时，将最先进入缓存的数据置换掉。<br>get(key)：返回key对应的value值。<br>实现：维护一个FIFO队列，按照时间顺序将各数据（已分配页面）链接起来组成队列，并将置换指针指向队列的队首。再进行置换时，只需把置换指针所指的数据（页面）顺次换出，并把新加入的数据插到队尾即可。<br>缺点：判断一个页面置换算法优劣的指标就是缺页率，而FIFO算法的一个显著的缺点是，在某些特定的时刻，缺页率反而会随着分配页面的增加而增加，这称为Belady现象。产生Belady现象现象的原因是，FIFO置换算法与进程访问内存的动态特征是不相容的，被置换的内存页面往往是被频繁访问的，或者没有给进程分配足够的页面，因此FIFO算法会使一些页面频繁地被替换和重新申请内存，从而导致缺页率增加。因此，现在不再使用FIFO算法。</p>
<h3 id="LRU算法"><a href="#LRU算法" class="headerlink" title="LRU算法"></a>LRU算法</h3><p><strong>LRU（The Least Recently Used，最近最久未使用算法）</strong>的思想是：如果一个数据在最近一段时间没有被访问到，那么可以认为在将来它被访问的可能性也很小。因此，<strong>当空间满时，最久没有访问的数据最先被置换（淘汰）。</strong><br>LRU算法的描述： 设计一种缓存结构，该结构在构造时确定大小，假设大小为 K，并有两个功能：<br>set(key,value)：将记录(key,value)插入该结构。当缓存满时，将最久未使用的数据置换掉。<br>get(key)：返回key对应的value值。<br>实现：最朴素的思想就是用数组+时间戳的方式，不过这样做效率较低。因此，我们可以用双向链表（LinkedList）+哈希表（HashMap）实现（链表用来表示位置，哈希表用来存储和查找），在Java里有对应的数据结构LinkedHashMap。</p>
<h3 id="LFU算法"><a href="#LFU算法" class="headerlink" title="LFU算法"></a>LFU算法</h3><p><strong>LFU（Least Frequently Used ，最近最少使用算法）</strong>的思想是：如果一个数据在最近一段时间很少被访问到，那么可以认为在将来它被访问的可能性也很小。因此，当空间满时，<strong>最小频率访问的数据最先被淘汰</strong>。<br>LFU 算法的描述：<br>设计一种缓存结构，该结构在构造时确定大小，假设大小为 K，并有两个功能：<br>set(key,value)：将记录(key,value)插入该结构。当缓存满时，将访问频率最低的数据置换掉。<br>get(key)：返回key对应的value值。<br>算法实现策略：考虑到 LFU 会淘汰访问频率最小的数据，我们需要一种合适的方法按大小顺序维护数据访问的频率。LFU 算法本质上可以看做是一个 top K 问题(K = 1)，即选出频率最小的元素，因此我们很容易想到可以用二项堆来选择频率最小的元素，这样的实现比较高效。最终实现策略为小顶堆+哈希表。</p>
<blockquote>
<p>  <a href="https://www.cnblogs.com/snow826520/p/8574824.html">https://www.cnblogs.com/snow826520/p/8574824.html</a><br>    <a href="https://www.cnblogs.com/hongdada/p/10406902.html">https://www.cnblogs.com/hongdada/p/10406902.html</a></p>
</blockquote>
<h1 id="计算机系统架构"><a href="#计算机系统架构" class="headerlink" title="计算机系统架构"></a>计算机系统架构</h1><h2 id="7层模型（OSI模型）和4层模型（TCP-IP-模型），每一层有哪些常见协议？"><a href="#7层模型（OSI模型）和4层模型（TCP-IP-模型），每一层有哪些常见协议？" class="headerlink" title="7层模型（OSI模型）和4层模型（TCP/IP 模型），每一层有哪些常见协议？"></a>7层模型（OSI模型）和4层模型（TCP/IP 模型），每一层有哪些常见协议？</h2><p>应用层、传输层、网络层常见协议：DNS 、 HTTP 、FTP、 STMP 、SSL、 TCP、 UDP、 ARP、 IP  </p>
<h2 id="路由器-交换机是哪一层"><a href="#路由器-交换机是哪一层" class="headerlink" title="路由器/交换机是哪一层"></a>路由器/交换机是哪一层</h2><p>网络层</p>
<h2 id="网络层用来干嘛？传输层用来干嘛？"><a href="#网络层用来干嘛？传输层用来干嘛？" class="headerlink" title="网络层用来干嘛？传输层用来干嘛？"></a>网络层用来干嘛？传输层用来干嘛？</h2><p>网络层只是负责传输，把数据交给指定的目标，<br>传输层用来把收到的数据，根据协议分发给各个应用</p>
<h2 id="怎么给大量url和ip去重"><a href="#怎么给大量url和ip去重" class="headerlink" title="怎么给大量url和ip去重"></a>怎么给大量url和ip去重</h2><ol>
<li>内存够用，将URL存入hash链表，每个URL读入到hash链表中，遇到重复的就舍弃，否则加入到链表里面，最后遍历得到所有不重复的URL。空间复杂度M，时间复杂度为O(N+N/M)，M为不重复的URL，N为总URL数，但是M无法预测，所以存在风险，可能内存不足以存储所有的不重复URL。 </li>
<li>为了解决内存可能不足的问题，需要把hash链表变化成普通的hash表，每个hash表元素指向一个文件文件，这个文件记录了所有该hash值对应的无重复的URL，那么在加入URL的时候就遍历对应文件中的URL，没有重复则加入到文件中。这样做时间复杂度没有提升，但是每次都要读写文件，消耗的时间应该是上一种方式的三倍（依赖于io速度），而对内存的要求比较小。一个改进是加入URL的时候进行排序，这样能减少比对的次数。 </li>
</ol>
<h2 id="当在局域网中使用ping-www-xxx-com时，用到了哪些协议？"><a href="#当在局域网中使用ping-www-xxx-com时，用到了哪些协议？" class="headerlink" title="当在局域网中使用ping www.xxx.com时，用到了哪些协议？"></a>当在局域网中使用ping www.xxx.com时，用到了哪些协议？</h2><ol>
<li>因为ping的话 后面跟的是地址，所以要先<strong>将域名转换为ip地址，即用到了DNS</strong> </li>
<li>获取到ip地址后，<strong>在数据链路层是根据MAC地址传输的，所以要用到ARP解析服务，获取到MAC地址</strong></li>
<li>ping功能是测试另一台主机是否可达，<strong>程序发送一份ICMP回显请求给目标主机，并等待返回ICMP回显应答,</strong>（ICMP主要是用于ip主机、路由器之间传递控制信息，控制信息是指网络通不通，主机是否可达）</li>
</ol>
<blockquote>
<p>  <a href="https://blog.csdn.net/weixin_34212762/article/details/85514043">https://blog.csdn.net/weixin_34212762/article/details/85514043</a><br>    <a href="https://www.nowcoder.com/questionTerminal/e515ae7a18924fe3b6952ae7fbb985bc">https://www.nowcoder.com/questionTerminal/e515ae7a18924fe3b6952ae7fbb985bc</a></p>
</blockquote>
<h1 id="待整理问题"><a href="#待整理问题" class="headerlink" title="待整理问题"></a>待整理问题</h1><h3 id="为什么要并发控制？"><a href="#为什么要并发控制？" class="headerlink" title="为什么要并发控制？"></a>为什么要并发控制？</h3><h3 id="了解索引吗？知道实现原理吗？（B-树）"><a href="#了解索引吗？知道实现原理吗？（B-树）" class="headerlink" title="了解索引吗？知道实现原理吗？（B+树）"></a>了解索引吗？知道实现原理吗？（B+树）</h3><h3 id="DNS是什么？内部如何实现？"><a href="#DNS是什么？内部如何实现？" class="headerlink" title="DNS是什么？内部如何实现？"></a>DNS是什么？内部如何实现？</h3><h3 id="ARP是什么？ARP内部如何实现？"><a href="#ARP是什么？ARP内部如何实现？" class="headerlink" title="ARP是什么？ARP内部如何实现？"></a>ARP是什么？ARP内部如何实现？</h3><h3 id="Linux-io模型-select-poll-epoll的区别，水平触发和边缘触发的区别"><a href="#Linux-io模型-select-poll-epoll的区别，水平触发和边缘触发的区别" class="headerlink" title="Linux io模型(select, poll, epoll的区别，水平触发和边缘触发的区别)"></a>Linux io模型(select, poll, epoll的区别，水平触发和边缘触发的区别)</h3><h3 id="GDB有用过哪些"><a href="#GDB有用过哪些" class="headerlink" title="GDB有用过哪些"></a>GDB有用过哪些</h3><h3 id="谈谈你对前后端交互中使用的JSON的理解"><a href="#谈谈你对前后端交互中使用的JSON的理解" class="headerlink" title="谈谈你对前后端交互中使用的JSON的理解"></a>谈谈你对前后端交互中使用的JSON的理解</h3><h3 id="操作系统为什么有用户态和内核态，用户级线程与内核级线程如何转换"><a href="#操作系统为什么有用户态和内核态，用户级线程与内核级线程如何转换" class="headerlink" title="操作系统为什么有用户态和内核态，用户级线程与内核级线程如何转换"></a>操作系统为什么有用户态和内核态，用户级线程与内核级线程如何转换</h3><h3 id="函数调用汇编-怎么传参数-函数a-call-b，参数放在哪个栈帧"><a href="#函数调用汇编-怎么传参数-函数a-call-b，参数放在哪个栈帧" class="headerlink" title="函数调用汇编 怎么传参数 函数a call b，参数放在哪个栈帧"></a>函数调用汇编 怎么传参数 函数a call b，参数放在哪个栈帧</h3><h3 id="虚拟内存作用？-内存分页的作用？"><a href="#虚拟内存作用？-内存分页的作用？" class="headerlink" title="虚拟内存作用？ 内存分页的作用？"></a>虚拟内存作用？ 内存分页的作用？</h3><h3 id="缺页异常的介绍"><a href="#缺页异常的介绍" class="headerlink" title="缺页异常的介绍"></a>缺页异常的介绍</h3><h3 id="OOM问题和-StackOverFlow的区别"><a href="#OOM问题和-StackOverFlow的区别" class="headerlink" title="OOM问题和 StackOverFlow的区别"></a>OOM问题和 StackOverFlow的区别</h3><h3 id="数组怎么扩容？"><a href="#数组怎么扩容？" class="headerlink" title="数组怎么扩容？"></a>数组怎么扩容？</h3><p>新建一个更大新数组，然后复制过去……</p>
<h3 id="复制的时候锁住数组，所有的操作都阻塞？"><a href="#复制的时候锁住数组，所有的操作都阻塞？" class="headerlink" title="复制的时候锁住数组，所有的操作都阻塞？"></a>复制的时候锁住数组，所有的操作都阻塞？</h3><p>（面试之后去看了线程池里的阻塞队列，似乎都是用链表实现的，没有用数组。用到的队列似乎默认都是动态扩容的，最大为整数最大值。如果队列满了又不支持动态扩容，可以通过设置饱和策略来处理，默认是中止，也就是抛出 RejectedExecutionException。）</p>
<h3 id="读写分离有什么用？"><a href="#读写分离有什么用？" class="headerlink" title="读写分离有什么用？"></a>读写分离有什么用？</h3><p>写操作都在主库，读操作都在分库，让读操作能并发，提高效率。</p>
]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title>进程 线程 协程</title>
    <url>/2020/12/12/%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h1><h2 id="进程与线程的区别"><a href="#进程与线程的区别" class="headerlink" title="进程与线程的区别"></a>进程与线程的区别</h2><p>进程是资源（CPU、内存等）分配的基本单位，它是程序执行时的一个实例。程序运行时系统就会创建一个进程，并为它分配资源，然后把该进程放入进程就绪队列，进程调度器选中它的时候就会为它分配CPU时间，程序开始真正运行。<br>线程是程序执行时的最小单位，它是进程的一个执行流，是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。同样多线程也可以实现并发操作，每个请求分配一个线程来处理。<br><a id="more"></a></p>
<ol>
<li>进程是资源分配的最小单位，线程是程序执行的最小单位。</li>
<li>进程有自己的独立地址空间，每启动一个进程，系统就会为它分配地址空间，建立数据表来维护代码段、堆栈段和数据段，这种操作非常昂贵。而线程是共享进程中的数据的，使用相同的地址空间，因此CPU切换一个线程的花费远比进程要小很多，同时创建一个线程的开销也比进程要小很多。</li>
<li>线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（IPC)进行。不过如何处理好同步与互斥是编写多线程程序的难点。</li>
<li>多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间。</li>
</ol>
<h2 id="同一进程中的线程"><a href="#同一进程中的线程" class="headerlink" title="同一进程中的线程"></a>同一进程中的线程</h2><p>线程共享的环境包括：进程代码段、进程的公有数据(利用这些共享的数据，线程很容易的实现相互之间的通讯)、进程打开的文件描述符、信号的处理器、进程的当前目录和进程用户ID与进程组ID。<br><strong>在一个进程的线程共享堆区，而进程中的线程各自维持自己堆栈。</strong><br>堆：是大家共有的空间，分全局堆和局部堆。全局堆就是所有没有分配的空间，局部堆就是用户分配的空间。堆在操作系统对进程初始化的时候分配，运行过程中也可以向系统要额外的堆，但是记得用完了要还给操作系统，要不然就是内存泄漏。<br>栈：是个线程独有的，保存其运行状态和局部自动变量的。栈在线程开始的时候初始化，每个线程的栈互相独立，因此，栈是　thread safe的。操作系统在切换线程的时候会自动的切换栈，就是切换　ＳＳ／ＥＳＰ寄存器。栈空间不需要在高级语言里面显式的分配和释放。</p>
<h2 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h2><h3 id="怎么创建新进程，内核做了啥？"><a href="#怎么创建新进程，内核做了啥？" class="headerlink" title="怎么创建新进程，内核做了啥？"></a>怎么创建新进程，内核做了啥？</h3><p>fork, vfork，然后具体do_fork过程，copy on write等<br>进程 : 代码 + 数据 + 堆栈 + PCB<br>PCB （进程控制块）：pid 进程标识符 + pwd 进程标识 + ppid 父进程进程号</p>
<ol>
<li>分配一个 PID 从小到大找一个未被使用的进程号<br>0号进程是内核进程，它创建1号进程、还将物理内存搬到磁盘和磁盘搬到物理内存</li>
<li>分配PCB，拷贝父进程的 PCB的绝大部分数据</li>
<li>给子进程分配资源</li>
<li>复制父进程地址空间 </li>
<li>将子进程置成就绪状态，放入就绪队列</li>
</ol>
<p>在unix中系统调用的是：fork，fork会创建一个与父进程一摸一样的副本<br>在windows中该系统调用是：cresteprocess，既负责处理进程的创建，也负责把正确的程序装入新进程<br>无论是unix还是windows，进程只有一个父进程，不同的是：<br>在unix中所有的进程，都是以init进程为根，组成树形结构。父子进程共同组成一个进程组<br>在windows中，没有进程层次的概念，所有的进程地位都相同</p>
<h3 id="fork-作用"><a href="#fork-作用" class="headerlink" title="fork()作用"></a>fork()作用</h3><p>Linux系统函数fork()可以<strong>在父进程中创建一个子进程</strong>，这样的话，在一个进程接到来自客户端新的请求时就可以复制出一个子进程让其来处理，父进程只需负责监控请求的到来，然后创建子进程让其去处理，这样就能做到并发处理。<strong>fork函数会返回两次结果</strong>，因为操作系统会把当前进程的数据复制一遍，然后程序就分两个进程继续运行后面的代码，fork分别在父进程和子进程中返回，在子进程返回的值pid永远是0，在父进程返回的是子进程的进程id。打印的顺序与系统的进程调度有关。</p>
<ul>
<li>父子进程交替进行</li>
<li>父进程死亡，子进程将变成孤儿进程，由 1号 进程领养</li>
<li>子进程死亡，成为僵尸进程</li>
</ul>
<h3 id="多进程相关函数"><a href="#多进程相关函数" class="headerlink" title="多进程相关函数"></a>多进程相关函数</h3><p>multiprocessing模块就是跨平台版本的多进程模块。创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。<br>join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。<br>如果要启动大量的子进程，可以用进程池的方式批量创建子进程：对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了。<br>subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。<br>小结：</p>
<ul>
<li>在Unix/Linux下，可以使用fork()调用实现多进程。</li>
<li>要实现跨平台的多进程，可以使用multiprocessing模块。</li>
<li>进程间通信是通过Queue、Pipes等实现的。</li>
</ul>
<h3 id="进程间通信方式"><a href="#进程间通信方式" class="headerlink" title="进程间通信方式"></a>进程间通信方式</h3><ol>
<li>管道pipe：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。</li>
<li>命名管道FIFO：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。<br>消息队列MessageQueue：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。</li>
<li>共享存储SharedMemory：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。</li>
<li>信号量Semaphore：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，<strong>主要作为进程间以及同一进程内不同线程之间的同步手段</strong>。</li>
<li><strong>套接字Socket</strong>：套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。</li>
<li>信号signal： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。</li>
</ol>
<h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><h3 id="多线程的实现"><a href="#多线程的实现" class="headerlink" title="多线程的实现"></a>多线程的实现</h3><ol>
<li>继承Thread类，重写run方法</li>
<li>创建MyRunnable类实现runnable接口，重写run方法，创建MyRunnable类的对象，创建Thread类的对象，并把MyRunnable类对象作为参数传递</li>
<li>使用ExecutorService、Callable、Future实现有返回结果的多线程</li>
<li>通过线程池创建</li>
</ol>
<h3 id="优劣分析"><a href="#优劣分析" class="headerlink" title="优劣分析"></a>优劣分析</h3><ul>
<li>继承Thread<br>优势：编写简单，如果需要访问当前线程，则无需使用Thread.currentThread()方法，直接使用this即可获得当前线程。<br>劣势是：线程类已经继承了Thread类，所以不能再继承其他父类。</li>
<li>使用runnable或者callable<br>优势是：线程类只是实现了Runnable接口或Callable接口，还可以继承其他类。<br>在这种方式下，多个线程可以共享同一个target对象，所以非常适合多个相同线程来处理同一份资源的情况，从而可以将CPU、代码和数据分开，形成清晰的模型，较好地体现了面向对象的思想。<br>劣势是：编程稍微复杂，如果要访问当前线程，则必须使用Thread.currentThread()方法。</li>
<li>runnable和callable区别<br>Callable规定（重写）的方法是call()，Runnable规定（重写）的方法是run()。<br>Callable的任务执行后可返回值，而Runnable的任务是不能返回值的。<br>Call方法可以抛出异常，run方法不可以。<br>运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，以等待计算的完成，并检索计算的结果。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果。</li>
</ul>
<h3 id="多线程优点"><a href="#多线程优点" class="headerlink" title="多线程优点"></a>多线程优点</h3><ul>
<li>在一个程序中，有很多的操作是非常耗时的，如数据库读写操作，IO操作等，如果使用单线程，那么程序就必须等待这些操作执行完成之后才能执行其他操作。使用多线程，可以在将耗时任务放在后台继续执行的同时，同时执行其他操作。</li>
<li>可以提高程序的效率。</li>
<li>在一些等待的任务上，如用户输入，文件读取等，多线程就非常有用了。</li>
</ul>
<h3 id="多线程缺点"><a href="#多线程缺点" class="headerlink" title="多线程缺点"></a>多线程缺点</h3><ul>
<li>使用太多线程，是很耗系统资源，因为线程需要开辟内存。更多线程需要更多内存。</li>
<li>影响系统性能，因为操作系统需要在线程之间来回切换。</li>
<li>需要考虑线程操作对程序的影响，如线程挂起，中止等操作对程序的影响。</li>
<li>线程使用不当会发生很多问题。</li>
</ul>
<h3 id="什么是线程调度器-Thread-Scheduler-和时间分片-Time-Slicing-？"><a href="#什么是线程调度器-Thread-Scheduler-和时间分片-Time-Slicing-？" class="headerlink" title="什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing)？"></a>什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing)？</h3><p>线程调度器是一个操作系统服务，它负责为Runnable状态的线程分配CPU时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。时间分片是指将可用的CPU时间分配给可用的Runnable线程的过程。分配CPU时间可以基于线程优先级或者线程等待的时间。线程调度并不受到Java虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。</p>
<h3 id="线程的调度策略"><a href="#线程的调度策略" class="headerlink" title="线程的调度策略"></a>线程的调度策略</h3><p>线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行：</p>
<ul>
<li>线程体中调用了yield方法让出了对cpu的占用权利</li>
<li>线程体中调用了sleep方法使线程进入睡眠状态</li>
<li>线程由于IO操作受到阻塞</li>
<li>另外一个更高优先级线程出现</li>
<li>在支持时间片的系统中，该线程的时间片用完</li>
</ul>
<h3 id="线程的状态"><a href="#线程的状态" class="headerlink" title="线程的状态"></a>线程的状态</h3><ol>
<li>新建状态（New）：新建一个线程对象。</li>
<li>就绪/可运行状态（Runnable）：线程对象创建后，其他线程调用了该对象的start方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。</li>
<li>运行状态（Running）：就绪状态的线程获得CPU并执行程序代码。</li>
<li>阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：<br> <strong>等待阻塞</strong>：运行的线程执行wait方法，JVM会把该线程放入等待池中。(wait会释放持有的锁)<br> <strong>同步阻塞</strong>：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。<br> <strong>其他阻塞</strong>：运行的线程执行sleep或join方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep的状态超时、join等待线程终止或者超时、以及I/O处理完毕时，线程重新转入就绪状态。</li>
<li>死亡状态（Dead）：线程执行完成或者因异常退出run方法，该线程结束生命周期。</li>
</ol>
<h2 id="线程的相关问题"><a href="#线程的相关问题" class="headerlink" title="线程的相关问题"></a>线程的相关问题</h2><h3 id="ThreadLocal是什么？有什么用？"><a href="#ThreadLocal是什么？有什么用？" class="headerlink" title="ThreadLocal是什么？有什么用？"></a>ThreadLocal是什么？有什么用？</h3><p>ThreadLocal是一个本地线程副本变量工具类。主要用于将私有线程和该线程存放的副本对象做一个映射，各个线程之间的变量互不干扰，在高并发场景下，可以实现无状态的调用，特别适用于各个线程依赖不通的变量值完成操作的场景。<br>简单说ThreadLocal就是一种以空间换时间的做法，在每个Thread里面维护了一个以开地址法实现的ThreadLocal.ThreadLocalMap，把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了。</p>
<h3 id="为什么sleep是Thread的方法而不是Object的"><a href="#为什么sleep是Thread的方法而不是Object的" class="headerlink" title="为什么sleep是Thread的方法而不是Object的"></a>为什么sleep是Thread的方法而不是Object的</h3><ul>
<li>在java的内置锁机制中，每个对象都可以成为锁，也就是说每个对象都可以去调用wait，notify方法，而Object类是所有类的一个父类，把这些方法放在Object中，则java中的所有对象都可以去调用这些方法了。</li>
<li>一个线程可以拥有多个对象锁，wait，notify，notifyAll跟对象锁之间是有一个绑定关系的，比如你用对象锁aObject调用的wait()方法，那么你只能通过aObject.notify()或者aObject.notifyAll()来唤醒这个线程，这样jvm很容易就知道应该从哪个对象锁的等待池中去唤醒线程，假如用Thread.wait()，Thread.notify()，Thread.notifyAll()来调用，虚拟机根本就不知道需要操作的对象锁是哪一个。</li>
<li>sleep()是让某个线程暂停运行一段时间,其控制范围是由当前线程决定,也就是说,在线程里面决定.sleep()可以将一个线程睡眠，参数可以指定一个时间。</li>
<li>wait(),是由某个确定的对象来调用的，wait()可以将一个线程挂起，直到超时或者该线程被唤。</li>
<li>wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用，而sleep可以在任何地方使用。</li>
</ul>
<h3 id="线程B怎么知道线程A修改了变量"><a href="#线程B怎么知道线程A修改了变量" class="headerlink" title="线程B怎么知道线程A修改了变量"></a>线程B怎么知道线程A修改了变量</h3><ul>
<li>volatile修饰变量<br>Volatile实现内存可见性是通过store和load指令完成的；也就是对volatile变量执行写操作时，会在写操作后加入一条store指令，即强迫线程/进程将最新的值刷新到主内存中；而在读操作时，会加入一条load指令，即强迫从主内存中读入变量的值。每次读取volatile的变量时，都要从它的内存地址中读取，并没有说每次修改完volatile的变量后，都要立刻将它的值写回内存。volatile只提供了内存可见性，而没有提供原子性。<br><em>可见性：当多个线程同时访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。</em><br><em>原子性：一个操作或多个操作要么全部执行完成且执行过程不被中断，要么就不执行。</em><br><em>有序性：程序执行的顺序按照代码的先后顺序执行。</em></li>
<li>synchronized修饰修改变量的方法</li>
<li>wait/notify<br>wait()：使调用该方法的线程释放共享资源锁，然后从运行状态退出，进入等待队列，直到被再次唤醒。<br>wait(long)：超时等待一段时间，这里的参数时间是毫秒，也就是等待长达n毫秒，如果没有通知就超时返回。<br>wait(long，int)：对于超时时间更细力度的控制，单位为纳秒。<br>notify()：随机唤醒等待队列中等待同一共享资源的一个线程，并使该线程退出等待队列，进入可运行状态，也就是notify()方法仅通知一个线程。<br>notifyAll()：使所有正在等待队列中等待同一共享资源的全部线程退出等待队列，进入可运行状态。此时，优先级最高的那个线程最先执行，但也有可能是随机执行，这取决于JVM虚拟机的实现。</li>
<li>while轮询</li>
</ul>
<h3 id="保证线程安全怎么做？"><a href="#保证线程安全怎么做？" class="headerlink" title="保证线程安全怎么做？"></a>保证线程安全怎么做？</h3><p>当多个线程访问某个方法时，不管你通过怎样的调用方式、或者说这些线程如何交替地执行，我们在主程序中不需要去做任何的同步，这个类的结果行为都是我们设想的正确行为，那么我们就可以说这个类是线程安全的。<br>最常见的一种不安全的情况，就是我们A线程在进入方法后，拿到了count的值，刚把这个值读取出来，还没有改变count的值的时候，结果线程B也进来的，那么导致线程A和线程B拿到的count值是一样的。</p>
<ol>
<li>synchronized关键字<br>用来控制线程同步的，保证我们的线程在多线程环境下，不被多个线程同时执行，确保我们数据的完整性，使用方法一般是加在方法上。当synchronized锁住一个对象之后，别的线程如果想要获取锁对象，那么就必须等这个线程执行完释放锁对象之后才可以，否则一直处于等待状态。</li>
<li>Lock<br>Lock是在Java1.6被引入进来的，Lock的引入让锁有了可操作性。从使用上说Lock明显没有synchronized使用起来方便快捷。进入方法我们首先要获取到锁，然后去执行我们业务代码，这里跟synchronized不同的是，Lock获取的所对象需要我们亲自去进行释放，为了防止我们代码出现异常，所以我们的释放锁操作放在finally中，因为finally中的代码无论如何都是会执行的。<br>Lock在获取锁的时候，如果拿不到锁，就一直处于等待状态，直到拿到锁。<br>tryLock有一个Boolean的返回值，如果没有拿到锁，直接返回false，停止等待。</li>
</ol>
<h3 id="怎么等一个线程做完"><a href="#怎么等一个线程做完" class="headerlink" title="怎么等一个线程做完"></a>怎么等一个线程做完</h3><p>join ：主线程等待p终止（强调：是主线程处于等的状态，而p是处于运行的状态）<br>p.join只能join住start开启的进程，而不能join住run开启的进程 （即task）</p>
<h2 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h2><h3 id="什么是线程池"><a href="#什么是线程池" class="headerlink" title="什么是线程池"></a>什么是线程池</h3><p>线程池就是提前创建若干个线程，如果有任务需要处理，线程池里的线程就会处理任务，处理完之后线程并不会被销毁，而是等待下一个任务。</p>
<h3 id="为什么要线程池"><a href="#为什么要线程池" class="headerlink" title="为什么要线程池"></a>为什么要线程池</h3><p>由于创建和销毁线程都是消耗系统资源的，所以当需要频繁的创建和销毁线程的时候就可以考虑使用线程池来提升系统的性能。</p>
<h3 id="四种线程池的创建："><a href="#四种线程池的创建：" class="headerlink" title="四种线程池的创建："></a>四种线程池的创建：</h3><ol>
<li>newCachedThreadPool创建一个可缓存线程池</li>
<li>newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数。</li>
<li>newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。</li>
<li>newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务。</li>
</ol>
<h3 id="线程池的优点？"><a href="#线程池的优点？" class="headerlink" title="线程池的优点？"></a>线程池的优点？</h3><ol>
<li>重用存在的线程，减少对象创建销毁的开销。</li>
<li>可有效的控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。</li>
<li>提供定时执行、定期执行、单线程、并发数控制等功能。</li>
</ol>
<h3 id="线程池的中的线程数量能不能无限增加，为什么"><a href="#线程池的中的线程数量能不能无限增加，为什么" class="headerlink" title="线程池的中的线程数量能不能无限增加，为什么?"></a>线程池的中的线程数量能不能无限增加，为什么?</h3><p>不能，线程需要一定开销，太多线程会耗尽计算机资源，（而且过多线程也无法发挥多线程的优势，毕竟cpu核就那么多）</p>
<h3 id="java-里任务提交给线程池后，那些任务是存储在哪的"><a href="#java-里任务提交给线程池后，那些任务是存储在哪的" class="headerlink" title="java 里任务提交给线程池后，那些任务是存储在哪的?"></a>java 里任务提交给线程池后，那些任务是存储在哪的?</h3><p>把任务用 Runable（其实也可以是Callable） 表示，放在一个阻塞队列里。</p>
<h3 id="阻塞队列怎么实现？"><a href="#阻塞队列怎么实现？" class="headerlink" title="阻塞队列怎么实现？"></a>阻塞队列怎么实现？</h3><p>创建一个数组或者链表，每次取元素或者放元素就对数组操作。没有元素而要取元素时，阻塞，满了而要放元素时，阻塞。</p>
<h3 id="队列满了，那个线程池的-submit-方法会阻塞在那里？"><a href="#队列满了，那个线程池的-submit-方法会阻塞在那里？" class="headerlink" title="队列满了，那个线程池的 submit 方法会阻塞在那里？"></a>队列满了，那个线程池的 submit 方法会阻塞在那里？</h3><ol>
<li>如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，可以继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务</li>
<li>如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue 中，ArrayBlockingQueue满了，会根据maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是AbortPolicy。</li>
</ol>
<h2 id="协程"><a href="#协程" class="headerlink" title="协程"></a>协程</h2><h3 id="线程和协程区别"><a href="#线程和协程区别" class="headerlink" title="线程和协程区别"></a>线程和协程区别</h3><p>协程（Coroutines）是一种比线程更加轻量级的存在，<strong>一个线程可以拥有多个协程。协程不是被操作系统内核所管理的，而是完全由程序所控制，也就是在用户态执行。</strong>这样带来的好处是性能大幅度的提升，因为不会像线程切换那样消耗资源。<br>协程不是进程也不是线程，而是一个特殊的函数，这个函数可以在某个地方挂起，并且可以重新在挂起处外继续运行。所以说，协程与进程、线程相比并不是一个维度的概念。</p>
<ul>
<li>进程的切换者是操作系统，切换时机是根据操作系统自己的切换策略，用户是无感知的。进程的切换内容包括页全局目录、内核栈、硬件上下文，切换内容保存在内存中。进程切换过程是由“用户态到内核态到用户态”的方式，切换效率低。</li>
<li>线程的切换者是操作系统，切换时机是根据操作系统自己的切换策略，用户无感知。线程的切换内容包括内核栈和硬件上下文。线程切换内容保存在内核栈中。线程切换过程是由“用户态到内核态到用户态”， 切换效率中等。</li>
<li>协程的切换者是用户（编程者或应用程序），切换时机是用户自己的程序所决定的。协程的切换内容是硬件上下文，切换内存保存在用户自己的变量（用户栈或堆）中。协程的切换过程只有用户态，即没有陷入内核态，因此切换效率高。</li>
</ul>
<h3 id="协程Coroutine用法"><a href="#协程Coroutine用法" class="headerlink" title="协程Coroutine用法"></a>协程Coroutine用法</h3><ul>
<li>将复杂操作分帧计算。</li>
<li>异步下载。</li>
<li>使用yield return coroutine等待协程，将多个异步逻辑串联。如先进行异步下载，完成下载任务后再接着运算。</li>
<li>创建互斥区。如某个下载函数同一时刻只能有一个协程进入。</li>
</ul>
<h3 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h3><p>协程的完成主要靠yield关键字，带有yield关键字的函数自动变成生成器，根据yield函数的记录标签位置继续执行协程函数的后面部分代码。</p>
<ul>
<li>yield return new WaitForSeconds(3.0f); // 等待3秒，然后继续从此处开始，常用于做定时器</li>
<li>yield return null; // 这一帧到此暂停，下一帧再从暂停处继续，常用于循环中</li>
<li>yield return new WaitForEndOfFrame(); // 等到这一帧的cameras和GUI渲染结束后再从此处继续，即等到这帧的末尾再往下运行。这行之后的代码还是在当前帧运行，是在下一帧开始前执行，跟return null很相似</li>
<li>yield return new WaitForFixedUpdate(); // 在下一次执行FixedUpdate的时候继续执行这段代码，即等一次物理引擎的更新</li>
<li>yield return www; // 等待直至异步下载完成</li>
<li>yield break; // 直接跳出协程，对某些判定失败必须跳出的时候，比如加载AssetBundle的时候，WWW失败了，后边加载bundle没有必要了，这时候可以yield break跳出。</li>
<li>yield return StartCoroutine(methodName); // 等待另一个协程执行完。这是把协程串联起来的关键，常用于让多个协程按顺序逐个运行</li>
</ul>
<blockquote>
<p>  <a href="https://www.cnblogs.com/zhehan54/p/6130030.html">https://www.cnblogs.com/zhehan54/p/6130030.html</a><br>    <a href="https://blog.csdn.net/zhaohong_bo/java/article/details/89552188">https://blog.csdn.net/zhaohong_bo/java/article/details/89552188</a><br>    <a href="https://blog.csdn.net/shuilan0066/article/details/7683315">https://blog.csdn.net/shuilan0066/article/details/7683315</a><br>    <a href="https://www.cnblogs.com/YeLing0119/p/9754090.html">https://www.cnblogs.com/YeLing0119/p/9754090.html</a><br>    <a href="https://blog.csdn.net/boteman123/article/details/81000631">https://blog.csdn.net/boteman123/article/details/81000631</a><br>    <a href="https://blog.csdn.net/beidaol/article/details/89135277">https://blog.csdn.net/beidaol/article/details/89135277</a><br>    <a href="https://blog.csdn.net/tanmomo/article/details/99671622">https://blog.csdn.net/tanmomo/article/details/99671622</a><br>    <a href="https://www.jianshu.com/p/bb07a4d047eb">https://www.jianshu.com/p/bb07a4d047eb</a><br>    <a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017628290184064">https://www.liaoxuefeng.com/wiki/1016959663602400/1017628290184064</a><br>    <a href="https://www.cnblogs.com/linuxAndMcu/p/11064916.html">https://www.cnblogs.com/linuxAndMcu/p/11064916.html</a><br>    <a href="https://www.cnblogs.com/CharlesGrant/p/8125841.html">https://www.cnblogs.com/CharlesGrant/p/8125841.html</a><br>    <a href="https://blog.csdn.net/y277an/java/article/details/98697454">https://blog.csdn.net/y277an/java/article/details/98697454</a><br>    <a href="https://www.jianshu.com/p/6dde7f92951e">https://www.jianshu.com/p/6dde7f92951e</a><br>    <a href="https://blog.csdn.net/qq_18995513/article/details/51944602?utm_source=itdadao&amp;utm_medium=referral">https://blog.csdn.net/qq_18995513/article/details/51944602?utm_source=itdadao&amp;utm_medium=referral</a></p>
</blockquote>
]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>programming language</tag>
      </tags>
  </entry>
  <entry>
    <title>HashMap</title>
    <url>/2020/12/23/HashMap/</url>
    <content><![CDATA[<h2 id="java里hashmap是如何实现的？"><a href="#java里hashmap是如何实现的？" class="headerlink" title="java里hashmap是如何实现的？"></a>java里hashmap是如何实现的？</h2><p>JDK1.7：数组+链表<br>JDK1.8：数组+链表+红黑树</p>
<h2 id="HashMap-为什么线程不安全？为什么ConcurrHashMap线程安全？"><a href="#HashMap-为什么线程不安全？为什么ConcurrHashMap线程安全？" class="headerlink" title="HashMap 为什么线程不安全？为什么ConcurrHashMap线程安全？"></a>HashMap 为什么线程不安全？为什么ConcurrHashMap线程安全？</h2><p>jdk 1.7使用的是头插法，在多线程环境下，使用HashMap的put操作会引起死循环，原因是多线程会导致HashMap的Entry链表形成环形数据结构，导致Entry的next节点永远不为空，就会产生死循环获取Entry。<br>ConcurrentHashMap 采用分段锁的方式，即每一段数据配一把锁，当一个线程占用其中一段时，其它段的数据仍然可以put或get。<br><a id="more"></a></p>
<h2 id="ConcurrentHashMap的具体实现"><a href="#ConcurrentHashMap的具体实现" class="headerlink" title="ConcurrentHashMap的具体实现"></a>ConcurrentHashMap的具体实现</h2><p>底层分段数组+链表，线程安全。把Map分为N个Segment，提供相同的线程安全，允许多个修改并发，可以段内扩容。<br>ConcurrentHashMap由Segment数组结构和HashEntry数组结构组成；<br>Segment是一种可重入锁（ReentrantLock），HashEntry用于存储键值对数据；<br>一个ConcurrentHashMap包含一个由若干个Segment对象组成的数组，每个Segment对象守护整个散列映射表的若干个桶，每个桶是由若干个HashEntry对象链接起来的链表，table是一个由HashEntry对象组成的数组，table数组的每一个数组成员就是散列映射表的一个桶。<br>把“碰撞”的HashEntry对象链接成一个链表。由于HashEntry的next域为final型，所以新节点只能在链表的表头处插入。（头插法）</p>
<h2 id="hashArray和list有什么区别？"><a href="#hashArray和list有什么区别？" class="headerlink" title="hashArray和list有什么区别？"></a>hashArray和list有什么区别？</h2><h2 id="ConcurrentHashMap的扩容过程"><a href="#ConcurrentHashMap的扩容过程" class="headerlink" title="ConcurrentHashMap的扩容过程"></a>ConcurrentHashMap的扩容过程</h2><h2 id="ConcurrentHashMap多线程rehash的整体流程"><a href="#ConcurrentHashMap多线程rehash的整体流程" class="headerlink" title="ConcurrentHashMap多线程rehash的整体流程"></a>ConcurrentHashMap多线程rehash的整体流程</h2><h2 id="ConcurrentHashMap某个桶内超过八个结点会树化，什么情况下超过八个结点不会树化"><a href="#ConcurrentHashMap某个桶内超过八个结点会树化，什么情况下超过八个结点不会树化" class="headerlink" title="ConcurrentHashMap某个桶内超过八个结点会树化，什么情况下超过八个结点不会树化"></a>ConcurrentHashMap某个桶内超过八个结点会树化，什么情况下超过八个结点不会树化</h2><p>链表超过了八个结点但是数组大小没有超过64时会进行扩容而不是树化</p>
<h2 id="HashMap-1-7-1-8-的区别，在JDK1-8中有哪些改进？"><a href="#HashMap-1-7-1-8-的区别，在JDK1-8中有哪些改进？" class="headerlink" title="HashMap 1.7 / 1.8 的区别，在JDK1.8中有哪些改进？"></a>HashMap 1.7 / 1.8 的区别，在JDK1.8中有哪些改进？</h2><p>1.8加入了红黑树。只要用的还是链表，就一直有可能出现链表长度过于长的问题。链表插入效率高查询效率低，红黑树的插入和查询效率都不错。为什么不用别的树，是基于综合考虑：不仅要保证put的效率也要保证get的效率，红黑树的插入和查询效率相同，是一个折衷的考虑。<br>1.8加入了treeify树化的阈值（默认为8），含义是某个index上链表元素大于等于8时需要将链表改成红黑树，8个元素了此时查询效率较低。当一个红黑树结点不足6个就untreeify改成链表。这两个阈值不等的原因是为了防止在临界值进行插入or删除导致的频繁的数据结构转变。<br>1.8中初始化和扩容都在resize()函数中。<br>1.7中元素直接是Entry，1.8的元素称为Node，其中的属性都是一样的，红黑树中结点是TreeNode，TreeNode既是一个树节点又是一个双向链表结点（包含属性prev和next）<br>1.8中如果到达了树化阈值也不一定会树化，如果数组长度小于最小树化容量(MIN_TREEIFY_CAPACITY默认为64)，则会进行扩容而不是继续树化。因为这种情况下通过扩容，大概率也可以缩短这个链表而不需要树化。如果数组大小已经超过64了，那么处于节省内存空间的考虑，会选择树化而不是扩容。<br><strong>因此在1.8中只有当插入之前链表元素大于等于8且数组大小超过64的情况下会转换成红黑树</strong><br>1.8在扩容时应用了扩容的规律，即如果新的有效最高位是1则index是原index+原数组大小。<br>1.8中没有rehash，即1，8在扩容过程中不会重新计算hashcode<br>1.8中链表转红黑树首先把链表上每个结点改为TreeNode，在头结点上开始遍历，更改每个结点关于红黑树的属性即可（红黑树的根节点一定是黑色的）。比较过程先比较hash值再比较key的大小，若仍然相同则比较key的class再比较内部hash值（identityHashCode即jdk自带的hash值）</p>
<h2 id="ConcurrentHashMap-1-7-1-8-的区别，在JDK1-8中有哪些改进？"><a href="#ConcurrentHashMap-1-7-1-8-的区别，在JDK1-8中有哪些改进？" class="headerlink" title="ConcurrentHashMap 1.7 / 1.8 的区别，在JDK1.8中有哪些改进？"></a>ConcurrentHashMap 1.7 / 1.8 的区别，在JDK1.8中有哪些改进？</h2><h2 id="为什么-HashMap的size为2的幂次方-？"><a href="#为什么-HashMap的size为2的幂次方-？" class="headerlink" title="为什么 HashMap的size为2的幂次方 ？"></a>为什么 HashMap的size为2的幂次方 ？</h2><p>方便减一做与运算计算index。</p>
<h2 id="HashMap-resize-过程能否介绍-？"><a href="#HashMap-resize-过程能否介绍-？" class="headerlink" title="HashMap resize()过程能否介绍 ？"></a>HashMap resize()过程能否介绍 ？</h2><p>resize()即扩容过程。如果没有扩容机制，即数组大小固定，那么如果元素很多，一定会出现链表太长的情况，get()方法效率降低。扩容会新建一个大小翻倍的Entry数组，再使用transfer()方法将已有元素转移到新的数组上：根据元素已包含的hash值(hashcode)来计算在新数组中的index。原本index相同的两个元素可能位置仍然相同也可能相差原始数组的大小，因为只可能是新的最高有效位有区别，低k位一定相同(因为原来在同一位置)。<br>JDK 7中循环数组遍历链表，如果key为null则hash值为0，否则就是这个key的hash值。在新的数组容量下重新计算index，用头插法插入这个元素。<strong>key为null是一定存放在index = 0的位置。</strong>transfer过程中会判断是否需要重新计算hashcode，默认不会重新计算(rehash = false)。<br>JDK 8中计算e.hash &amp; oldCapacity，假设oldCapacity是2的k次幂则它的二进制表示中只有k+1位是1，这个与运算是在判断e.hash的第k+1位是否为0，如果是0，则index不变（低位lo），如果是1，则index += oldCapacity（高位hi）。对原始数组某一位置上的链表，逐个做与运算，根据结果分成两条链（尾插法形成链），直接放到新数组的lo和hi位。对于原始数组某一位置上的红黑树，分成两棵树，如果子树的元素总数小于untreeify的阈值（6）则转换成链表，否则转换成红黑树（如果没有分裂，则等价于将原始红黑树转移），分裂出新的树的时候需要对树进行调整并将新的root挪到链表的起点。</p>
<h2 id="HashMap效率受什么影响？"><a href="#HashMap效率受什么影响？" class="headerlink" title="HashMap效率受什么影响？"></a>HashMap效率受什么影响？</h2><p> (负载因子、hash数组size)</p>
<h2 id="HashMap中扰动函数的作用-？"><a href="#HashMap中扰动函数的作用-？" class="headerlink" title="HashMap中扰动函数的作用 ？"></a>HashMap中扰动函数的作用 ？</h2><h2 id="HashMap中put如何实现（向hashmap插入-key-value-）"><a href="#HashMap中put如何实现（向hashmap插入-key-value-）" class="headerlink" title="HashMap中put如何实现（向hashmap插入(key, value)）"></a>HashMap中put如何实现（向hashmap插入(key, value)）</h2><p>原理：</p>
<ol>
<li><p>（JDK1.7实现）判断是否为空table，如果为空，则初始化一个数组。要么使用默认容量16，要么使用传入的容量。在初始化过程中找到不小于这个容量的最小的2的幂(roundUpToPowerOf2(tosize))。比如传入initialCapacity为10，则初始化数组大小为16。最大容量是2^30(1&lt;<30)。重算扩容的阈值（min(capacity * loadfactor, max_capacity + 1)）。hashmap的参数：initialcapacity, loadfactor （初始容量默认16和加载因子默认0.75）entry<k, v>包含四个属性：key，value，next(Entry)，hash值(int)。hash值用于两个Entry对象的比较。</30)。重算扩容的阈值（min(capacity></p>
</li>
<li><p>计算hashcode<br>调用hash方法：调用key.hashCode()方法并与hash seed做异或运算，再对这个值进行右移或者异或运算。右移相当于高位移动到了低位，低位省略了，异或从而使得高位也可以参与index的计算，使index更平均（异或^：相同为0，不同为1）。hashCode方法可以重写。将计算好的hashCode存入Entry的hash属性。<br>1.8中仍然时右移和异或，只是没有1.7那么复杂，key为null时hashcode为0。</p>
</li>
<li><p>计算index<br>即：根据hashCode值计算数组下标。<br>计算key.hashCode() &amp; (table.length - 1) （二进制位运算速度快。table.length是2的k次幂，table.length - 1的低k位全1高位全0，该与操作相当于只保留了key的hashcode的后k位作为index，index的取值范围正好是0到table.length - 1）作为该entry存放在数组中的index。hashmap放入一个键值对时，将该键值对视作一个entry，把这个entry的引用放入数组的某个位置or这个位置连接的链表上，这个位置即index，需要通过key的值来计算。如果取余，很容易冲突（不同key放在数组的同一位置），且取余操作比位操作慢。</p>
</li>
<li><p>(JDK1.7中先扩容再添加，JDK1.8是先添加再扩容)扩容：1.7中，如果元素个数超过阈值且当前要放入的index处不为null则开始扩容。1.8中只判断了元素个数超过阈值就扩容。扩容方法：resize()<br>1.7在数组的每次扩容时会判断是否需要扩容，需要扩容时会改变hashseed，使得计算出的hashcode更加分散。</p>
</li>
<li><p>如果当前数组的当前index处为空，则新建一个Entry对象，包含key，value，hash和null（next为空）直接放入index处。否则在JDK 1.7中使用头插法：(key, value, hash, header)，其中header是链表头部，即table[index]，再把这个Entry放入table[index]，添加之后size++；而在1.8中使用尾插法，因为本来就需要遍历整个链表来判断元素个数是否达到treeify的阈值，同时在遍历链表时可以判断是否已经存在整个key，如果存在则直接覆盖，如果当前元素的key不相等，则判断当前元素的结点是否instanceof TreeNode，如果已经是一个树的则直接进行红黑树TreeNode结点的插入，否则当前index一定是链表，循环并累计链表上结点个数，对每个结点判断是都需要覆盖，如果下一个是空则说明已经到了尾部，插入这个结点，判断是否需要树化。</p>
</li>
</ol>
<p>如果是覆盖已有的key，则会用新的value覆盖旧的value并返回旧的value。在遍历寻找是否存在这个key的过程中先判断hash值是否相等，如果相等，再进一步比较key是否equals，这里的equals方法可能被重写得较为复杂，因此先对hash进行判断可以减少equals的执行从而提高代码运行效率。先寻找有无相同key，若无再添加这个Entry并返回null。</p>
<blockquote>
<p>hashmap.put(“1”, “2”)<br>String value = hashmap.put(“1”, “3”) // value = 2<br>String value = hashmap.put(“2”, “3”) // value = null, no overlap</p>
</blockquote>
<h2 id="为什么不直接用key做hash值而是与高16位做异或运算"><a href="#为什么不直接用key做hash值而是与高16位做异或运算" class="headerlink" title="为什么不直接用key做hash值而是与高16位做异或运算"></a>为什么不直接用key做hash值而是与高16位做异或运算</h2><p>异或可以使得高位也可以参与index的计算，使index更平均。</p>
<h2 id="Hashtable-和-HashMap的区别"><a href="#Hashtable-和-HashMap的区别" class="headerlink" title="Hashtable 和 HashMap的区别"></a>Hashtable 和 HashMap的区别</h2><p>底层数据结构 (JDK1.8后不同)、父类不同  、扩容方法不同 、 线程上锁范围不同（重点）<br>hashtable：底层数组+链表实现，无论是key还是value都不能为null。线程安全，在修改时synchronize锁住了整个hashTable，效率低。<br>HashMap：底层数组+链表实现，可以存储null作为key或value，线程不安全。加载因子：为了降低哈希冲突，键值对达到数组内存75%的时候进行扩容，直接*2。<br>ConcurrentHashMap不支持key为null<br>HashTable容器使用sychronized来保证线程安全，采取锁住整个表结构来达到同步目的，在线程竞争激烈的情况下，当一个线程访问HashTable的同步方法，其他线程也访问同步方法时，会进入阻塞或轮询状态；如线程1使用put方法时，其他线程既不能使用put方法，也不能使用get方法，效率非常低下。</p>
<h2 id="hashtable的理想的查找效率是多少？"><a href="#hashtable的理想的查找效率是多少？" class="headerlink" title="hashtable的理想的查找效率是多少？"></a>hashtable的理想的查找效率是多少？</h2><h2 id="哈希碰撞（在同一位置存放多个元素）怎么解决？"><a href="#哈希碰撞（在同一位置存放多个元素）怎么解决？" class="headerlink" title="哈希碰撞（在同一位置存放多个元素）怎么解决？"></a>哈希碰撞（在同一位置存放多个元素）怎么解决？</h2><p>链表法：在数组中的entry的引用上加上一个next指针指向下一个元素。对于单链表来说，插入头部的速度时最快的。但是在查找过程中，单链表从数组中存放的entry开始只能向下查找，无法向上回溯，因此可能无法有效获取头插法插入的元素。解决方法：头插之后将链表向下移动，将插入的头元素作为链表放在数组中的那个元素。由于移动头节点相当于移动整个链表，只需将头节点放入数组：table[index] = new Entry(key, value, table[index])<br>探查法，双重散列（发生冲突之后以当前地址为基础找下一个地址直到没有冲突），双重哈希</p>
<blockquote>
<p>参考：<a href="https://juejin.cn/post/6844904057128091655">https://juejin.cn/post/6844904057128091655</a><br>     <a href="https://www.cnblogs.com/sulishihupan/p/13418915.html">https://www.cnblogs.com/sulishihupan/p/13418915.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title>Adversarial Machine Learning</title>
    <url>/2021/01/12/DL_Adversarial%20Machine%20Learning/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Regular ML: build a model based on sample data $x$ in order to make prediction<br>Attack: add invisible noise $\eta$ to clean sample data and fool the model(make the wrong prediction)<br><strong>Adversarial Machine Learning: attempts to fool models by supplying deceptive input</strong><br><em>The malfunction of a ML model may cause damage in many aspects.</em><br><a id="more"></a></p>
<ul>
<li>Why ML models can be fooled?<br>The precision $\epsilon$ of an individual input feature is limited so that adding noise beyond that limitation($||\eta||&lt;\epsilon$) is <strong>both invisible to human and cannot be distinguished by hardware</strong>.<br>During the training period, the original data is processed by activation where slight perturbation may be magnified and thus leads to malfunction of the model.<br>the adversarial perturbation causes the activation to grow by $w^T\eta$:<script type="math/tex; mode=display">w^T\tilde{x}=w^Tx-w^T\eta</script><strong>We want to get the max $w^T\eta$ to fool the model.</strong><br><strong>And we need to get the min $\eta$ so that the slight change is hard to be detected.</strong><br>so we assign:<script type="math/tex; mode=display">\eta = \epsilon sign(w)</script>The activation will grow by <strong>$\epsilon mn$</strong> where $m$ is the average magnitude of each element of n-dimensional vector $w$.<br>A simple linear model can have adversarial examples if its input has sufficient dimensionality.</li>
</ul>
<h1 id="How-to-attack-a-ML-model"><a href="#How-to-attack-a-ML-model" class="headerlink" title="How to attack a ML model"></a>How to attack a ML model</h1><h2 id="White-box-Attack"><a href="#White-box-Attack" class="headerlink" title="White-box Attack"></a>White-box Attack</h2><p>An attacker has complete access to the ML model.<br>The main idea od white box attack is to generate adversarial examples.</p>
<h3 id="Fast-Gradient-Sign-Method-FGSM"><a href="#Fast-Gradient-Sign-Method-FGSM" class="headerlink" title="Fast Gradient Sign Method(FGSM)"></a>Fast Gradient Sign Method(FGSM)</h3><p>$\textit{J}(w,x,y^{true})$: loss function<br>SGD: update weights to <strong>decrease</strong> the loss: </p>
<script type="math/tex; mode=display">w = w - \nabla \textit{J}_w(w,x,y)</script><p>Attack: update samples to <strong>increase</strong> the loss: </p>
<script type="math/tex; mode=display">x = x + \nabla \textit{J}_x(w,x,y)</script><p>Perturbation:</p>
<script type="math/tex; mode=display">\eta = \epsilon sign(\nabla \textit{J}_x(w,x,y))\quad x = x + \eta</script><p>THis algorithm is cheap because we don’t update weights or parameters $w$.</p>
<h3 id="One-step-target-class-method"><a href="#One-step-target-class-method" class="headerlink" title="One-step target class method"></a>One-step target class method</h3><p>In FGSM:</p>
<script type="math/tex; mode=display">\tilde{x} = x + \epsilon sign(\nabla \textit{J}_x(w,x,y^{true}))</script><p>Using $y^{target}$ which are unlikely to be the correct class of $x$:</p>
<script type="math/tex; mode=display">\tilde{x} = x - \epsilon sign(\nabla \textit{J}_x(w,x,y^{target}))</script><h2 id="Black-box-Attack"><a href="#Black-box-Attack" class="headerlink" title="Black-box Attack"></a>Black-box Attack</h2><p>Attacker has no access to parameters but only has some idea of the domain of interests (eg. image classification). The number of queries to the system is limited.</p>
<h1 id="How-to-defend"><a href="#How-to-defend" class="headerlink" title="How to defend?"></a>How to defend?</h1><h1 id="How-to-effectively-defend"><a href="#How-to-effectively-defend" class="headerlink" title="How to effectively defend?"></a>How to effectively defend?</h1><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1>]]></content>
      <categories>
        <category>Neural Network and Deep Learning 2</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Reinforcement Learning</title>
    <url>/2021/01/19/DL_Reinforcement_Learning/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Alpha Go: beat the best human Go player<br>AlphaFold: a solution to the protein folding</p>
<p>Reinforcement Learning is a <strong>feedback-based</strong> machine learning technique: an agent learns to behave in an environment by performing the actions and getting the feedback.<br>Sequential decision making + long-term goal(getting the maximum positive rewards)<br>No pre-program and only learns from its own experience<br><a id="more"></a></p>
<p>During the period of exploring the environment, agent’s state gets changed, and agent receives a reward or penalty after each action as a feedback. The goal of RL is to learn which actions lead to maximum point.</p>
<h1 id="Terms-used-in-RL"><a href="#Terms-used-in-RL" class="headerlink" title="Terms used in RL"></a>Terms used in RL</h1><h2 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h2><p>An entity that can perceive/explore the environment and act upon it.<br>based on hit and trail process;<br>may get a delayed reward;<br>get the maximum positive rewards.</p>
<h2 id="Discount-factor"><a href="#Discount-factor" class="headerlink" title="Discount factor"></a>Discount factor</h2><p>Determines the importance of future rewards. A factor of 0 makes the agent “opportunistic” by only considering current rewards, while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, Q-values may diverge.</p>
<h2 id="Approaches-to-implement-RL"><a href="#Approaches-to-implement-RL" class="headerlink" title="Approaches to implement RL"></a>Approaches to implement RL</h2><ul>
<li><p>Value-based<br>The optimal value function, which is the maximum value at a state under any policy.</p>
</li>
<li><p>Policy-based<br>Find the optimal policy for the maximum future rewards without using the value function. (Deterministic VS Stochastic policy)</p>
</li>
<li><p>Model-based<br>The agent explores a virtual model before exploring the environment.</p>
</li>
</ul>
<h2 id="4-key-elements-of-RL"><a href="#4-key-elements-of-RL" class="headerlink" title="4 key elements of RL"></a>4 key elements of RL</h2><ul>
<li><p>Policy: how agent behave at a given time.<br>It can be a look-up table or computation as a search process.<br>Deterministic: $a = \pi (s)$<br>Stochastic: $\pi (a|s) = P[A_t=a|S_t=s]$</p>
</li>
<li><p>Reward Signal: at each state, environment sends a real-time reward signal.<br>Reward can change the policy to maximize the sum of rewards of actions.</p>
</li>
<li><p>Value Function: how much reward an agent can expect in the current state.<br>Depends on the reward, the goal of estimating values is to achieve more rewards.</p>
</li>
<li><p>Model of the environment: mimics the behavior of the environment for planning.<br>One can take actions by considering all future situations before actually experiencing them. (Model-based approach VS Model-free approach)</p>
</li>
</ul>
<h2 id="Types-of-Reinforcement-learning"><a href="#Types-of-Reinforcement-learning" class="headerlink" title="Types of Reinforcement learning"></a>Types of Reinforcement learning</h2><ul>
<li><p>Positive Reinforcement<br>Increase the strength of behavior<br>Sustain the changes for a long time: an overload of states that can reduce the consequences</p>
</li>
<li><p>Negative Reinforcement<br>Avoid negative conditions<br>More effective<br>Provides reinforcement only to meet minimum behavior</p>
</li>
</ul>
<h1 id="How-does-Reinforcement-Learning-work"><a href="#How-does-Reinforcement-Learning-work" class="headerlink" title="How does Reinforcement Learning work?"></a>How does Reinforcement Learning work?</h1><p>First we have an agent and an environment.<br>We assign an intial value to each state and update it according to the rewards.</p>
<p><strong>Bellman equation</strong><br>It is used to calculate the values of a decision in dynamic programming at a certain point by including the values of previous states.<br>key elements used in bellman equation:<br>$a$: action performed by the agent<br>$s$: state occurred by performing the action<br>$R$: reward or feedback obtained at a state by performing an action<br>$\gamma$: a discount factor</p>
<script type="math/tex; mode=display">V(s)=max_a[R(s,a)+\gamma V(s')]</script><p>$V(s)$: value at state s<br>$V(s’)$: value at the next state of s<br>We start the calculation from the block that is next to the target block.<br>Represent the agent state as the <strong>Markov Decision Process</strong>.<br>If the environment is <strong>completely observable</strong>, then its dynamic can be modeled as a Markov Process.<br>At each action, the environment responds and generates a new state.<br><strong>Current state transition doesn’t depend on past actions/states</strong><br><img src="/images/nndl2/RL_0.png" alt="RL_environment"><br>four elements contained in MDP:<br>$S$: a set of finite states<br>$A$: a set of finite actions<br>$R_a$: rewards received after transitioning from $S$ to $S’$ due to action a<br>$P_a$: probability of taking action a<br><strong>In RL, we only consider finite MDP.</strong><br>State $S$ and transition function $P$ can define the dynamics of the system.</p>
<h1 id="The-RL-Algorithms"><a href="#The-RL-Algorithms" class="headerlink" title="The RL Algorithms"></a>The RL Algorithms</h1><p>On-policy vs Off-policy<br>On-policy: learn policy $\pi$ from experience sampled from $\pi$, i.e, evaluate the policy that is used to generate data.<br>Off-policy: learn policy $\pi$ from experience sampled from $\mu$, i.e, improve a policy different from that used to generate data.</p>
<p><strong>Temporal Difference Learning</strong><br>learn how to predict a quantity that depends on future values of a given signal, comparing temporally successive predictions.</p>
<h2 id="Q-Learning-off-policy"><a href="#Q-Learning-off-policy" class="headerlink" title="Q-Learning (off-policy)"></a>Q-Learning (off-policy)</h2><p>It learns the value function $Q(S,a)$ which maps the state and action to rewards.<br><img src="/images/nndl2/RL_1.png" alt="RL_Q_Learning"><br>A model-free RL based on Bellman equation:</p>
<script type="math/tex; mode=display">V(s)=max_a[R(s,a)+\gamma \Sigma_{s'}P(s,a,s')V(s')]</script><p>$P$ is the probability of turning to state $s’$ when taking action $a$ on state $s$.<br>Value of a state $s$ depends on the action that has the max Q-value on this state:</p>
<script type="math/tex; mode=display">V(s) = max_a[Q(s, a)],</script><p>So we have:</p>
<script type="math/tex; mode=display">Q(s,a)=R(s,a)+\gamma \Sigma_{s'}P(s,a,s')max_{a'}Q(s',a')]</script><p>After each action, the table is updated, and the Q-values are stored within the table.</p>
<h2 id="State-Action-Reward-State-Action-SARSA-on-policy"><a href="#State-Action-Reward-State-Action-SARSA-on-policy" class="headerlink" title="State Action Reward State Action (SARSA, on-policy)"></a>State Action Reward State Action (SARSA, on-policy)</h2><p>SARSA is an on-policy temporal difference learning method, selects the action for each state while learning using a specific policy.<br>The goal of SARSA is to calculate the $Q_{\pi}(s, a)$ for the selected current policy $\pi$ and all pairs of $(s, a)$. SARSA uses the quintuple $Q(s, a, r,s’,a’)$:<br>$s$ and $a$: original state and action,<br>$r$: reward observed while following the states,<br>$s’$ and $a’$: new state, action pair.</p>
<h2 id="Deep-Q-Neural-Network-off-policy"><a href="#Deep-Q-Neural-Network-off-policy" class="headerlink" title="Deep Q Neural Network (off-policy)"></a>Deep Q Neural Network (off-policy)</h2><p>In a complex environment where exploring all possible states and actions costs too much, we can use neural network to replace the Q table and make approximation of sequential actions.<br><img src="/images/nndl2/RL_2.png" alt="Deep_Q_NN"></p>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning 2</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Generative Adversarial Networks</title>
    <url>/2021/01/26/DL_GAN/</url>
    <content><![CDATA[<h1 id="Introduction-to-GANs"><a href="#Introduction-to-GANs" class="headerlink" title="Introduction to GANs"></a>Introduction to GANs</h1><p>Discriminator learns from real dataset and make judgement of input data.<br>Generator uses noise vestor to generate fake data and feed into discriminator, based on the feedback it can generate better samples.<br><a id="more"></a></p>
<ul>
<li>Deep Convolutional GAN can train stable GANs at scale.</li>
<li>BigGan can generate synthetic photos that are practically indistinguishable.</li>
<li>Generate faces of anime characters.</li>
<li>Image-to-Image Translation: transfer from daytime to nighttime; sketcher to color.</li>
<li>Text-to-Image: generate plausible photos from textual descriptions of simple objects.</li>
<li>Semantic-Image-to-Photo: given a semantic image or sketch as input.</li>
<li>Face Frontal View Generation: given photos taken at an angle.</li>
<li>Photos to Emojis: translate images from one domain to another.</li>
<li>Photograph Editing: reconstruct photos of faces with specific specified features.</li>
<li>Face Aging: generate photos of faces with different apparent ages.</li>
<li>Photo Blending: blend elements from different photos.</li>
<li>Super Resolution: generate pictures with higher pixel resolution.</li>
<li>Photo Inpainting: filling in an area of a photo that was removed.</li>
<li>Clothing Translation: input someone with a cloth on and output the cloth in catalog</li>
<li>Video Preodiction: predict videos given a single static image (ambiguous task)</li>
</ul>
<h1 id="Training-of-GANs"><a href="#Training-of-GANs" class="headerlink" title="Training of GANs"></a>Training of GANs</h1><p>Two neural networks contest with each other in a zero-sum game.<br>The goal of generator is to <strong>fool the discriminator</strong> but not to imitate a specific image.<br>This enables the model to learn in an unsupervised manner(no specific target).<br>Unsupervised: learn the <strong>distribution function</strong> $p(x,y)$ to generate synthetic $x’$ and $y’$.<br>Supervised: model the <strong>conditional probability distriburion function(p.d.f)</strong> $p(y|x)$.</p>
<p>Other generative models:</p>
<ul>
<li>Variational Autoencoders(VAEs)</li>
<li>pixelCNN / pixelRNN</li>
<li>real NVP</li>
</ul>
<p>GANs are popular because it can <strong>generate high dimensional data</strong>.<br>Generative models can be viewed as <strong>containing more information</strong> than discriminative models and they are also used for discriminative tasks like classification.</p>
<p>Generative Adversarial Networks are composed of two models:</p>
<ul>
<li><strong>Generator</strong> $G(z,\theta_1)$: aims to generate new data similar to the expected one, maps input noise variables $z$ to the desired data space $x$.<br>Goal: <strong>Maximize $D(G(z))$</strong></li>
<li><strong>Discriminator</strong> $D(x,\theta_2)$: recognizes if an input data is <em>real</em> by outputing the probability that the data comes from the real dataset.<br>Goal: <strong>Maximize $D(x)$ and minimize $D(G(z))$</strong></li>
</ul>
<p>The competition between these two models is what improves their knowledge until Generator succeeds in creating realistic data.<br>In practice we use a log loss instead of the raw probability.</p>
<p>Discriminator and Generator are trying to optimize the opposite loss function: two agents playing a <em>minimax game</em> with value function $V(G,D)$:</p>
<script type="math/tex; mode=display">\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_{data} (x)}[logD(x)]+\mathbb{E}_{z\sim p_z(z)}[log(1-D(G(z)))]</script><p><img src="/images/nndl2/GAN_0.png" alt="GAN"><br><img src="/images/nndl2/GAN_1.png" alt="GAN"><br>The fundamental steps to train a GAN:</p>
<ol>
<li>Sample a noise set and a real-data set, each with size m</li>
<li>Train the Discriminator on this data batch <em>for several steps</em></li>
<li>Sample a different noise subset with size m</li>
<li>Train the Generator on this data batch</li>
<li>Repeat from Step 1</li>
</ol>
<h1 id="DC-GAN-Conditional-GAN-and-W-GAN"><a href="#DC-GAN-Conditional-GAN-and-W-GAN" class="headerlink" title="DC-GAN, Conditional GAN, and W-GAN"></a>DC-GAN, Conditional GAN, and W-GAN</h1><h2 id="Deep-Convolutional-Generative-Adversarial-Networks"><a href="#Deep-Convolutional-Generative-Adversarial-Networks" class="headerlink" title="Deep Convolutional Generative Adversarial Networks"></a>Deep Convolutional Generative Adversarial Networks</h2><ol>
<li>Use <strong>strided convolutions</strong> instead of deterministic spatial pooling functions(eg. max pooling), so the Discriminator can learn its own spatial downsampling and Generator can learn its own spatial upsampling.</li>
<li><strong>Eliminate fully connected layers</strong> on top of convolutional features.</li>
<li><strong>Batch Normalization</strong> helps gradients flow in deeper models, not applied to Generator’s output layer and Discriminator’s input layer to avoid sample oscillation and model instability.</li>
<li>Generator uses <strong>ReLU</strong> for all layers except for output which uses <strong>Tanh</strong>,<br>Discriminator uses <strong>LeakyReLU</strong> for all layers.</li>
</ol>
<p>Generator: <strong>No fully connected or pooling layers are used.</strong><br><img src="/images/nndl2/GAN_2.png" alt="DC-GAN"></p>
<p>Discriminator is like an inverse of Generator.<br><img src="/images/nndl2/GAN_3.png" alt="DC-GAN"></p>
<h2 id="Conditional-GANs"><a href="#Conditional-GANs" class="headerlink" title="Conditional GANs"></a>Conditional GANs</h2><p>Generators is trained to generate examples <strong>from the input domain(conditions)</strong>, so its input is combined with some additional input(eg. a class value).<br>Discriminator is also conditioned and its input image is combined with an additional input.<br><img src="/images/nndl2/GAN_4.png" alt="Conditional GANs"><br>The conditional domain allows for applications such as text-to-image or image-to-image.<br>We can perform conditioning by feeding the extra information $y$ into both the discriminator and generator as additional input layer:</p>
<script type="math/tex; mode=display">\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_{data} (x)}[logD(x|y)]+\mathbb{E}_{z\sim p_z(z)}[log(1-D(G(z|y)))]</script><p>If the generator ignores the input conditions, the discriminator will give them a low score even though the input image is realistic.<br><img src="/images/nndl2/GAN_5.png" alt="Conditional GANs"></p>
<h2 id="Wasserstein-GAN-WGAN"><a href="#Wasserstein-GAN-WGAN" class="headerlink" title="Wasserstein GAN (WGAN)"></a>Wasserstein GAN (WGAN)</h2><p>WGAN uses a critic that <strong>scores the realness or fakeness</strong> of a given image to replace the discriminator who predicts the probability as being real or fake.</p>
<p>The benefit is that the training process is more stable and less sensitive to model architecture and choice of hyperparameter and the loss of the discriminator appears to relate to the quality of images created by the generator.</p>
<p><strong>The Wasserstein Distance</strong> is the minimum cost of transporting mass in converting the real data distribution $r$ to generated data distribution $g$ , which is defined as the <strong>greatest lower bound(infimum) for any transport plan</strong>(i.e. the cost of the cheapest plan):</p>
<script type="math/tex; mode=display">W(\mathbb{P}_r,\mathbb{P}_g)=\inf_{\gamma\in\prod(\mathbb{P}_r,\mathbb{P}_g)}\mathbb{E}_{(x,y)}[||x-y||]</script><p>Using Kantorovich-Rubinstein duality, we can simplify it to:</p>
<script type="math/tex; mode=display">W(\mathbb{P}_r,\mathbb{P}_\theta)=\sup_{||f||_L\leq 1} \mathbb{E}_{x\sim \mathbb{P}_r}[f(x)] - \mathbb{E}_{x\sim \mathbb{P}_\theta}[f(x)]</script><p>$sup$ is the least upper bound and $f$ is a 1-Lipschitz function which means:</p>
<script type="math/tex; mode=display">|f(x_1)-f(x_2)|\leq |x_1-x_2|</script><p>so that the minimum $|f(x_1)-f(x_2)|$ correspond to the minimum $|x_1-x_2|$ .<br>We can build a deep network to learn the 1-Lipschitz function. The network is similar to discriminator, just without the sigmoid function and output a score to show how real the input images are rather than a probability.<br><img src="/images/nndl2/GAN_6.png" alt="WGANs"><br>$f$ has to be a 1-Lipschitz function. To enforce the constraint, WGAN applies a very simple clipping to restrict the maximum weight value in $f$ , i.e. the weights of the discriminator must be within a certain range controlled by the hyperparameters $c$.<br><img src="/images/nndl2/GAN_7.png" alt="WGANs"><br>GAN’s loss measures how well it fools the discriminator, while WGAN’s loss function reflects the <strong>image quality</strong> which is more desirable.<br><img src="/images/nndl2/GAN_8.png" alt="WGANs"><br><strong>WGAN is more stable as it can perform without batch normalization but GANs without BN collapse.</strong></p>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning 2</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Classical Planning 1 -- Agents, Environment &amp; Domain</title>
    <url>/2021/01/14/AI_Classical_Planning_1/</url>
    <content><![CDATA[<h1 id="Agents"><a href="#Agents" class="headerlink" title="Agents"></a>Agents</h1><p>Perceives its environment through sensors and acts upon the environment through actuators<br><strong>Rational agent</strong>: Acts to achieve the best (expected) outcome<br>For each possible percept sequence, a rational agent should <strong>select an action</strong> that is expected to <strong>maximize its performance measure</strong>, given the evidence provided by the <strong>percept sequence</strong> and whatever <strong>built-in knowledge</strong> the agent has.<br><a id="more"></a></p>
<h1 id="Environment"><a href="#Environment" class="headerlink" title="Environment"></a>Environment</h1><p><img src="/images/CS5446/Environment.png" alt="Environment"></p>
<h1 id="Classical-Planning"><a href="#Classical-Planning" class="headerlink" title="Classical Planning"></a>Classical Planning</h1><p>Planning: make a sequence of decisions. (choose actions and organize them)<br>Classical planning deals with <em>Fully observable, Deterministic, Finite, Discrete</em> environments.</p>
<h1 id="Domain-independent-heuristics"><a href="#Domain-independent-heuristics" class="headerlink" title="Domain independent heuristics"></a>Domain independent heuristics</h1><p>Without good heuristics, planning can become difficult very quickly as the problem size grows.<br>Problem solving with search takes a lot of time and memory.<br>Deriving heuristics for individual problems is time consuming.<br><strong>Domain independent heuristics tend to work well in planning.</strong><br>Need specific representations:</p>
<ul>
<li>Strips</li>
<li>Satisfiability</li>
<li>Planning graph </li>
</ul>
]]></content>
      <categories>
        <category>AI Planning and Decision Making</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI Planning</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Classical Planning 2 -- STRIPS VS PDDL</title>
    <url>/2021/01/21/AI_Classical_Planning_2/</url>
    <content><![CDATA[<h1 id="Planning"><a href="#Planning" class="headerlink" title="Planning"></a>Planning</h1><p>Planning means identifying appropriate actions and their sequence, it’s the basis of rational behavior.<br>We need to design intelligent machines because:</p>
<ol>
<li>the machine may be out of reach, eg. satellites</li>
<li>experts are not always available.</li>
</ol>
<p>So the machine should learn how to work rationally by itself.<br><a id="more"></a></p>
<h2 id="Types-of-planning"><a href="#Types-of-planning" class="headerlink" title="Types of planning"></a>Types of planning</h2><ul>
<li>Path and motion planning<br>Generate a path from start to end and a control trajectory along the path.</li>
<li>Perception planning<br>Sensing actions to gather information and model the environment.</li>
<li>Navigation planning<br>Motion + perception planning to generate a policy based on localization/sensor-based primitives to explore an area.</li>
<li>Manipulation planning<br>Concerned with handling objects (think automobile assembly line).</li>
</ul>
<h2 id="Solving-planning-problems"><a href="#Solving-planning-problems" class="headerlink" title="Solving planning problems"></a>Solving planning problems</h2><p><strong>Need domain independent representation and efficient solution mechanism.</strong></p>
<h1 id="STRIPS"><a href="#STRIPS" class="headerlink" title="STRIPS"></a>STRIPS</h1><p>STRIPS and PDDL both are languages to specify the problem.<br>STRIPS is one of the first representation languages for planning by Stanford.</p>
<h2 id="Factored-representation"><a href="#Factored-representation" class="headerlink" title="Factored representation"></a>Factored representation</h2><p><strong>A atomic representation</strong> is one in which each state is treated as a black box, having no internal structure: the state either does or does not match the goal.</p>
<p><strong>A factored representation</strong> is one in which the states are defined by set of features, having more internal structure. State is represented as <strong>a set of attribute/value pairs</strong>, e.g. in Strips and Pddl, we use <strong>binary variables</strong> which menas the state is either true or false.</p>
<p>In planning problems, <strong>state variables are often referred to as fluents</strong>.</p>
<h2 id="Logic-notation"><a href="#Logic-notation" class="headerlink" title="Logic notation"></a>Logic notation</h2><p><strong>Term</strong>: an object in the world. Can be a variable, constant or function<br><strong>Atomic sentence or Atom</strong>: a predicate(<em>relations</em>) symbol, optionally followed by a parenthesized list of terms, it can be true or false<br><strong>Literal</strong>: atom or its negation<br><strong>Ground literal</strong>: A literal with no variable<br><strong>Sentence or Formula</strong>: Formed from atoms together with quantifiers ($\forall$, $\exists$), logical connectives ($\wedge$, $\vee$, $\neg$), equality symbol ($=$), Sentence can take values 𝑇𝑟𝑢𝑒 or 𝐹𝑎𝑙𝑠𝑒<br><strong>Substitution</strong>: replaces variables by terms<br><strong>Unifier</strong>: takes 2 sentences and returns a substitution that makes the sentence look identical, if such a substitution exists</p>
<p>For every pair of unifiable sentences, there is a <strong>most general unifier (MGU)</strong>, which is unique up to renaming and substitution of variables<br>Let $\sigma$ be a most general unifier and $\theta$ be a unifier. After applying $\sigma$, we can find another unifier $w$ such that applying $w$ to the output of $\sigma$ gives the same effect as applying $\theta$.</p>
<h2 id="STRIPS-1"><a href="#STRIPS-1" class="headerlink" title="STRIPS"></a>STRIPS</h2><p>To specify a problem, we need 5 components:</p>
<ul>
<li>Initial state</li>
<li>Goal test</li>
<li>Set of actions</li>
<li>Transition model</li>
<li>Path cost</li>
</ul>
<p>With STRIPS, we can specify 4 of them:</p>
<ul>
<li>Initial state</li>
<li>Actions available</li>
<li>Result of applying an action in a  (Effect)</li>
<li>Goal state<br><strong>There’s no cost function in STRIPS</strong></li>
</ul>
<p><strong>State</strong> is represented by a <strong>conjunction of positive</strong> literals that are <strong>grounded and function-free</strong>. Literals not mentioned are false due to <strong>closed-world assumption</strong>. (unique name assumption)<br>Eg. 𝐴𝑡(𝐹𝑎𝑡ℎ𝑒𝑟(𝐹𝑟𝑒𝑑), 𝑆𝑦𝑑𝑛𝑒𝑦) is not allowed because STRIPS doesn’t allow functions.</p>
<p><strong>Goal</strong> is a partially specified state, represented as a conjunction of <strong>positive ground literals</strong>.<br>A state 𝑠 satisfies a goal 𝑔 if 𝑠 contains all the literals in 𝑔.</p>
<p><strong>Action Schema</strong> consists of：</p>
<ul>
<li>the action name, </li>
<li>list of variables used, </li>
<li><strong>precondition</strong>(conjunction of <strong>function-free positive literals</strong>),</li>
<li><strong>effect</strong>(conjunction of <strong>function-free (positive/negative) literals</strong>).</li>
</ul>
<p>Action 𝑎 is <strong>applicable</strong> in state 𝑠 if all precondition of 𝑎 is satisfied in 𝑠.</p>
<p>Ground actions: obtained by instantiating the variables in the schema.</p>
<p>For an action 𝑎 with 𝑣 variables in a domain with 𝑘 objects, there are $O(k^v)$ distinct ground actions in the worst case. So there’s a blow up in the number of ground actions which are difficult to evaluate.</p>
<p>Result of executing 𝑎 in state 𝑠 is a state 𝑠′ which is a set of fluents formed as follows:</p>
<ul>
<li>Start from 𝑠</li>
<li>Remove fluents that appear as negative literals in the effect (delete list: 𝐷𝐸𝐿(𝑎))</li>
<li>Add fluents that appear as positive literals in the effect (add list: 𝐴𝐷𝐷(𝑎))</li>
</ul>
<script type="math/tex; mode=display">𝑅𝐸𝑆𝑈𝐿𝑇(𝑠, 𝑎)=(𝑠 −𝐷𝐸𝐿(𝑎))\cup 𝐴𝐷𝐷(𝑎)</script><h2 id="STRIPS-summary"><a href="#STRIPS-summary" class="headerlink" title="STRIPS summary"></a>STRIPS summary</h2><p><strong>State</strong>: conjunction of ground positive function-free literals (Closed world assumption)<br><strong>Goal</strong>: conjunction of ground positive function-free literals (Satisfied by states that contain all literals)<br><strong>Action precondition</strong>: conjunction of positive function-free literals<br><strong>Action effects</strong>: conjunction of function-free literals<br>Positive literals for add list<br>Negative literals for delete list</p>
<h1 id="Planning-domain"><a href="#Planning-domain" class="headerlink" title="Planning domain"></a>Planning domain</h1><p><strong>A set of action schemas define planning domain</strong><br>Planning problem: <strong>action schema + initial state + goal</strong></p>
<h1 id="PDDL"><a href="#PDDL" class="headerlink" title="PDDL"></a>PDDL</h1><p>Strips – restrictive<br>$\surd$ Efficient to solve (∵ of restrictions)<br>$\Large\times$ Hard to describe the problem</p>
<p>So we need more expressive power.</p>
<p><strong>Action Description Language (ADL)</strong></p>
<ul>
<li>positive and negative literals in states, <em>open world assumption</em></li>
<li>quantified variables at goal, conjunctions and disjunctions at goal</li>
<li>conditional effects</li>
<li>equality predicate, typed variables</li>
</ul>
<p><strong>PDDL : ADL + extensions</strong><br><strong>Inequality predicate – not in STRIPS but available in PDDL</strong>   eg. $a\neq b$</p>
<h1 id="Solving-the-planning-problem"><a href="#Solving-the-planning-problem" class="headerlink" title="Solving the planning problem"></a>Solving the planning problem</h1><ul>
<li>Forward Search (heuristics)<br><strong>Very large state space</strong> – exponential with the number of state variables<br><strong>Action space can be very large</strong><br>Forward search is hopeless without <strong>good heuristics</strong></li>
<li>Backward Search (Research / relevant-state search)</li>
<li>or combined</li>
</ul>
]]></content>
      <categories>
        <category>AI Planning and Decision Making</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI Planning</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Classical Planning 3 -- Solving the planning problem</title>
    <url>/2021/01/28/AI_Classical_Planning_3/</url>
    <content><![CDATA[<h1 id="Forward-search-Progression-search"><a href="#Forward-search-Progression-search" class="headerlink" title="Forward search (Progression search)"></a>Forward search (Progression search)</h1><p>Forward search is not useful without heuristic.<br>From the current state, we try to apply actions and see what states we can get in this domain until we reach the goal.<br><a id="more"></a></p>
<h1 id="Backward-search-Regression-Relevant-state-search"><a href="#Backward-search-Regression-Relevant-state-search" class="headerlink" title="Backward search (Regression / Relevant-state search)"></a>Backward search (Regression / Relevant-state search)</h1><p><strong>state</strong>: conjunction of ground literals<br><strong>description</strong>: conjunction of relevant ground literals + any other fluent</p>
<ul>
<li>Subset of first-order logic</li>
<li>Easy for us to understand and specify</li>
<li>Hard to deal with</li>
<li>Hard to develop good heuristics</li>
<li>Allows formula to represent a set of propositional variables</li>
</ul>
<p><em>Description represents a set of states.</em><br>For $n$ ground fluents, there is $2^n$ ground states(true or false) and $3^n$ descriptions(true, false and NA).</p>
<p>Backward search starts from the goal states and go back.<br>It needs inverse transitions and relevant actions.</p>
<p>Forward:</p>
<script type="math/tex; mode=display">𝑔=(𝑠 −𝐷𝐸𝐿(𝑎))\cup 𝐴𝐷𝐷(𝑎)</script><p>Backward:</p>
<script type="math/tex; mode=display">𝑔′=(𝑔 −𝐴𝐷𝐷(𝑎))\cup 𝑃𝑟𝑒𝑐𝑜𝑛𝑑(𝑎)</script><p>Preconditions must have been true ⇒ add $𝑃𝑟𝑒𝑐𝑜𝑛𝑑(𝑎)$ to $𝑔′$<br>Add-list (or effects) need not be true before the action ⇒ remove $𝐴𝐷𝐷(𝑎)$<br>$𝐷𝐸𝐿(𝑎)$ disappears: it doesn’t matter if they were true or not before the action.</p>
<p><strong>Relevant actions</strong>:<br>At least one of the action’s effect must unify with an element of the current goal;<br>Must not contain effects that negate an element of the current goal.</p>
<p>Backward search can improve efficiency, has lower branching factor as compared to forward search (mostly) and does not rule out any solution.</p>
<h1 id="Heuristics-for-Planning-Relax-the-problem"><a href="#Heuristics-for-Planning-Relax-the-problem" class="headerlink" title="Heuristics for Planning (Relax the problem)"></a>Heuristics for Planning (Relax the problem)</h1><h2 id="Ignore-preconditions-heuristics"><a href="#Ignore-preconditions-heuristics" class="headerlink" title="Ignore preconditions heuristics"></a>Ignore preconditions heuristics</h2><p>Drop all the preconditions<br>Relaxation: actions applicable everywhere<br>Number of steps to solve the problem ≈ number of unsatisfied goals</p>
<ul>
<li>Further relax: remove all remove effects except goal literals<br>Need minimum number of actions such that their union satisfies the goal<br>Becomes a set-cover problem (NP-Hard)<br>Greedy algorithm – within a factor of log⁡(𝑛) factor of the optimal</li>
</ul>
<h2 id="Ignore-delete-list-heuristic"><a href="#Ignore-delete-list-heuristic" class="headerlink" title="Ignore delete list heuristic"></a>Ignore delete list heuristic</h2><p>Monotonically move towards the goal<br>Relaxation : no action will undo progress made by another action<br>Optimal length of the solution can be used as heuristic<br>Still NP-hard (searching is still exponential)<br>Hill-climbing gives approximate solution in polynomial time(not exact solution)</p>
<h2 id="Problem-decomposition"><a href="#Problem-decomposition" class="headerlink" title="Problem decomposition"></a>Problem decomposition</h2><p>If each subplan uses an admissible heuristic, take maximum of them for the plan<br>Assume subgoal independence: decompose to disjoint subsets<br>Sum of cost of each plan, if admissible, is better than max of the heuristics</p>
<p>These are heuristics because:</p>
<ol>
<li>There are factor representation</li>
<li>Use STRIPS to represent the problem</li>
</ol>
<h1 id="SATPLAN"><a href="#SATPLAN" class="headerlink" title="SATPLAN"></a>SATPLAN</h1><p>Transfer planning(search) problem to boolean satisfiability problem.<br>SAT is NP-hard, but is still effective in practice with millions of variables and constraints so it can be solved in a good(polynominal) time.</p>
<p>SAT: Finding an assignment to variables such that: boolean formula becomes TRUE<br>Input to SAT solvers: Conjunctive Normal Form (CNF) formulas</p>
<ul>
<li>CNF – conjunction of clauses</li>
<li>Clause – disjunction of literals</li>
<li>Literal – variable or its negation</li>
<li>$\neg$ should appear only on literals</li>
</ul>
<h2 id="Any-Boolean-formula-can-be-converted-to-CNF"><a href="#Any-Boolean-formula-can-be-converted-to-CNF" class="headerlink" title="Any Boolean formula can be converted to CNF"></a>Any Boolean formula can be converted to CNF</h2><p>Step1: Eliminate $\Leftrightarrow$; replace $\alpha\Leftrightarrow\beta$ with $\alpha\Rightarrow\beta\wedge\beta\Rightarrow\alpha$<br>Step2: Eliminate $\Rightarrow$; replace $\alpha\Rightarrow\beta$ with $\neg\alpha\vee\beta$<br>Step3: Move $\neg$ inwards by repeatedly applying:<br>$\qquad\neg(\neg\alpha)\equiv\alpha\quad$    Double negation elimination<br>$\qquad\neg(\alpha\wedge\beta)\equiv\neg\alpha\vee\neg\beta\quad$DeMorgan<br>$\qquad\neg(\alpha\vee\beta)\equiv\neg\alpha\wedge\neg\beta\quad$DeMorgan<br>Step4: Apply distributive law<br>Distribute $\vee$ over $\wedge$ when possible.</p>
<h2 id="Transfrom-STRIPS-to-SAT"><a href="#Transfrom-STRIPS-to-SAT" class="headerlink" title="Transfrom STRIPS to SAT"></a>Transfrom STRIPS to SAT</h2><ol>
<li>Propositionize actions<br>replace each action scheme with all ground actions.</li>
<li>Define initial state<br>Assert $F^0$ for every fluent $F$ in initial state and $\neg F^0$ for every fluent not in the initial state.</li>
<li>Propositionalize the goal<br>Construct a disjunction over <em>all possible ground conjunctions</em> obtained by replacing the variables with constants.</li>
<li>Add <strong>successor-state axiom</strong> (defining the transition)<br>Stating what is and what is not affected by the action<br>For each fluent $F$ add axiom of the form:<script type="math/tex; mode=display">F^{t+1}\Leftrightarrow ActionCausesF^t\vee(F^t\wedge \neg ActionCausesNotF^t)</script>which means F is true i.i.f. some action caused it or it was already true and no action changed it.<br>$ActionCausesF^t$ is a disjunction of all ground actions that have $F$ in their add list.<br>$ActionCausesNotF^t$ is a disjunction of all ground actions that have $F$ in their delete list.<br><strong>Frame problem</strong><br>Most actions leave most fluents unchanged.<br>Solution: write frame axioms to show nothing else changed on taking an action.<br>For $m$ actions and $n$ fluents, need $O(mn)$ axioms<br>Improvement: successor-state axiom only need $O(n)$<br>Each axiom is longer, but only involves actions that has an affect on the fluent.</li>
<li>Add <strong>precondition axioms</strong><br>If an action is taken at time $t$, its preconditions must have been true.<br>For each ground action $A$, add axiom $A^t\Rightarrow PRE(A)^t$</li>
<li>Add <strong>action exclusion axiom</strong><br>Every action is distinct from every other action (only one action is allowed at each time step, two actions cannot happen at the same time)<br>Specify mutual exclusion $\neg (A_i^t\wedge A_j^t)\Leftrightarrow\neg A_i^t\vee\neg A_j^t$</li>
<li>Create a conjunction of all the axioms (This is the problem definition)</li>
<li>Convert this conjunction to CNF</li>
</ol>
<h2 id="SATPLAN-1"><a href="#SATPLAN-1" class="headerlink" title="SATPLAN"></a>SATPLAN</h2><p>Problem: do not know how many steps<br>Solution: try values $T$ up to $T_{max}$<br>If you get a solution within $T_{max}$ then stop and output it, otherwise we do not know if there exists valid solutions.<br>After finding a solution, we extract the actions and trace back what actions have been done.<br><img src="/images/CS5446/satplan.png" alt="satplan"></p>
<h2 id="Complexity-of-planning"><a href="#Complexity-of-planning" class="headerlink" title="Complexity of planning"></a>Complexity of planning</h2><p>Worst case: intractable<br>PlanSat: answers the question is there any plan that solves a planning problem.<br>Bounded PlanSat: answers the question if there is a solution of length $k$ or less.<br><strong>Decidable for classical planning as the number of states is finite.</strong><br>PlanSat and BoundedPlanSat are <strong>PSPACE</strong> (solvable with polynomial space)</p>
]]></content>
      <categories>
        <category>AI Planning and Decision Making</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI Planning</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Planning Graph</title>
    <url>/2021/02/03/AI_Planning_Graph/</url>
    <content><![CDATA[<h1 id="Planning-Graphs"><a href="#Planning-Graphs" class="headerlink" title="Planning Graphs"></a>Planning Graphs</h1><p>Search trees grow <strong>exponentially</strong> large very fast. Depth at which the goal first appears means <strong>the minimum number of steps</strong> required to achieve the goal.<br><a id="more"></a></p>
<p>Planning graph is a <strong>polynomial</strong> sized approximation to the search tree.<br><img src="/images/CS5446/plan_graph.png" alt="Planning_graph"><br>$S_i$ contains all literals that could hold at time $i$ and $A_i$ contains all actions that could have their preconditions satisfied at time $i$.<br>The level at which a literal first appears is <strong>a good estimate of how difficult</strong> it is to achieve the literal. The graph can also estimates which set of propositions are <strong>reachable</strong> from $S_0$ with which actions.</p>
<p>Planning graphs work only on <strong>propositional planning problems</strong> – need to propositionalize PDDL action schemas.<br><strong>Propositionalize the actions</strong>: replace each action schema with a set of <strong>ground actions</strong> formed by substituting constants for each of the variables. These ground actions are not part of the translation, but will be used in subsequent steps.</p>
<h2 id="Construct-a-planning-graph"><a href="#Construct-a-planning-graph" class="headerlink" title="Construct a planning graph"></a>Construct a planning graph</h2><p><strong>Level off</strong> – the graph is constructed until two consecutive levels are identical - <strong>constructed level by level</strong></p>
<p>Search tree:</p>
<ul>
<li>one action at each level; branch on each action</li>
<li>gives true depth when goal is achieved</li>
</ul>
<p>Planning graph:</p>
<ul>
<li>all actions that have preconditions satisfied appear in parallel and produce all possible effects<br>No need to choose among actions<br>Records if some choices are impossible</li>
<li>optimistic estimate of goal depth</li>
</ul>
<p>Goal: Construct a plan where at each time step, have such a set of actions<br>Planning graph – search for plan with parallel actions OR usual plan search</p>
<p><img src="/images/CS5446/mutex_link.png" alt="mutex_link"></p>
<h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><p>For a planning graph with $l$ literals and $a$ actions,<br>Each $S_i$ has at most $l$ nodes and $O(l^2)$ mutex links</p>
<p>Each $A_i$ has at most $a+l$ nodes (including no-op), $(a+l)^2$ mutex links, and $2(al+l)$ precondition and effect links</p>
<p>Entire graph with $n$ levels has size $𝑂(n(a+l)^2)$<br>Same complexity for time to build the graph</p>
<p><img src="/images/CS5446/Heuristic.png" alt="Heuristic"></p>
<h1 id="GraphPlan-algorithm"><a href="#GraphPlan-algorithm" class="headerlink" title="GraphPlan algorithm"></a>GraphPlan algorithm</h1><p>A planning graph is a polynominal approximation of the search tree, which means it is much smaller than a search tree for the same planning problem.<br>Constructed level by level.<br>Constrained by mutex links.</p>
<p>Two-stage: limit the search space + search for solution<br>Expand the graph, if the goal is satisfied, extract the solution(similar to backward search)<br><img src="/images/CS5446/graph_plan.png" alt="graph_plan"></p>
]]></content>
      <categories>
        <category>AI Planning and Decision Making</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI Planning</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Architecture Search (NAS)</title>
    <url>/2021/02/03/DL_NAS/</url>
    <content><![CDATA[<h1 id="Why-do-we-need-neural-architecture-search"><a href="#Why-do-we-need-neural-architecture-search" class="headerlink" title="Why do we need neural architecture search?"></a>Why do we need neural architecture search?</h1><p>To get higher accuracy, the neural nets are getting deeper and more complicated.<br>It requires a lot of expert knowledge and takes lots of time and eventually will go beyond human capacity.</p>
<p>Neural Architecture Search (NAS) is a kind of Automated Machine Learning (AutoML). Basic idea is to <strong>generate some candidate children networks</strong>, train them, optimize their accuracy until successfully select the best architecture for a given task.<br><a id="more"></a><br>Different optimization methods for NAS:</p>
<ul>
<li><strong>Reinforcement Learning</strong> (Zoph &amp; Le 2017; Pham et al. 2018)</li>
<li>Sequential model-based method (Liu et al. 2018)</li>
<li>Evolutionary method (Liu et al. 2018)</li>
<li>Bayesian optimization (Jin et al. 2018)</li>
<li>Gradient-based model (Liu et al. 2019)</li>
</ul>
<h1 id="Vanilla-NAS-—-using-RL-and-RNN-to-search-an-architecture"><a href="#Vanilla-NAS-—-using-RL-and-RNN-to-search-an-architecture" class="headerlink" title="Vanilla NAS — using RL and RNN to search an architecture"></a>Vanilla NAS — using RL and RNN to search an architecture</h1><h2 id="Generate-CNN-using-RNN"><a href="#Generate-CNN-using-RNN" class="headerlink" title="Generate CNN using RNN"></a>Generate CNN using RNN</h2><h3 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h3><p>The structure and connectivity of a neural network can be specified by a<br>variable-length string which can be generated by a RNN(the controller).<br>Training the network specified by the string – the “child network” –<br>on the real data will result in an accuracy on a validation set.<br>Using this accuracy as the reward signal, we can <strong>compute the policy gradient to update the controller</strong>.<br>In the next iteration, the controller will give higher probabilities to architectures that receive high valid accuracies.<br><img src="/images/nndl2/NAS.png" alt="NAS"></p>
<h3 id="Controller-—-RNN"><a href="#Controller-—-RNN" class="headerlink" title="Controller — RNN"></a>Controller — RNN</h3><p>Use the controller(RNN) to generate their hyper-parameters as a sequence of tokens. It predicts filter height, filter width, stride height, stride width, and number of filters for one layer and repeats. Every prediction is carried out by a softmax classifier and then fed into the next time-step as input.<br>The process of generating an architecture stops if the number of layers is larger than a certain value which is increased in the training process.<br><strong>The parameters of the controller RNN, $\theta_c$, are optimized to maximize the expected validation accuracy of the proposed architectures</strong><br><img src="/images/nndl2/NAS_RNN.png" alt="NAS_RNN"><br>This process can be viewed as reinforcement learning:<br>The list of tokens that the controller predicts can be viewed as a list of actions $a_{1∶T}$ to design an architecture for a child network and the valid accuracy of a given dataset is the feedback $R$.<br>The goal of the controller is to maximize its expected reward $J(\theta_c)$.<br><img src="/images/nndl2/NAS_reward.png" alt="NAS_reward"><br><img src="/images/nndl2/NAS_R.png" alt="NAS_R"><br>The baseline b is an exponential moving average of the previous architecture accuracies and does not depend on the current action. (Similar to Adam)</p>
<h3 id="Generate-architecture-with-Skip-Connections"><a href="#Generate-architecture-with-Skip-Connections" class="headerlink" title="Generate architecture with Skip Connections"></a>Generate architecture with Skip Connections</h3><p><strong>Set-selection type attention</strong><br>At layer N, we add an anchor point which has <strong>N − 1 content-based sigmoids to indicate the previous layers that need to be connected</strong>.<br>Each sigmoid is a function of current hidden-state of the controller and previous hidden-states of the previous N − 1 anchor points.<br><img src="/images/nndl2/attention.png" alt="attention"><br><img src="/images/nndl2/attention_1.png" alt="attention"></p>
<p>Failures:</p>
<ul>
<li>One layer is not compatible with another layer</li>
<li>One layer may not have any input or output</li>
</ul>
<p>Solutions:</p>
<ul>
<li>Pad the smaller layer with zeros.</li>
<li>If no input layer, take the original data as input.</li>
<li>Concat all the layers where output is not connected and send this hidden state to the classifier.</li>
</ul>
<h2 id="Generate-RNN-using-RNN"><a href="#Generate-RNN-using-RNN" class="headerlink" title="Generate RNN using RNN"></a>Generate RNN using RNN</h2><p><img src="/images/nndl2/RNN_1.png" alt="RNN_1"><br><img src="/images/nndl2/RNN_2.png" alt="RNN_2"></p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>In experiments of autoML, we still need human knowledge such as the filter height, number of filters and initialazition of parameters to design the model.<br>Reward for updating the contoller is the maximum validation accuracy of the last 5 epochs(total 50) of one generated model.</p>
<h3 id="CNN-for-Cifar10"><a href="#CNN-for-Cifar10" class="headerlink" title="CNN for Cifar10"></a>CNN for Cifar10</h3><p>Accuracy by human of denseNet is a little higher than NAS. We wish to minimize the parameters to make the model cheap enough to set into a mobile device. NAS can generate small models with reasonable accuracy.</p>
<h3 id="RNN-for-Penn-Tree-Bank-dataset"><a href="#RNN-for-Penn-Tree-Bank-dataset" class="headerlink" title="RNN for Penn Tree-Bank dataset"></a>RNN for Penn Tree-Bank dataset</h3><p>LR for controller is 0.0005.<br>The number of input pairs to the RNN cell is called the “<strong>base number</strong>“ and set to 8 in our experiments(much more complex than 2 in the above example).<br>Every child model is constructed and trained for 35 epochs.<br>Every child model has two layers, with the number of hidden units adjusted so that total number of learnable parameters roughly match the “medium” baselines.<br>The reward function is $\frac{c}{(validation\quad perplexity)^2}$.<br>When the base number is 8, the search space has approximately $6×10^{16}$ architectures, it is too expensive to search them all.</p>
<p>Goal: keep a medium size model<br>NAS works well in this task to have both less parameters and lower test perplexity.</p>
<h3 id="Policy-Gradient-vs-Random-Search"><a href="#Policy-Gradient-vs-Random-Search" class="headerlink" title="Policy Gradient vs Random Search"></a>Policy Gradient vs Random Search</h3><p><img src="/images/nndl2/vs_random.png" alt="vs_random"></p>
<h1 id="NASNet-searching-a-cell-rather-than-an-architecture"><a href="#NASNet-searching-a-cell-rather-than-an-architecture" class="headerlink" title="NASNet: searching a cell rather than an architecture"></a>NASNet: searching a cell rather than an architecture</h1><p>Limitations of vanilla NAS:</p>
<ul>
<li>Computationally expensive even for small datasets (e.g. Cifar-10)</li>
<li>No transferable between datasets (from small to large?)</li>
</ul>
<p>People observe that successful models are repeating special patterns. So instead of searching the whole architecture, we can search for efficient patterns.</p>
<h2 id="Concepts-and-Method"><a href="#Concepts-and-Method" class="headerlink" title="Concepts and Method"></a>Concepts and Method</h2><p>It may be possible for the controller to predict a general convolutional cell expressed in terms of motifs. The cell can then be stacked in series to handle inputs of arbitrary spatial dimensions and filter depth.<br>The overall architectures of the convolutional nets are manually predetermined.</p>
<ul>
<li>They are composed of convolutional cells repeated many times</li>
<li>Each cell has the same architecture, but different weights</li>
</ul>
<p>Two type of cells to easily build scalable architectures for images of any size:<br><strong>Normal Cell</strong>: returning a feature map of the same dimension.<br><strong>Reduction Cell</strong>: returning a feature map where the feature map’s<br>height and width is reduced by a factor of two.<br>Researchers empirically found it beneficial to learn two separate architectures for normal cell and reduction cell.<br><img src="/images/nndl2/NASNet.png" alt="NASNet"><br>We use a common heuristic to <strong>double the number of filters in the output</strong> whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension.<br>We consider the number of motif repeating times N and the number of initial convolutional filters as free parameters.</p>
<p>Now the question is: how to use a controller to generate a block of a cell.<br>So we first build a block, and combine several blocks as a cell, then use the cell to build the architecture.<br><img src="/images/nndl2/NASNet_block.png" alt="NASNet_block"><br>To allow the controller to predict both Normal Cell and Reduction Cell, we simply make the controller have 2 × 5B predictions in total.<br>The first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell.<br><img src="/images/nndl2/NAS34.png" alt="NAS34"><br><img src="/images/nndl2/NAS5.png" alt="NAS5"><br><img src="/images/nndl2/NASNetA.png" alt="NASNetA"></p>
<h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><p><img src="/images/nndl2/NASNet_random.png" alt="NASNet_random"></p>
]]></content>
      <categories>
        <category>Neural Network and Deep Learning 2</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>Decision Theory 1</title>
    <url>/2021/02/03/AI_Decision_Theory_1/</url>
    <content><![CDATA[<h1 id="Uncertainty"><a href="#Uncertainty" class="headerlink" title="Uncertainty"></a>Uncertainty</h1><p>The real environment is full of uncertainty.<br>Partial observability(traffic) + non-determinism(car break down)</p>
<h1 id="Utility-theory"><a href="#Utility-theory" class="headerlink" title="Utility theory"></a>Utility theory</h1><p>“How agent can make a rational decision in a random environment?”<br>Question for machine is what a rational decision is and how to make rational decisions.<br><a id="more"></a></p>
<p><strong>Axiomatic approach</strong><br>We say something about the environment and assert they are always true.<br>To define the axioms: start with <strong>preferences</strong> between outcomes $A$ and $B$. Based on the preferences, agents can make a choice.<br>$A&gt;B$: agent prefers $A$ over $B$ (Partial order)<br>$A\sim B$: agent is indifferent between $A$ and $B$<br>$A\ge B$: agent prefers $A$ over $B$ or is indifferent</p>
<p><strong>Utility – consequence of the preference</strong><br>A function $U$, for any $A,B\in S$ (outcomes in one domain):</p>
<script type="math/tex; mode=display">U(A)>U(B)\Leftrightarrow A>B</script><script type="math/tex; mode=display">U(A)=U(B)\Leftrightarrow A\sim B</script><p>Function $U$ is not unique, any monotonically increasing transformation will <strong>preserve the preference relation</strong>. </p>
<p><strong>Lottery</strong> is to model the state of <em>chance</em>.<br>A lottery $L$ with outcomes $S_1,S_2,…,S_n$ that occur with probabilities $p_1,p_2,…,p_n$ is denoted as:</p>
<script type="math/tex; mode=display">L=[p_1,S_1;p_2,S_2;...p_n,S_n]</script><p>Each of the outcome $S_i$ can be an atomic state or another lottery(another probability).</p>
<h1 id="Axioms-of-Utility-theory"><a href="#Axioms-of-Utility-theory" class="headerlink" title="Axioms of Utility theory"></a>Axioms of Utility theory</h1><h2 id="Axiom-1-Orderability-or-Completeness"><a href="#Axiom-1-Orderability-or-Completeness" class="headerlink" title="Axiom 1: Orderability or Completeness"></a>Axiom 1: Orderability or Completeness</h2><p>Given any 2 outcomes $A,B\in S$, exactly one of the following holds:<br>$𝐴&gt;B$, $B&gt;A$ and $A\sim B$. </p>
<h2 id="Axiom-2-Transitivity"><a href="#Axiom-2-Transitivity" class="headerlink" title="Axiom 2: Transitivity"></a>Axiom 2: Transitivity</h2><p>If the agent prefers $A$ to $B$ and $B$ to $C$, then the agent must prefer $A$ to $C$(Similarly for $\sim$):</p>
<script type="math/tex; mode=display">(A>B)\wedge (B>C)\Rightarrow A>C</script><script type="math/tex; mode=display">(A\sim B)\wedge (B\sim C)\Rightarrow A\sim C</script><p><em>If the order is circle, the transitivity is broken, and the agent is behaving irrationally.</em><br><em>Group preference may be non-transitive, studied in social choice theory</em></p>
<h2 id="Axiom-3-Continuity"><a href="#Axiom-3-Continuity" class="headerlink" title="Axiom 3: Continuity"></a>Axiom 3: Continuity</h2><p>If $B$ is in between $A$ and $C$ in preference, there must be a probability $p$, such that the agent is indifferent to:</p>
<script type="math/tex; mode=display">A>B>C\Rightarrow\exists p [p,A;(1-p),C]\sim B</script><h2 id="Axiom-4-Substitutability"><a href="#Axiom-4-Substitutability" class="headerlink" title="Axiom 4: Substitutability"></a>Axiom 4: Substitutability</h2><p>If an agent is indifferent between two lotteries $A$ and $B$, the agent is indifferent to two complex lotteries that are the same, except $B$ is substituted for $A$ in one of the (Similarly for preference):</p>
<script type="math/tex; mode=display">A\sim B\Rightarrow [p,A;(1-p),C]\sim [p,B;(1-p),C]</script><h2 id="Axiom-5-Monotonicity"><a href="#Axiom-5-Monotonicity" class="headerlink" title="Axiom 5: Monotonicity"></a>Axiom 5: Monotonicity</h2><p>Prefer higher probability of getting prefered outcome:</p>
<script type="math/tex; mode=display">A>B\Rightarrow ((p>q)\Leftrightarrow [p,A;(1-p),B]>[q,A;(1-q),B])</script><h2 id="Axiom-6-Decomposability"><a href="#Axiom-6-Decomposability" class="headerlink" title="Axiom 6: Decomposability"></a>Axiom 6: Decomposability</h2><p>Compound lotteries can be reduced to simpler ones using laws of probability:</p>
<script type="math/tex; mode=display">[p,A;(1-p),[q,B;(1-q),C]]\sim [p,A;(1-p)q,B;(1-p)(1-q)C]</script><p><img src="/images/CS5446/Decomposability.png" alt="Decomposability"></p>
<h1 id="Axioms-to-consequence"><a href="#Axioms-to-consequence" class="headerlink" title="Axioms to consequence"></a>Axioms to consequence</h1><ol>
<li><p>Existence of Utility function<br>If agent’s preferences obey the axioms of utility, then there exists a function 𝑈 such that, for any two lotteries 𝐴 and 𝐵:</p>
<script type="math/tex; mode=display">U(A)>U(B)\Leftrightarrow A>B</script><script type="math/tex; mode=display">U(A)\sim U(B)\Leftrightarrow A\sim B</script></li>
<li><p>Expected Utility of a lottery<br>The utility of a lottery is the expected value of the utilities of the outcomes:</p>
<script type="math/tex; mode=display">U([p_1,S_1;...;p_n,S_n])=\sum_ip_iU(S_i)</script></li>
<li><p>Acting Rationally<br>The agent acts rationally, i.f.f. it chooses the action that <strong>maximizes the expected utility</strong>.<br>Agent’s behavior doesn’t change if $U$ is subjected to an <em>affine transformation</em>: $U’(s)=aU(s)+b$ with $a&gt;0$.</p>
</li>
</ol>
<p><em>An affine transformation is any transformation that preserves collinearity (i.e., all points lying on a line initially still lie on a line after transformation) and ratios of distances (e.g., the midpoint of a line segment remains the midpoint after transformation).</em></p>
<h1 id="Rational-agent"><a href="#Rational-agent" class="headerlink" title="Rational agent"></a>Rational agent</h1><p><strong>Maximum Expected Utility</strong>: Rational agent should choose the action that maximized its expected utility:</p>
<script type="math/tex; mode=display">\arg\max_a EU(a|e)=\arg\max_a \sum_{s'}P(Result(a)=s'|a,e)U(s')</script><p>Human Irrationality:<br>Decision theory is normative – describe how ration agent should act.<br>Descriptive theory – describe how humans actually act.</p>
]]></content>
      <categories>
        <category>AI Planning and Decision Making</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI Planning</tag>
      </tags>
  </entry>
  <entry>
    <title>Decision Theory 2</title>
    <url>/2021/02/10/AI_Decision_Theory_2/</url>
    <content><![CDATA[<h1 id="Decision-Networks"><a href="#Decision-Networks" class="headerlink" title="Decision Networks"></a>Decision Networks</h1><p>Essentially, decision networks are extensions of Bayesian networks.<br>Bayesian networks decrease the number of variables leading to the result.<br>Decision Networks (aka. influence diagrams) is to combine BN with <strong>action and utility nodes</strong> to figure out how to get a specific result.<br><a id="more"></a><br><img src="/images/CS5446/DecisionNetwork.png" alt="Decision Network"><br>A simplified form is to eliminate chance nodes that represent outcome state, which means that action node and current state are directly connected to the utility node.<br>Utility node represents the expected utility associated with each action and is then associated with the action-utility function or 𝑄-function(Q: quality of action)<br>In this lecture we view Probability Inference(refer to Uncertainty in AI) as a black box and use tool to solve it.</p>
<h1 id="Value-of-information"><a href="#Value-of-information" class="headerlink" title="Value of information"></a>Value of information</h1><p>Assume exact evidence can be obtained about variable $E_j$;<br>To compute <strong>value of perfect information (VPI)</strong>:</p>
<ol>
<li>Given current evidence $e$, expected utility with current best action $a$:<script type="math/tex; mode=display">EU(a|e)=\max_a\sum_{s'}P(Result(a)=s'|a,e)U(s')</script></li>
<li>Value of the best new action after $E_j=e_j$ is obtained:<script type="math/tex; mode=display">EU(a_{e_j}|e,e_j)=\max_a\sum_{s'}P(Result(a)=s'|a,e,e_j)U(s')</script></li>
<li>Variable $E_j$ can take multiple values $e_{jk}$, so on averaging:<script type="math/tex; mode=display">VPI_e(E_j)=\sum_k P(E_j=e_{jk}|e)EU(a_{e_{jk}}|e,e_{jk})-EU(a|e)</script></li>
</ol>
<p>Proporties:</p>
<ul>
<li><p>Expected value of information is always non-negative:</p>
<script type="math/tex; mode=display">\forall e,e_j\quad VPI_e(E_j)\geq 0</script></li>
<li><p>VPI is not additive:</p>
<script type="math/tex; mode=display">VPI_e(E_j,E_k)\neq VPI_e(E_j) + VPI_e(E_k)</script></li>
<li><p>VPI is order independent:</p>
<script type="math/tex; mode=display">VPI_e(E_j,E_k)= VPI_e(E_j) + VPI_{e,e_j}(E_k)= VPI_e(E_k) + VPI_{e,e_k}(E_j)</script></li>
</ul>
<h1 id="Design-a-new-agent"><a href="#Design-a-new-agent" class="headerlink" title="Design a new agent"></a>Design a new agent</h1><p>Agent should gather information before taking actions, if possible.<br><img src="/images/CS5446/ChooseInfo.png" alt="Choose Infomation"></p>
]]></content>
      <categories>
        <category>AI Planning and Decision Making</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI Planning</tag>
      </tags>
  </entry>
  <entry>
    <title>Markov Decision Process</title>
    <url>/2021/02/17/AI_Markov_Decision_Processes/</url>
    <content><![CDATA[<h1 id="Sequential-decision-problems"><a href="#Sequential-decision-problems" class="headerlink" title="Sequential decision problems"></a>Sequential decision problems</h1><p>Deals with fully observable and partially observable cases.<br>Markovian assumption: Probability depends only on current state $s$ not the history of earlier states.<br>Markov decision process (MDP): A sequential decision problem for a fully observable stochastic environment with Markovian transition and additive rewards.</p>
<a id="more"></a>
<p>MDP: $(S, A, T, R)$<br>A set of states, $S$, with initial state $s_0$<br>A set of actions, $A$<br>A transition model, $T$, defined by $P(s’|s,a)$, outcomes of actions are stochastic.<br>A reward function, $R(s)$ (sometimes, more generally: $R(s,a,a’)$, Utility function depends on sequence of states while each state has a corresponding reward 𝑅(𝑠)</p>
<p>Policy($\pi (s)$) is a solution to MDP(<em>a function from state to action</em>) and <strong>is not unique</strong>. It is a mapping given whatever state in a system and output appropriate actions.<br>The difference is how good these policies are(quality of the policy): measured by the <strong>expected return</strong>(Expected utility of possible state sequence generated by the policy).<br>Optimal policy $\pi^{*}$: policy that generates highest expected utility.</p>
<h1 id="Ways-to-evaluate"><a href="#Ways-to-evaluate" class="headerlink" title="Ways to evaluate"></a>Ways to evaluate</h1><p><strong>Finite horizon</strong>: </p>
<ul>
<li>fixed time $N$ and terminate,</li>
<li>return is usually the addition of rewards over the sequence:<script type="math/tex; mode=display">U_h([s_0,s_1,...,s_N])=R(s_0)+R(s_1)+...+R(s_N)</script></li>
<li>optimal action at a state can change depending on how many steps remain,</li>
<li>optimal policy is <strong>non-stationary</strong>.</li>
</ul>
<p><strong>Infinite horizon</strong>:</p>
<ul>
<li>no fixed time limit,</li>
<li>optimal action depends only current state: <strong>stationary</strong>.</li>
<li>comparing utilities are difficult: utilities can be infinite.</li>
<li><strong>discounted rewards</strong>: (bounded by $\pm R_{max}$)<script type="math/tex; mode=display">U_h([s_0,s_1,...,s_N])=R(s_0)+\gamma R(s_1)+\gamma^2R(s_2)+...</script><script type="math/tex; mode=display">U_h([s_0,s_1,...,s_N])=\sum_{t=1}^{\infty}\gamma^tR(s_t)\leq\sum_{t=1}^{\infty}\gamma^tR_{max}=\frac{R_{max}}{1-\gamma}</script></li>
<li><strong>Expected utility</strong> of executing $\pi$ starting from $s$:<script type="math/tex; mode=display">U^{\pi}(s)=E[\sum_{t=1}^{\infty}\gamma^tR_{t}]</script>Optimal policy:<script type="math/tex; mode=display">\pi_s^{*}=\arg\max_{\pi}U^{\pi}(s)</script>Number of policies $\pi$ is finite if the number of states is finite.<br>$n$ states and $a$ actions lead to $a^n$ policies.</li>
</ul>
<h1 id="Value-function"><a href="#Value-function" class="headerlink" title="Value function"></a>Value function</h1><p><strong>Value of optimal policy is called value function $U^{\pi^{*}}(s)$</strong><br>Given the value function, we can compute an optimal policy as:</p>
<script type="math/tex; mode=display">\pi^{*}(s)=\arg\max_{a\in A(s)}\sum_{s'}P(s'|s,a)U(s')</script><h1 id="Value-iteration"><a href="#Value-iteration" class="headerlink" title="Value iteration"></a>Value iteration</h1><p>What if the agent performs suboptimally on purpose? Is the agent still rational? (eg. taking longer route to reduce the probability of utility loss)<br><strong>State variables</strong>: factorced representation, each variable can have various values;<br><strong>State</strong>: atomic representation, each variable is set to be a specific value. A state is an assignment of state variable.</p>
<p>Fixed point algorithm: run the algorithms many times and it will finally converge to a fixed result.</p>
<p>If apply an operator to a function, it will be updated to another function.</p>
<p>The algorithm comes to an end when the utility function no longer update.</p>
<p>All MDP can be solved by value iteration in theory, but in practice if there are too many states it cannot be solved.</p>
<h1 id="Policy-iteration"><a href="#Policy-iteration" class="headerlink" title="Policy iteration"></a>Policy iteration</h1><p>Compared to Bellman: no $max$ — linear equations</p>
<p>“one-step look ahead” to update the policy’s action</p>
<h1 id="Improving-policy-iteration"><a href="#Improving-policy-iteration" class="headerlink" title="Improving policy iteration"></a>Improving policy iteration</h1><h2 id="Modified-policy-iteration"><a href="#Modified-policy-iteration" class="headerlink" title="Modified policy iteration"></a>Modified policy iteration</h2><p>Do only $k$ iterations instead of going to convergence.<br>Evaluate approximately.</p>
<h2 id="Asynchronous-policy-iteration"><a href="#Asynchronous-policy-iteration" class="headerlink" title="Asynchronous policy iteration"></a>Asynchronous policy iteration</h2><p>Pick only a subset of states for either policy improvement or for updating in policy evaluation.<br>Converges as long as we continuously update all states.</p>
<h2 id="Generalized-policy-iteration-a-general-scheme-of-solution"><a href="#Generalized-policy-iteration-a-general-scheme-of-solution" class="headerlink" title="Generalized policy iteration (a general scheme of solution)"></a>Generalized policy iteration (a general scheme of solution)</h2><p>Update value according to policy, and improve policy WRT the value function.<br>VI, PI, Async-PI are all special cases of GPI.</p>
<h1 id="Online-search"><a href="#Online-search" class="headerlink" title="Online search"></a>Online search</h1>]]></content>
      <categories>
        <category>AI Planning and Decision Making</category>
      </categories>
      <tags>
        <tag>NUS note</tag>
        <tag>AI Planning</tag>
      </tags>
  </entry>
  <entry>
    <title>Student Performance Analysis</title>
    <url>/2021/02/12/Info_Vis_A0/</url>
    <content><![CDATA[<p>This Student Performance dataset includes scores from three exams and a variety of personal, social, and economic factors that have interaction effects upon them.<br>Data source: <a href="http://roycekimmons.com/tools/generated_data/exams">http://roycekimmons.com/tools/generated_data/exams</a><br><a id="more"></a><br>Data samples:<br><img src="/images/CS5346/StudentPerformance/table_sample.png" alt="table_sample.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Bar</span><br><span class="line"></span><br><span class="line">color = sns.color_palette()</span><br><span class="line">SP = pd.read_csv(<span class="string">"StudentsPerformance.csv"</span>)</span><br></pre></td></tr></table></figure>
<p>Purpose of visualization:</p>
<h1 id="How-effective-is-the-test-preparation-course"><a href="#How-effective-is-the-test-preparation-course" class="headerlink" title="How effective is the test preparation course?"></a>How effective is the test preparation course?</h1><p>(x, y) represents the proportion(y) of the number of people with this score(x) to the total number of people with/without test preparation.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prep_none = SP.loc[SP[<span class="string">'test preparation course'</span>] == <span class="string">'none'</span>]</span><br><span class="line">prep = SP.loc[SP[<span class="string">'test preparation course'</span>] == <span class="string">'completed'</span>]</span><br><span class="line"></span><br><span class="line">bar0 = Bar(<span class="string">"Test Preparation Efficiency"</span>, <span class="string">"Math"</span>)</span><br><span class="line">bar0.add(<span class="string">"none"</span>, prep_none[<span class="string">'math score'</span>].value_counts().sort_index().index,</span><br><span class="line">         prep_none[<span class="string">'math score'</span>].value_counts().sort_index().values / len(prep_none[<span class="string">'math score'</span>]))</span><br><span class="line">bar0.add(<span class="string">"completed"</span>, prep[<span class="string">'math score'</span>].value_counts().sort_index().index,</span><br><span class="line">         prep[<span class="string">'math score'</span>].value_counts().sort_index().values / len(prep[<span class="string">'math score'</span>]))</span><br><span class="line">bar0.render(<span class="string">"1-prep2math.html"</span>)</span><br></pre></td></tr></table></figure><br><img src="/images/CS5346/StudentPerformance/1_Test_math.png" alt="1_Test_math.png"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bar1 = Bar(<span class="string">"Test Preparation Efficiency"</span>, <span class="string">"Reading"</span>)</span><br><span class="line">bar1.add(<span class="string">"none"</span>, prep_none[<span class="string">'reading score'</span>].value_counts().sort_index().index,</span><br><span class="line">         prep_none[<span class="string">'reading score'</span>].value_counts().sort_index().values / len(prep_none[<span class="string">'reading score'</span>]))</span><br><span class="line">bar1.add(<span class="string">"completed"</span>, prep[<span class="string">'reading score'</span>].value_counts().sort_index().index,</span><br><span class="line">         prep[<span class="string">'reading score'</span>].value_counts().sort_index().values / len(prep[<span class="string">'reading score'</span>]))</span><br><span class="line">bar1.render(<span class="string">"1-prep2reading.html"</span>)</span><br></pre></td></tr></table></figure><br><img src="/images/CS5346/StudentPerformance/1_Test_reading.png" alt="1_Test_reading.png"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bar2 = Bar(<span class="string">"Test Preparation Efficiency"</span>, <span class="string">"Writing"</span>)</span><br><span class="line">bar2.add(<span class="string">"none"</span>, prep_none[<span class="string">'writing score'</span>].value_counts().sort_index().index,</span><br><span class="line">         prep_none[<span class="string">'writing score'</span>].value_counts().sort_index().values / len(prep_none[<span class="string">'writing score'</span>]))</span><br><span class="line">bar2.add(<span class="string">"completed"</span>, prep[<span class="string">'writing score'</span>].value_counts().sort_index().index,</span><br><span class="line">         prep[<span class="string">'writing score'</span>].value_counts().sort_index().values / len(prep[<span class="string">'writing score'</span>]))</span><br><span class="line">bar2.render(<span class="string">"1-prep2writing.html"</span>)</span><br></pre></td></tr></table></figure><br><img src="/images/CS5346/StudentPerformance/1_Test_writing.png" alt="1_Test_writing.png"></p>
<p>These histograms show that test preparation courses do not play a big role in overall scores, but they do help people to get high scores(100) in reading and writing. </p>
<h1 id="What-is-the-distribution-of-parent-level-of-education-and-how-they-contribute-to-students’-scores"><a href="#What-is-the-distribution-of-parent-level-of-education-and-how-they-contribute-to-students’-scores" class="headerlink" title="What is the distribution of parent level of education and how they contribute to students’ scores?"></a>What is the distribution of parent level of education and how they contribute to students’ scores?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">15</span>, <span class="number">9</span>)</span><br><span class="line">sns.countplot(SP[<span class="string">'parental level of education'</span>], palette=<span class="string">'RdPu'</span>)</span><br><span class="line">plt.title(<span class="string">'Comparison of Parental Education'</span>, fontweight=<span class="number">30</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Degree'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'count'</span>)</span><br><span class="line">plt.savefig(<span class="string">"2-EducationLevel.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br></pre></td></tr></table></figure>
<p><img src="/images/CS5346/StudentPerformance/2-EducationLevel.png" alt="EducationLevel.png"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getgrade</span><span class="params">(score)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> score &gt;= <span class="number">90</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'&gt;=90'</span></span><br><span class="line">    <span class="keyword">if</span> score &gt;= <span class="number">80</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'80~90'</span></span><br><span class="line">    <span class="keyword">if</span> score &gt;= <span class="number">70</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'70~80'</span></span><br><span class="line">    <span class="keyword">if</span> score &gt;= <span class="number">60</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'60~70'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Fail'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sns.countplot(x=SP[<span class="string">'parental level of education'</span>], data=SP, hue=SP.apply(<span class="keyword">lambda</span> x: getgrade(x[<span class="string">'math score'</span>]), axis=<span class="number">1</span>),</span><br><span class="line">              palette=<span class="string">'Reds'</span>)</span><br><span class="line">plt.title(<span class="string">'Parent Education vs Math Scores'</span>, fontsize=<span class="number">20</span>, fontweight=<span class="number">30</span>)</span><br><span class="line">plt.savefig(<span class="string">"2-Education2Math.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">sns.countplot(x=SP[<span class="string">'parental level of education'</span>], data=SP,</span><br><span class="line">              hue=SP.apply(<span class="keyword">lambda</span> x: getgrade(x[<span class="string">'reading score'</span>]), axis=<span class="number">1</span>), palette=<span class="string">'Blues'</span>)</span><br><span class="line">plt.title(<span class="string">'Parent Education vs Reading Scores'</span>, fontsize=<span class="number">20</span>, fontweight=<span class="number">30</span>)</span><br><span class="line">plt.savefig(<span class="string">"2-Education2Reading.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">sns.countplot(x=SP[<span class="string">'parental level of education'</span>], data=SP,</span><br><span class="line">              hue=SP.apply(<span class="keyword">lambda</span> x: getgrade(x[<span class="string">'writing score'</span>]), axis=<span class="number">1</span>), palette=<span class="string">'Set2'</span>)</span><br><span class="line">plt.title(<span class="string">'Parent Education vs Writing Scores'</span>, fontsize=<span class="number">20</span>, fontweight=<span class="number">30</span>)</span><br><span class="line">plt.savefig(<span class="string">"2-Education2Writing.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br></pre></td></tr></table></figure><br><img src="/images/CS5346/StudentPerformance/2-Education2Math.png" alt="Education2Math.png"><br><img src="/images/CS5346/StudentPerformance/2-Education2Reading.png" alt="Education2Reading.png"><br><img src="/images/CS5346/StudentPerformance/2-Education2Writing.png" alt="Education2Writing.png"><br>Most of parents have degree from college or high school, only a few parents have bachelor’s or master’s degrees. Students whose parents have bachelor’s or master’s degrees are less likely to fail the class.</p>
<h1 id="What-are-the-statistical-characteristics-of-each-subject’s-scores-and-the-distribution-of-total-score"><a href="#What-are-the-statistical-characteristics-of-each-subject’s-scores-and-the-distribution-of-total-score" class="headerlink" title="What are the statistical characteristics of each subject’s scores and the distribution of total score?"></a>What are the statistical characteristics of each subject’s scores and the distribution of total score?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">15</span>, <span class="number">9</span>)</span><br><span class="line">colorDic = dict(boxes=<span class="string">'DarkGreen'</span>, whiskers=<span class="string">'DarkOrange'</span>, medians=<span class="string">'DarkBlue'</span>, caps=<span class="string">'Gray'</span>)</span><br><span class="line">SP.plot(kind=<span class="string">'box'</span>, color=colorDic, sym=<span class="string">'r+'</span>, vert=<span class="literal">False</span>)</span><br><span class="line">plt.title(<span class="string">'Comparison of Scores'</span>, fontweight=<span class="number">30</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.savefig(<span class="string">"3-Scores_box.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br></pre></td></tr></table></figure>
<p><img src="/images/CS5346/StudentPerformance/3-Scores_box.png" alt="Education2Writing.png"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">15</span>, <span class="number">9</span>)</span><br><span class="line">SP.hist([<span class="string">'math score'</span>, <span class="string">'reading score'</span>, <span class="string">'writing score'</span>], ec=<span class="string">'black'</span>, grid=<span class="literal">False</span>, color=<span class="string">'slategrey'</span>)</span><br><span class="line">plt.savefig(<span class="string">"3-Scores_hist.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br></pre></td></tr></table></figure><br><img src="/images/CS5346/StudentPerformance/3-Scores_hist.png" alt="Education2Writing.png"><br>The highest scores in three subjects are 100. Median of writing and reading is around 72 while median of math score is around 66. 25% - 75% scores are in range of 56/58 to 78/80.<br>The distribution of each subject scores can be approximated to the normal distribution.</p>
<h1 id="What-is-the-hidden-relationship-among-students’-performance-in-various-subjects"><a href="#What-is-the-hidden-relationship-among-students’-performance-in-various-subjects" class="headerlink" title="What is the hidden relationship among students’ performance in various subjects?"></a>What is the hidden relationship among students’ performance in various subjects?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">15</span>, <span class="number">9</span>)</span><br><span class="line">SP.plot.scatter(x=<span class="string">'math score'</span>, y=<span class="string">'reading score'</span>, c=<span class="string">'writing score'</span>, colormap=<span class="string">'viridis'</span>)</span><br><span class="line">plt.savefig(<span class="string">"4-Relationship.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br></pre></td></tr></table></figure>
<p><img src="/images/CS5346/StudentPerformance/4-Relationship.png" alt="Relation.png"><br>We can see from the scatter plot combined with heat map that the performance of each student in the three courses is very similar, and there is no obvious biased student. Most students pass all three courses. Students with high reading scores also have high writing scores, but their math scores are not necessarily as high. There are more people with a score of 80 or higher in reading and writing than in math.  </p>
<h1 id="How-does-gender-and-race-influence-to-each-subject"><a href="#How-does-gender-and-race-influence-to-each-subject" class="headerlink" title="How does gender and race influence to each subject?"></a>How does gender and race influence to each subject?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.countplot(x=SP[<span class="string">'gender'</span>], data=SP,</span><br><span class="line">              hue=SP.apply(<span class="keyword">lambda</span> x: getgrade((x[<span class="string">'math score'</span>] + x[<span class="string">'reading score'</span>] + x[<span class="string">'writing score'</span>]) / <span class="number">3</span>), axis=<span class="number">1</span>),</span><br><span class="line">              palette=<span class="string">'pastel'</span>)</span><br><span class="line">plt.title(<span class="string">'Gender vs Average Scores'</span>, fontsize=<span class="number">20</span>, fontweight=<span class="number">30</span>)</span><br><span class="line">plt.savefig(<span class="string">"5-Gender2Ave.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br></pre></td></tr></table></figure>
<p><img src="/images/CS5346/StudentPerformance/5-Gender2Ave.png" alt="Gender2Ave.png"><br>This histogram shows that the average score of female is higher than that of male, and the proportion of male who drop out of courses is greater than female.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.countplot(x=SP[<span class="string">'race/ethnicity'</span>], data=SP,</span><br><span class="line">              hue=SP.apply(<span class="keyword">lambda</span> x: getgrade((x[<span class="string">'math score'</span>] + x[<span class="string">'reading score'</span>] + x[<span class="string">'writing score'</span>]) / <span class="number">3</span>), axis=<span class="number">1</span>),</span><br><span class="line">              palette=<span class="string">'husl'</span>)</span><br><span class="line">plt.title(<span class="string">'Race/Ethnicity vs Average Scores'</span>, fontsize=<span class="number">20</span>, fontweight=<span class="number">30</span>)</span><br><span class="line">plt.savefig(<span class="string">"5-Race2Ave.png"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.clf()</span><br></pre></td></tr></table></figure><br><img src="/images/CS5346/StudentPerformance/5-Race2Ave.png" alt="Race2Ave.png"><br>Group C has a higher proportion of failing the sources in average while group E are more likely to get a nice score (&gt;=90).</p>
]]></content>
      <categories>
        <category>Information Visualization</category>
      </categories>
      <tags>
        <tag>Information Visualization</tag>
      </tags>
  </entry>
  <entry>
    <title>Mixture of Experts (MoE)</title>
    <url>/2021/02/18/DL_Mixture%20_of_Experts/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Simply increasing the number of parameters may have a negative impact on the performance.<br>Question is: how to scale the model effectively?<br>eg. Switch-Transformer(1.6T parameters)<br>The core framework is <strong>Mixture of Experts</strong>.<br><a id="more"></a></p>
<p>Conditional computation: word embedding(transfer a high-dimensional vector to a low-dimensional vector)<br>The goal is to increase model capacity without a proportional increase in conputational costs.</p>
<h1 id="Sparsely-Gated-MoE-for-LSTM"><a href="#Sparsely-Gated-MoE-for-LSTM" class="headerlink" title="Sparsely-Gated MoE for LSTM"></a>Sparsely-Gated MoE for LSTM</h1><p>MoE layer has n expert networks $E_1,…,E_n$, each has its own parameters.<br>$G(x)$: output of the gating network, a sparse n-dimensional vector.<br>$E_i(x)$: output of the $i^{th}$ expert network.<br>Wherever $G(x)_i$ = 0, we don’t need to compute $E_i(x)$.</p>
<ul>
<li>Idea of conditional computation.</li>
<li>We can have up to thousands of experts, but only need to evaluate a few of them for every example.<br><img src="/images/nndl2/moe.png" alt="MixtureOfExperts"><br>Gating networks:</li>
</ul>
<ol>
<li>Softmax Gating: a simple choice of non-sparse function</li>
<li>Noisy Top-K Gating: sparsity(save computation) and noise(helps with load balancing)<br><img src="/images/nndl2/TopKgating.png" alt="Top-K Gating"><br><strong>Load-Unbalanced Issue</strong><br>Some experts have too many tasks, other experts are idle.</li>
</ol>
<ul>
<li>The system’s speed will be limited by the slowest expert(the busiest expert), which is a very serious issue on distributed systems.</li>
<li>The gating network tends to converge to a state where it always produces large weights for the same few experts.</li>
<li>The load-imbalance is <em>self-reinforcing</em>: the favored experts are trained more rapidly and thus are selected even more by the gating network.</li>
</ul>
<p><img src="/images/nndl2/moe_loss.png" alt="moe_loss"></p>
<h1 id="Why-Transformers"><a href="#Why-Transformers" class="headerlink" title="Why Transformers?"></a>Why Transformers?</h1><p>Many researchers are using transformer to replace LSTM.<br>Transformer: Encoder &amp; Decoder</p>
<h1 id="Sparsely-Gated-MoE-for-Transformers"><a href="#Sparsely-Gated-MoE-for-Transformers" class="headerlink" title="Sparsely-Gated MoE for Transformers"></a>Sparsely-Gated MoE for Transformers</h1>]]></content>
      <categories>
        <category>Neural Network and Deep Learning 2</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>NUS note</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Object_detection</title>
    <url>/2021/02/18/Object_detection/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM VS Transformer</title>
    <url>/2021/02/18/LSTM%20VS%20Transformer/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>面试准备</category>
      </categories>
      <tags>
        <tag>NN</tag>
        <tag>ML</tag>
      </tags>
  </entry>
</search>
